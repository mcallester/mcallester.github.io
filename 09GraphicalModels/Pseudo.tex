\input ../SlidePreamble
\input ../preamble

\begin{document}

{\Huge

  \centerline{\bf TTIC 31230, Fundamentals of Deep Learning}
  \bigskip
  \centerline{David McAllester}
  \vfill
  \vfill
  \centerline{\bf Masked Language Modeling (MLM)}
  \vfill
  \centerline{\bf Gibbs Sampling}
  \vfill
  \centerline{\bf and Pseudo-Likelihood}
\vfill
\vfill
\vfill

\slide{Masked Language Models (MLMs)}

\centerline{\bf BERT: Pre-training of Deep Bidirectional Transformers ...}
\centerline{\bf Devlin et al., October 2018}

\slide{Masked Language Models (MLMs)}

Autoregressive text generation requires the words to be generated one at a time (sequentially).

\vfill
MLM allows the words to be generated in parallel.

\vfill
Parallel generation of novel text is very low quality.

\vfill
However, parallel generation in machine translation can have comparable performance to autoregressive translation.

\vfill
Parallel generation in translation can be faster than autoregressive translation.

\slide{Masked Language Models}

Consider a probability distribution on a block of text.

$$y = (w_1, \dots, w_T)$$

\vfill
In BERT 15\% of the words in a block of text are masked and the masked words are predicted from the unmasked words using a transformer model.

\slide{Pseudo-Likelihood}

MLM is closely reated to Pseudo-Likelihood (1975) and Gibbs Sampling (1984).

\vfill
For $y = (w_1,\ldots,w_T)$ define
$$y_{-i} = (w_1,\ldots,w_{i-1},M,w_{i+1},\ldots w_T)$$
where $M$ is a fixed mask.

\vfill
For a probability distribution $P$ on strings we define the pseudo-liklihood $\tilde{P}$ by
$$\tilde{P}(y) = \prod_i P(w_i\;|y_{-i})$$

\slide{Pseudo-Likelihood}
$$\tilde{P}(y) = \prod_i P(w_i\;|y_{-i})$$

\vfill
Pseudo-likelihood is particularly relevant to training Markov random fields (graphical models).

\vfill
But pseudo-likelihood corresponds to the objective function of MLMs with one mask per text block.

\begin{eqnarray*}
\Phi^* & = & \argmin_\Phi\; E_{y \sim \pop}\;-\ln \tilde{P}_\Phi(y) \\
\\
& = & \argmin_\Phi\; \sum_i\; E_{y \sim \pop}\;\;-\ln P_\Phi(w_i|y_{-i})
\end{eqnarray*}

\slide{Pseudo-Likelihood}

$$\Phi^* = \argmin_\Phi\; \sum_i \; E_{y \sim \pop}\;\;-\ln P_\Phi(w_i|y_{-i})$$

\vfill
Assuming universality we get

$$P_{\Phi^*}(w_i|y_{-i}) = \pop(w_i\;|\;y_{-i})$$

\slide{Gibbs Sampling}
$$P_{\Phi^*}(w_i|y_{-i}) = \pop(w_i\;|\;y_{-i})$$

\vfill
The ability to compute conditional probabilities does not immediately provide any way to compute $P_\Phi(y)$ or to sample $y$ from $P_\Phi(y)$.

\vfill
In principle sampling can be done with an MCMC process called Gibbs sampling.

\slide{Gibbs Sampling}

Let $y[i\leftarrow w]$ be the word sequence resulting from replacing the $i$th word in the word sequence $y$ by the word $w$.


\vfill
Gibbs sampling is defined by stochastic state transition

\begin{eqnarray*}
y^{t+1} & = & y^t[i\leftarrow w] \\
i & \sim & \mbox{uniform on $\{1,\ldots,T\}$} \\
w & \sim & P_\Phi(w_i\;|\;y_{-i})
\end{eqnarray*}

\slide{Markov Processes}

A Markov chain is an autoregressive probability distribution on infite sequences $s_1,s_2,s_3,\ldots$ defined by

\begin{eqnarray*}
P(s_1) & = & P_0(s_1) \\
P(s_{t+1}|s_1,\ldots,s_t) & = & P_T(s_{t+1}|s_t)
\end{eqnarray*}

\vfill
Here we are interested in the case where $s_t$ is is the translation sentence after $t$ rounds of parallel updates.

\vfill
This process defines a probability distribution $P_t(s)$ on sentences after $t$ rounds of updates.

\slide{Markov Processes}
For a distribution $Q$ on states (sentences) define $P(Q)$ to be the distribution on sentences defined by

\vfill
$$P(Q)(s) = P(s_{t+1} = s\;|\;s_t),\;\;\;s_t \sim Q$$

\vfill
A stationary distribution of a Markov process is a distribution $Q$ (on sentences in this example) such that $P(Q) = Q$.
for

\vfill
Any Markov chain (defined by transition probabilities on states) that is ``ergotic'' in the sense that every state can reach every state has a unique stationary distribution.

\slide{Gibbs Sampling and Pseudo-Liklihood}

Pseudo-liklihood defines a Gibbs Sampling Markov chain.

\vfill
It is a theorem that if this Markov Chain is ergotic then its stationary distribution equals the populaiton distribution.

\slide{Markov Processes}

If the conditional distributions allow any state (sentence) to reach any state then the conditional probabilities determine a unique distribution on
strings with the given conditional probabilities.

\vfill
Furtermore, we can in principle sample from this distribution by running the Gibbs Markov chain sufficiently long.

\slide{Gibbs Sampling}

For langauge modeling Gibbs sampling mixes too slowly --- it does not reach its stationary distribution in feasible time.

\vfill
However, in the case of translation the distribution on $y$ given $x$ is lower entropy and Gibbs sampling seems practicle.

\slide{END}
}
\end{document}


\slideplain{Contrastive Divergence (CDk)}

In contrastive divergence we first construct an MCMC process whose stationary distribution is ${\color{red} P_s}$.  This could be
Metropolis or Gibbs or something else.

\vfill
{\bf Algorithm CDk}: Given a gold segmentation ${\cal Y}$, start the MCMC process from initial state ${\cal Y}$ and run the process for $k$ steps
to get ${\color{red} {\cal Y}'}$.  Then take the loss to be

\vfill
{\color{red} $${\cal L}_{\mathrm{CD}}  = s({\cal Y}') - s({\cal Y})$$}

If $P_s = \pop$ then the the distribution on ${\cal Y}'$ is the same as the distribution on ${\cal Y}$ and the
expected loss gradient is zero.

\slideplain{Gibbs CD1}

CD1 for the Gibbs MCMC process is a particularly interesting special case.

\vfill
{\bf Algorithm (Gibbs CD1)}: Given ${\cal Y}$, select a node $n$ at random and draw {\color{red} $\ell \sim P({\cal Y}[n]=\ell\;| \;{\cal Y}/n)$}. Define {\color{red} ${\cal Y}[n=\ell]$}
to be the assignment (segmentation) which is the same as ${\cal Y}$ except that node $n$ is assigned label $\ell$.  Take the loss to be

\vfill
{\color{red} $${\cal L}_{\mathrm{CD}}  = s({\cal Y}[n=\ell]) - s({\cal Y})$$}

\slide{Gibbs CD1 Theorem}

Gibbs CD1 is equivalent in expectation to pseudolikelihood.

{\huge
\begin{eqnarray*}
{\cal L}_{\mathrm{PL}} & = & E_{{\cal Y} \sim \pop}\;\sum_n \; - \ln P_s({\cal Y}\;|\;{\cal Y}/n) \\
\\
 & = & E_{{\cal Y} \sim \pop}\;\sum_n -\ln \frac{e^{s({\cal Y})}}{Z_n}\;\;\;\;\;{Z_n = \sum_{\ell'} e^{s({\cal Y}[n=\ell'])}} \\
\\
& = & E_{{\cal Y} \sim \pop}\;\sum_n\; \left(\ln Z_n - s({\cal Y})\right) \\
\\
\nabla_\Phi {\cal L}_{\mathrm{PL}} & = & E_{{\cal Y} \sim \pop}\;\sum_n \left(\frac{1}{Z_n} \sum_{\ell'} e^{s({\cal Y}[n=\ell'])} \; \nabla_\Phi\;s({\cal Y}[n=\ell'])\right) - \nabla_\Phi s({\cal Y}) \\
\\
& = & E_{{\cal Y} \sim \pop}\;\sum_n \left(\sum_{\ell'} P_s({\cal Y}[n]=\ell'\;|\;{\cal Y}/n) \; \nabla_\Phi\;s({\cal Y}[n=\ell'])\right) - \nabla_\Phi s({\cal Y})
\end{eqnarray*}
}

\slideplain{Gibbs CD1 Theorem}

{\huge
\begin{eqnarray*}
\nabla_\Phi\;{\cal L}_{\mathrm{PL}} & = & E_{{\cal Y} \sim \pop}\;\sum_n \left(\sum_{\ell'} P_s({\cal Y}[n]=\ell'\;|\;{\cal Y}/n)\; \nabla_\Phi\;s({\cal Y}[n=\ell'])\right) - \nabla_\Phi s({\cal Y}) \\
\\
& = & E_{{\cal Y} \sim \pop}\;\sum_n \left(E_{\ell' \sim P_s({\cal Y}[n]=\ell'\;|\;{\cal Y}/n)} \nabla_\Phi\;s({\cal Y}[n=\ell'])\right) - \nabla_\Phi s({\cal Y}) \\
\\
& \propto & E_{{\cal Y} \sim \pop}\;E_n\;  E_{\ell' \sim P_s({\cal Y}[n]=\ell'\;|\;{\cal Y}/n)}\;\; (\nabla_\Phi\;s({\cal Y}[n=\ell']) - \nabla_\Phi s({\cal Y})) \\
\\
& = & E_{{\cal Y} \sim \pop}\;E_n\; E_{\ell' \sim P_s({\cal Y}[n]=\ell'\;|\;{\cal Y}/n)}\;\; \nabla_\Phi\;{\cal L}_{\mbox{Gibbs CD(1)}}
\end{eqnarray*}
}
