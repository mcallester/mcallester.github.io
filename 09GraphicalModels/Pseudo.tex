\input ../SlidePreamble
\input ../preamble

\begin{document}

{\Huge

  \centerline{\bf TTIC 31230, Fundamentals of Deep Learning}
  \bigskip
  \centerline{David McAllester, Autumn 2020}
  \vfill
  \vfill
  \centerline{\bf Pseudo-Likelihood and Contrastive Divergence}
\vfill
\vfill
\vfill

\slide{Notation}

$x$ is an input (e.g. an image).

\vfill
$\hat{\cal Y} [N]$ is a structured label for $x$ --- a vector $\hat{\cal Y}[0],\ldots,\hat{\cal Y}[N-1]$. (e.g., $n$ ranges over pixels where $\hat{\cal Y}[n]$ is a semantic label of pixel $n$.)

\vfill
$\hat{\cal Y}/n$ is the set of labels assigned by $\hat{\cal Y}$ at indeces (pixels) other than $n$.

\vfill
$\hat{\cal Y}[n=\ell]$ is the structured label identical to $\hat{\cal Y}$ except that it assigns label $\ell$ to index (pixel) $n$.

\slide{Intractable Exponential Softmax}

\vfill
We consider a softmax distribution

\vfill
\begin{eqnarray*}
P_s(\hat{\cal Y}) & = & \frac{1}{Z}e^{s(\hat{\cal Y})} \\
\\
Z & = & \sum_{\hat{\cal Y}} e^{s(\hat{\cal Y})}
\end{eqnarray*}

\vfill
Computing $Z$ is intractable.

\slide{Psuedo-Likelihood}

For any distribution $P(\hat{\cal Y})$ on structured labels $\hat{\cal Y}$,
we define the {\color{red} pseudo-likelihood}  $\tilde{P}(\hat{\cal Y})$ as follows

{\color{red} $$\tilde{P}(\hat{\cal Y}) = \prod_n\;P(\hat{\cal Y}[n]\;|\; \hat{\cal Y}/n)$$}

\vfill
$$P(\hat{\cal Y}[n]\;|\;\hat{\cal Y}/n) = \frac{1}{Z_n}e^{s(\hat{\cal Y})}\;\;\;\;\;Z_n = \sum_{\ell} e^{s(\hat{\cal Y}[n=\ell])}$$

\vfill
While computing $P_s(\hat{\cal Y})$ is intractable, computing $\tilde{P}_s(\hat{\cal Y})$ involves only local partition functions and is tractable.

\slide{Pseudo Cross-Entropy Loss}

We can then do SGD on pseudo cross-Entropy loss.
\vfill
{\color{red} $$\Phi^* = \argmin_\Phi E_{\tuple{x,{\cal Y}} \sim \pop}\;\;-\ln \tilde{P}_{\Phi,x}({\cal Y})$$}

\slide{Pseudolikelihood Theorem}

We will show that for any $Q$ we have

\vfill
$$E_{{\cal Y} \sim \pop}\;-\ln \widetilde{\pop}({\cal Y}) \leq \;E_{{\cal Y} \sim \pop}\;-\ln \tilde{Q}({\cal Y})$$

\vfill
Hence $\pop$ is a minimizer of the pseudo-likelihood cross-entropy.

\slide{Pseudolikelihood Theorem}

\vfill
$$E_{{\cal Y} \sim \pop}\;-\ln \widetilde{\pop}({\cal Y}) \leq \;E_{{\cal Y} \sim \pop}\;-\ln \tilde{Q}({\cal Y})$$

\vfill
It is also true that if the support of $\pop$ is ``ergotic'' in the sense that it is connected under the neighbor relation defined by changing a single node, then
$\pop$ is the {\em only} minimizer of the pseudo-liklihood loss.

\vfill
To see that ergodicity is needed consider a two node network with Boolean nodes that agree with probability 1.  The conditional distributions do not determine
the probability that the nodes are true and hence many distributions minimize the pseudo-likelihood cross-entropy.

\slide{Proof that $\pop$ is a Minimizer}

{\huge
\begin{eqnarray*}
  & & \min_Q \;E_{{\cal Y}\sim \pop} -\ln \tilde{Q}({\cal Y}) \\
  \\
  & = & \min_Q \;E_{{\cal Y}\sim \pop} \;\sum_n\;-\ln Q({\cal Y}[n]\;|\;{\cal Y}/n) \\
  \\
  & \geq & \min_{P_1,\ldots,P_N} \;E_{{\cal Y}\sim \pop} \;\sum_n\;-\ln P_n({\cal Y}[n]\;|\;{\cal Y}/n) \\
  \\
  & = & \min_{P_1,\ldots,P_N} \;\sum_n \;E_{{\cal Y} \sim \pop}\;\; -\ln P_n({\cal Y}[n]\;|\;{\cal Y}/n) \\
  \\          
  & = & \sum_n \;\min_{P_n}\;E_{{\cal Y} \sim \pop}\;\; -\ln P_n({\cal Y}[n]\;|\;{\cal Y}/n) \\
  \\
    & = & \sum_n E_{{\cal Y} \sim \pop}\;\; -\ln \pop({\cal Y}[n]\;|\;{\cal Y}/n) =  E_{{\cal Y} \sim \pop}{-\ln \widetilde{\pop}({\cal Y})}
\end{eqnarray*}
}

\slideplain{Contrastive Divergence (CDk)}

In contrastive divergence we first construct an MCMC process whose stationary distribution is ${\color{red} P_s}$.  This could be
Metropolis or Gibbs or something else.

\vfill
{\bf Algorithm CDk}: Given a gold segmentation ${\cal Y}$, start the MCMC process from initial state ${\cal Y}$ and run the process for $k$ steps
to get ${\color{red} {\cal Y}'}$.  Then take the loss to be

\vfill
{\color{red} $${\cal L}_{\mathrm{CD}}  = s({\cal Y}') - s({\cal Y})$$}

If $P_s = \pop$ then the the distribution on ${\cal Y}'$ is the same as the distribution on ${\cal Y}$ and the
expected loss gradient is zero.

\slideplain{Gibbs CD1}

CD1 for the Gibbs MCMC process is a particularly interesting special case.

\vfill
{\bf Algorithm (Gibbs CD1)}: Given ${\cal Y}$, select a node $n$ at random and draw {\color{red} $\ell \sim P({\cal Y}[n]=\ell\;| \;{\cal Y}/n)$}. Define {\color{red} ${\cal Y}[n=\ell]$}
to be the assignment (segmentation) which is the same as ${\cal Y}$ except that node $n$ is assigned label $\ell$.  Take the loss to be

\vfill
{\color{red} $${\cal L}_{\mathrm{CD}}  = s({\cal Y}[n=\ell]) - s({\cal Y})$$}

\slide{Gibbs CD1 Theorem}

Gibbs CD1 is equivalent in expectation to pseudolikelihood.

{\huge
\begin{eqnarray*}
{\cal L}_{\mathrm{PL}} & = & E_{{\cal Y} \sim \pop}\;\sum_n \; - \ln P_s({\cal Y}\;|\;{\cal Y}/n) \\
\\
 & = & E_{{\cal Y} \sim \pop}\;\sum_n -\ln \frac{e^{s({\cal Y})}}{Z_n}\;\;\;\;\;{Z_n = \sum_{\ell'} e^{s({\cal Y}[n=\ell'])}} \\
\\
& = & E_{{\cal Y} \sim \pop}\;\sum_n\; \left(\ln Z_n - s({\cal Y})\right) \\
\\
\nabla_\Phi {\cal L}_{\mathrm{PL}} & = & E_{{\cal Y} \sim \pop}\;\sum_n \left(\frac{1}{Z_n} \sum_{\ell'} e^{s({\cal Y}[n=\ell'])} \; \nabla_\Phi\;s({\cal Y}[n=\ell'])\right) - \nabla_\Phi s({\cal Y}) \\
\\
& = & E_{{\cal Y} \sim \pop}\;\sum_n \left(\sum_{\ell'} P_s({\cal Y}[n]=\ell'\;|\;{\cal Y}/n) \; \nabla_\Phi\;s({\cal Y}[n=\ell'])\right) - \nabla_\Phi s({\cal Y})
\end{eqnarray*}
}

\slideplain{Gibbs CD1 Theorem}

{\huge
\begin{eqnarray*}
\nabla_\Phi\;{\cal L}_{\mathrm{PL}} & = & E_{{\cal Y} \sim \pop}\;\sum_n \left(\sum_{\ell'} P_s({\cal Y}[n]=\ell'\;|\;{\cal Y}/n)\; \nabla_\Phi\;s({\cal Y}[n=\ell'])\right) - \nabla_\Phi s({\cal Y}) \\
\\
& = & E_{{\cal Y} \sim \pop}\;\sum_n \left(E_{\ell' \sim P_s({\cal Y}[n]=\ell'\;|\;{\cal Y}/n)} \nabla_\Phi\;s({\cal Y}[n=\ell'])\right) - \nabla_\Phi s({\cal Y}) \\
\\
& \propto & E_{{\cal Y} \sim \pop}\;E_n\;  E_{\ell' \sim P_s({\cal Y}[n]=\ell'\;|\;{\cal Y}/n)}\;\; (\nabla_\Phi\;s({\cal Y}[n=\ell']) - \nabla_\Phi s({\cal Y})) \\
\\
& = & E_{{\cal Y} \sim \pop}\;E_n\; E_{\ell' \sim P_s({\cal Y}[n]=\ell'\;|\;{\cal Y}/n)}\;\; \nabla_\Phi\;{\cal L}_{\mbox{Gibbs CD(1)}}
\end{eqnarray*}
}

\slide{END}
}
\end{document}
