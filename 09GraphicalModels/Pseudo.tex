\input ../SlidePreamble
\input ../preamble

\begin{document}

{\Huge

  \centerline{\bf TTIC 31230, Fundamentals of Deep Learning}
  \bigskip
  \centerline{David McAllester, Autumn 2022}
  \vfill
  \vfill
  \centerline{\bf Masked Language Modeling (MLM)}
  \vfill
  \centerline{\bf Gibbs Sampling}
  \vfill
  \centerline{\bf and Pseudo-Likelihood}
\vfill
\vfill
\vfill

\slide{Masked Language Models (MLMs)}

\centerline{\bf BERT: Pre-training of Deep Bidirectional Transformers ...}
\centerline{\bf Devlin et al., October 2018}

\vfill
Consider a probability distribution on a block of text.
$$y = (w_1, \dots, w_T)$$

\vfill
In BERT 15\% of the words in a block of text are masked and the masked words are predicted from the unmasked words using a transformer model.

\vfill
The intuition is that MLMs achieve a better ``understanding'' of the robability distribution because each prediction looks at both the past and the future.

\slide{Masked Language Models (MLMs)}

The BERT paper presents experiments showing that fine-tuning MLMs for downstream tasks (such as the the GLUE benchmark) outperformed fine-tuning 
of autoregressive models on those same tasks.

\slide{Masked Language Modeling}

However, the more recent application of MLMs has been in machine translation.

\vfill
The translation $y$ of a given sentence $x$ is generated by a word-parallel initialization followed by some number of rounds of word-parallel Gibbs Sampling.

\vfill
This word-parallel sampling is faster on parallel hardware than auto-regressive sampling.

\slide{Pseudo-Likelihood}

Here we will give a theoretical analysis of MLMs in terms of Pseudo-Likelihood (1975) and Gibbs Sampling (1984).

\vfill
For $y = (w_1,\ldots,w_T)$ define
$$y_{-i} = (w_1,\ldots,w_{i-1},M,w_{i+1},\ldots w_T)$$
where $M$ is a fixed mask.

\vfill
For a probability distribution $P$ on strings we define the pseudo-liklihood $\tilde{P}$ by
$$\tilde{P}(y) = \prod_i P(w_i\;|y_{-i})$$

\slide{Pseudo-Likelihood}
$$\tilde{P}(y) = \prod_i P(w_i\;|y_{-i})$$

\vfill
Pseudo-likelihood is particularly relevant to training Markov random fields (graphical models).

\vfill
But pseudo-likelihood corresponds to the objective function of MLMs with one mask per text block.

\begin{eqnarray*}
\Phi^* & = & \argmin_\Phi\; E_{y \sim \pop}\;-\ln \tilde{P}_\Phi(y) \\
\\
& = & \argmin_\Phi\; \sum_i\; E_{y \sim \pop}\;\;-\ln P_\Phi(w_i|y_{-i})
\end{eqnarray*}

\slide{Pseudo-Likelihood}

$$\Phi^* = \argmin_\Phi\; \sum_i \; E_{y \sim \pop}\;\;-\ln P_\Phi(w_i|y_{-i})$$

\vfill
Assuming universality we get

$$P_{\Phi^*}(w_i|y_{-i}) = \pop(w_i\;|\;y_{-i})$$

\slide{Gibbs Sampling}
$$P_{\Phi^*}(w_i|y_{-i}) = \pop(w_i\;|\;y_{-i})$$

\vfill
The ability to compute conditional probabilities does not immediately provide any way to compute $P_\Phi(y)$ or to sample $y$ from $P_\Phi(y)$.

\vfill
In principle sampling can be done with an MCMC process called Gibbs sampling.

\slide{Gibbs Sampling}

Let $y[i\leftarrow w]$ be the word sequence resulting from replacing the $i$th word in the word sequence $y$ by the word $w$.


\vfill
Gibbs sampling is defined by stochastic state transition

\begin{eqnarray*}
y^{t+1} & = & y^t[i\leftarrow w] \\
i & \sim & \mbox{uniform on $\{1,\ldots,T\}$} \\
w & \sim & P_\Phi(w_i\;|\;y_{-i})
\end{eqnarray*}

\slide{Gibbs Sampling}

Any Markov chain (defined by transition probabilities on states) that is ``ergotic'' in the sense that every state can reach every state has a unique stationary distribution.

\vfill
This implies that if the conditional distributions allow any state to reach any state then the conditional probabilities determine a unique distribution on
strings with the given conditional probabilities.

\vfill
Furtermore, we can in principle sample from this distribution by running the Gibbs Markov chain sufficiently long.

\slide{Gibbs Sampling}

For langauge modeling Gibbs sampling mixes too slowly --- it does not reach its stationary distribution in feasible time.

\vfill
However, in the case of translation the distribution on $y$ given $x$ is lower entropy and Gibbs sampling seems practicle.

\slide{END}
}
\end{document}


\slideplain{Contrastive Divergence (CDk)}

In contrastive divergence we first construct an MCMC process whose stationary distribution is ${\color{red} P_s}$.  This could be
Metropolis or Gibbs or something else.

\vfill
{\bf Algorithm CDk}: Given a gold segmentation ${\cal Y}$, start the MCMC process from initial state ${\cal Y}$ and run the process for $k$ steps
to get ${\color{red} {\cal Y}'}$.  Then take the loss to be

\vfill
{\color{red} $${\cal L}_{\mathrm{CD}}  = s({\cal Y}') - s({\cal Y})$$}

If $P_s = \pop$ then the the distribution on ${\cal Y}'$ is the same as the distribution on ${\cal Y}$ and the
expected loss gradient is zero.

\slideplain{Gibbs CD1}

CD1 for the Gibbs MCMC process is a particularly interesting special case.

\vfill
{\bf Algorithm (Gibbs CD1)}: Given ${\cal Y}$, select a node $n$ at random and draw {\color{red} $\ell \sim P({\cal Y}[n]=\ell\;| \;{\cal Y}/n)$}. Define {\color{red} ${\cal Y}[n=\ell]$}
to be the assignment (segmentation) which is the same as ${\cal Y}$ except that node $n$ is assigned label $\ell$.  Take the loss to be

\vfill
{\color{red} $${\cal L}_{\mathrm{CD}}  = s({\cal Y}[n=\ell]) - s({\cal Y})$$}

\slide{Gibbs CD1 Theorem}

Gibbs CD1 is equivalent in expectation to pseudolikelihood.

{\huge
\begin{eqnarray*}
{\cal L}_{\mathrm{PL}} & = & E_{{\cal Y} \sim \pop}\;\sum_n \; - \ln P_s({\cal Y}\;|\;{\cal Y}/n) \\
\\
 & = & E_{{\cal Y} \sim \pop}\;\sum_n -\ln \frac{e^{s({\cal Y})}}{Z_n}\;\;\;\;\;{Z_n = \sum_{\ell'} e^{s({\cal Y}[n=\ell'])}} \\
\\
& = & E_{{\cal Y} \sim \pop}\;\sum_n\; \left(\ln Z_n - s({\cal Y})\right) \\
\\
\nabla_\Phi {\cal L}_{\mathrm{PL}} & = & E_{{\cal Y} \sim \pop}\;\sum_n \left(\frac{1}{Z_n} \sum_{\ell'} e^{s({\cal Y}[n=\ell'])} \; \nabla_\Phi\;s({\cal Y}[n=\ell'])\right) - \nabla_\Phi s({\cal Y}) \\
\\
& = & E_{{\cal Y} \sim \pop}\;\sum_n \left(\sum_{\ell'} P_s({\cal Y}[n]=\ell'\;|\;{\cal Y}/n) \; \nabla_\Phi\;s({\cal Y}[n=\ell'])\right) - \nabla_\Phi s({\cal Y})
\end{eqnarray*}
}

\slideplain{Gibbs CD1 Theorem}

{\huge
\begin{eqnarray*}
\nabla_\Phi\;{\cal L}_{\mathrm{PL}} & = & E_{{\cal Y} \sim \pop}\;\sum_n \left(\sum_{\ell'} P_s({\cal Y}[n]=\ell'\;|\;{\cal Y}/n)\; \nabla_\Phi\;s({\cal Y}[n=\ell'])\right) - \nabla_\Phi s({\cal Y}) \\
\\
& = & E_{{\cal Y} \sim \pop}\;\sum_n \left(E_{\ell' \sim P_s({\cal Y}[n]=\ell'\;|\;{\cal Y}/n)} \nabla_\Phi\;s({\cal Y}[n=\ell'])\right) - \nabla_\Phi s({\cal Y}) \\
\\
& \propto & E_{{\cal Y} \sim \pop}\;E_n\;  E_{\ell' \sim P_s({\cal Y}[n]=\ell'\;|\;{\cal Y}/n)}\;\; (\nabla_\Phi\;s({\cal Y}[n=\ell']) - \nabla_\Phi s({\cal Y})) \\
\\
& = & E_{{\cal Y} \sim \pop}\;E_n\; E_{\ell' \sim P_s({\cal Y}[n]=\ell'\;|\;{\cal Y}/n)}\;\; \nabla_\Phi\;{\cal L}_{\mbox{Gibbs CD(1)}}
\end{eqnarray*}
}
