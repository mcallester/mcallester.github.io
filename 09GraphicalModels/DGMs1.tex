\input /users/davidmcallester/ICloude/tex/SlidePreamble
\input /users/davidmcallester/ICloude/tex/preamble

\begin{document}

{\Huge

  \centerline{\bf TTIC 31230, Fundamentals of Deep Learning}
  \bigskip
  \centerline{David McAllester, Autumn 2020}
  \vfill
  \vfill
  \centerline{\bf Deep Graphical Models}
  \vfill
  \centerline{\bf aka, Energy Based Models}
\vfill
\vfill
\vfill

\slide{Distributions on Exponentially Large Label Sets}

{\color{red}
$$\Phi^* = \argmin_\Phi E_{(x,{\cal Y}) \sim \mathrm{Pop}}\;-\ln \;P({\cal Y}|x)$$
}

\vfill
{\color{red} The structured case:} The number of possible labels is exponentially large --- iteration over the possible labels is infeasible.

\slide{Parsing and CKY}

Consider the case where $x$ is a sentence and $y$ is a parse tree for $x$.  There are exponentially many possible parse trees for a given sentence.

\vfill
The Cocke–Kasami–Younger (CKY) algorithm is a dynamic programming algorithm for finding the most probably parse under a certain family of energy based models.

\vfill
CKY is still the most accurate way to parse sentences where the energy based model is computed by a deep network.

\slide{Speech Recognition and CTC}

Consider the case where $x$ is the sound wave from a microphone and $y$ is the transcription into written language.  There are clearly exponentially many
possible transcriptions.

\vfill
Connectionist Temporal classification (CTC) is a dynamic programming algorithm for finding the most likely output under a certain family of energy based models.

\vfill
CTC is still the most accurate way to do speech recognition where the energy based model is computed by a deep network.


\slide{Semantic Segmentation}
\centerline{\includegraphics[height = 2.5in]{\images/SemSeg}}

\vfill
We want to assign each pixel to one of $L$ semantic classes.

\vfill
For example ``person'', ``car'', ``building'', ``sky'' or ``other''.

\slide{Semantic Segmentation}

\centerline{\includegraphics[height = 2.5in]{\images/SemSeg}}

\vfill
Although semantic segmentation is not currently done with energy based models, perhaps it should be.

\vfill
Semantic segmentation is simpler than parsing or speech recognition and will be used as a simple example of energy based models.

\slide{Distributions on Exponentially Large Sets: Notation}

$x$ is an input (e.g. an image).

\vfill
$\hat{\cal Y} [N]$ is a potential structured label for $x$ --- a vector $\hat{\cal Y}[0],\ldots,\hat{\cal Y}[N-1]$ with $\hat{\cal Y}[n]$ an integer in $\{1,\ldots,L\}$.

\vfill
For example $n$ might range over the pixels of an image and $\hat{\cal Y}[n]$ names one of $L$ a semantic labels for pixel $n$.

\vfill
${\cal Y}[N]$ is the gold label for input $x$ --- the structured label assigned to $x$ in the training data.

\slide{Exponential Softmax}

{\huge
\begin{eqnarray*}
{\color{red} P_s(\hat{{\cal Y}})} & = & \softmax_{\hat{{\cal Y}}}\;s(\hat{{\cal Y}}) \;\;\;\mbox{\color{red} all possible $\hat{{\cal Y}}$} \\
\\
\\
\\
\\
\mbox{cross-entropy loss} & = & - \ln P_s({\cal Y}) \;\;\;{\color{red} \;\;\;\;\mbox{gold label (training label) ${\cal Y}$}}
\end{eqnarray*}
}


\slidetwo{Compactly Representing Scores}{on Exponentially Many Labels}

We will call $n$ a ``node''.

\vfill
We will compute a ``node potential'' tensor {\color{red} $s^N[N,L]$} that holds $NL$ scores --- a score each possible assignment of a label $\ell$ to $n$.

\vfill
We assume a set $E$ of ``edges'' with $E \subseteq N \times N$.

\vfill
We will compute an ``edge potential'' tensor {\color{red} $s^E[E,L,L]$} that holds $ELL$ scores --- a score for each assignment of class labels $\ell_1$ and $\ell_2$
to the two nodes in the edge $e$.


\slide{Computing Scores}

{\huge
\begin{eqnarray*}
{\color{red} s(\hat{{\cal Y}})} & = & \sum_n\;s^N[n,\hat{{\cal Y}}[n]] + \sum_{\tuple{n,m} \in E}\;s^E[\tuple{n,m},\hat{{\cal Y}}[n],\;\hat{{\cal Y}}[m]] \\
\end{eqnarray*}
}

\vfill
The exponential softmax for scores of this form are intractible in general --- the partition function $Z_s$ requires summing over an exponentially large set and
computing $P_s({\cal Y})$ is $\#P$ hard.

\slide{Computing the Node and Edge Potential Tensors}

For input $x$ we use a network to compute the score tensors.

\vfill
\begin{eqnarray*}
s^N[N,L] & = & f^N_\Phi(x) \\
\\
\\
s^E[E,L,L] & = & f^E_\Phi(x)
\end{eqnarray*}

\slide{Back-Propagation Through Exponential Softmax}

\begin{eqnarray*}
s^N[I,L] & = & f^N_\Phi(x) \\
s^E[E,L,L] & = & f^E_\Phi(x)
\end{eqnarray*}

\vfill
\begin{eqnarray*}
{\color{red} s(\hat{{\cal Y}})} & = & \sum_n\;s^N[n,\hat{{\cal Y}}[n]] + \sum_{\tuple{n,m} \in \mathrm{Edges}}\;s^E[\tuple{n,m},\hat{{\cal Y}}[n],\;\hat{{\cal Y}}[m]] \\
\\
{\color{red} P_s(\hat{{\cal Y}})} & = & \softmax_{\hat{{\cal Y}}}\;s(\hat{{\cal Y}}) \;\;\mbox{\color{red} all possible $\hat{{\cal Y}}$} \\
\\
{\cal L} & = & {\color{red} - \ln P_s({\cal Y}) \;\;\;\mbox{gold label ${\cal Y}$}}
\end{eqnarray*}

\vfill
We want the gradients {\color{red} $s^N.\grad[N,L]$} and {\color{red} $s^E.\grad[E,L,L]$}.


\slide{END}

}

\end{document}
