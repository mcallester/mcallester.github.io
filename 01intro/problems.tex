\documentclass{article}
\input ../preamble
\parindent = 0em

\newcommand{\solution}[1]{}
%\newcommand{\solution}[1]{\bigskip {\color{red} {\bf Solution}: #1}}

\begin{document}


\centerline{\bf TTIC 31230 Fundamentals of Deep Learning}
\centerline{\bf Problems For Fundamental Equations.}

\bigskip
{\bf Problem 0: Backprogation through a ReLU linear threshold unit.}
Consider the computation
\begin{eqnarray*}
  y & = & \sigma(w^\top x) \\
  \ell & = & {\cal L}(y)
\end{eqnarray*}
for $w,x \in R^d$ with $\sigma(z) = \max(z,0)$ (the ReLU activation)
and for ${\cal L}(y)$ an arbitrary function (a loss function).  Let $w_i$ denote the $i$th component of the weight vector $w$.
Give an expression for $\frac{\partial \ell}{\partial w_i}$ as a function of $\frac{d{\cal L}(y)}{dy}$.
    
\end{document}
