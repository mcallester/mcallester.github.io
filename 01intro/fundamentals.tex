\input ../SlidePreamble
\input ../preamble

\begin{document}

{\Huge
  \centerline{\bf TTIC 31230,  Fundamentals of Deep Learning}
  \vfill
  \centerline{David McAllester, Autumn 2020}
  \vfill
  \centerline{\bf The Fundamental Equations of Deep Learning}

\slidetwo{What is a Deep Network?}
{VGG, Zisserman, 2014}

\centerline{\includegraphics[width = 7.0in]{\images/VGG}}
\centerline{\large Davi Frossard}

\vfill
\centerline{{\color{red} 138 Million Parameters}}

\slide{What is a Deep Network?}

{\color{red} We assume some set ${\cal X}$ of possible inputs, some set ${\cal Y}$ of possible outputs,
and a parameter vector $\Phi \in \reals^d$.}

\vfill
For $\Phi \in \reals^d$ and $x \in {\cal X}$ and $y \in {\cal Y}$ a deep network computes a probability {\color{red} $P_\Phi(y|x)$}.

\slide{The Fundamental Equation of Deep Learning}

We assume a ``population'' probability distribution $\mathrm{Pop}$ on pairs $(x,y)$.

\vfill
\begin{eqnarray*}
{\color{red} \Phi^*} & {\color{red}  =} & {\color{red} \argmin_\Phi \;E_{(x,y) \sim \mathrm{Pop}}\; \left[-\ln \;P_\Phi(y|x)\right]}
\end{eqnarray*}

\vfill
This loss function {\color{red} ${\cal L}(x,y,\Phi) = - \ln \;P_\Phi(y|x)$} is called {\color{red} cross entropy loss}.

\slidetwo{A Second Fundamental Equation}{{\color{red} Softmax}: Converting Scores to Probabilities}

We start from a ``score'' function $s_\Phi(y|x) \in \reals$.

\vfill
\begin{eqnarray*}
  {\color{red} P_\Phi(y|x)} & {\color{red} =} & {\color{red} \frac{1}{Z}\;e^{s_\Phi(y|x)}};\;\; Z = \sum_y e^{s_\Phi(y|x)} \\
  \\
  & = & {\color{red} \softmax_y s_\Phi(y|x)}
\end{eqnarray*}

\slide{Note the Final Softmax Layer}

\centerline{\includegraphics[width = 8.0in]{\images/VGG}}
\centerline{\large Davi Frossard}

\slide{How Many Possibilities}

We have {\color{red} $y \in {\cal Y}$} where {\color{red} ${\cal Y}$} is some set of ``possibilities''.

\vfill
Binary: {\color{red} $Y = \{-1,1\}$}

\vfill
Multiclass: {\color{red} $Y = \{y_1,\ldots y_k\}$} $k$ manageable.

\vfill
Structured: {\color{red} $y$} is a ``structured object'' like a sentence.  Here {\color{red} $|Y|$} is unmanageable.

\slidetwo{Binary Classification}{Does This Image Contain a Bicycle?}

We have a population distribution over $(x,y)$ with $y \in \{-1,1\}$.

\vfill
We compute a single score $s_\Phi(x)$ where

\vfill
for $s_\Phi(x) \geq 0$ predict $y = 1$

\vfill
for $s_\Phi(x) < 0$ predict $y = -1$

\slide{Binary Classification: Softmax Cross Entropy}

\begin{eqnarray*}
  P_\Phi(y|x) & = & \softmax_{y \in \{-1,1\}}\;ys_\Phi(x) \;=\; \frac{1}{Z} \;e^{ys(x)} \\
  \\
  \\
  & = & \frac{e^{ys(x)}}{e^{ys(x)} + e^{-ys(x)}} \\
  \\
  \\
  & = & \frac{1}{1 + e^{-2ys(x)}} \\
  \\
  \\
    & = & {\color{red} \frac{1}{1 + e^{-m}} \;\;\;\;\;\;m = 2ys(x)\;\mbox{is the margin}}
\end{eqnarray*}

\ignore{
\slide{Logistic Regression for Binary Classification}

\begin{eqnarray*}
  \Phi^* & = & \argmin_\Phi\;E_{(x,y) \sim \mathrm{Pop}}\;{\cal L}(x,y,\Phi) \\
  \\
  & = & {\color{red} \argmin_\Phi\;E_{(x,y) \sim \mathrm{Pop}}\;-\ln P_\Phi(y|x)} \\
  \\
  & = & \argmin_\Phi\;E_{(x,y) \sim \mathrm{Pop}}\;\ln \left(1 + e^{-m(y|x)}\right) \\
  \\
  \ln \left(1 + e^{-m(y|x)}\right) & \approx & 0 \;\;\;\mbox{for $m(y|x) >> 1$} \\
  \\
  \ln \left(1 + e^{-m(y|x)}\right) & \approx & -m(y|x) \;\;\;\mbox{for $-m(y|x) >> 1$} \\
\end{eqnarray*}
}

\slideplain{Binary Classification}

Softmax Cross Entropy: {\color{red} $- \ln P_\Phi(y|x) = \ln(1+e^{-m})$}
\vfill
SVM Hinge loss:  {\color{red} $\max(0,1-m)$}

\vfill
\centerline{\parbox{6in}{Loss \hspace{1em} $\vcenter{\includegraphics[width = 5.0in]{\images/logloss}}$}}

\vfill
\centerline{~\hspace{3in} margin {\color{red} $m = 2ys(x),\;\;y \in \{-1,1\}$}}

\slide{Multiclass Classification (Next Token Prediction)}

We have a population distribution over $(x,y)$ with $y \in \{y_1,\;\ldots,\;y_k\}$.

\vfill
$$P_\Phi(y|x) = \softmax_y\;s_\Phi(y|x)$$

\vfill
\begin{eqnarray*}
  \Phi^* & = & \argmin_\Phi\;E_{(x,y) \sim \mathrm{Pop}}\;{\cal L}(x,y,\Phi) \\
  \\
  & = & {\color{red} \argmin_\Phi\;E_{(x,y) \sim \mathrm{Pop}}\;\left[- \ln P_\Phi(y|x)\right]}
\end{eqnarray*}

\slide{Structured Labeling (Machine Translation)}

We have a population of translation pairs $(x,y)$ with $x \in V_x^*$ and $y \in V_y^*$ where
$V_x$ and $V_y$ are source and target vocabularies respectively.

\vfill
{\color{red} $$\Phi^* = \argmin_\Phi E_{x,y \sim \pop}\;\left[- \ln P_\Phi(y|x)\right]$$}

\slide{AutoRegresive Models}
For langage we typically use an {\bf autoregressive model}:

\vfill
\begin{eqnarray*}
  P_\Phi(y_{t+1} | x,y_1,\ldots,y_t) & = & \softmax_{y \in V_y \cup \mathrm{<EOS>}} \; s_\Phi(y\;|\;x,y_1,\ldots,y_t) \\
  \\
  P_\Phi(y|x) & = & \prod_{t = 0}^{|y|} \;P_\Phi(y_{t+1}\;|\;x,y_1,\ldots,y_t)
\end{eqnarray*}

\vfill
\begin{eqnarray*}
  -\ln P(y|x) & = & \sum_{t = 0}^{|y|} \;-\ln P_\Phi(y_{t+1}\;|\;x,y_1,\ldots,y_t)
\end{eqnarray*}

\slide{Other Models For Structured Labels}

Alternatives to autoregressive models for the structured case include (old school) graphical models and diffusion models.

\slide{Fundamental Equation: Unconditional Form}

{\color{red} $$\Phi^* = \argmin_\Phi E_{y \sim \pop}\;\left[- \ln P_\Phi(y)\right]$$}

\slide{Summary}

\begin{eqnarray*}
\Phi^* & = &  \argmin_\Phi \;E_{(x,y) \sim \mathrm{Pop}}\;\left[ -\ln \;P_\Phi(y|x) \right]
\end{eqnarray*}

\vfill
\begin{eqnarray*}
 P_\Phi(y|x) & = & \frac{1}{Z}\;e^{s_\Phi(y|x)};\;\; Z = \sum_y e^{s_\Phi(y|x)} \\
  \\
  & = & \softmax_y s_\Phi(y|x)
\end{eqnarray*}

\slide{END}
}
\end{document}
