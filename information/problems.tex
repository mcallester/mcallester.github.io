\documentclass{article}
\input ../preamble
\parindent = 0em

\newcommand{\solution}[1]{}
%\newcommand{\solution}[1]{\bigskip {\color{red} {\bf Solution}: #1}}

\begin{document}


\centerline{\bf TTIC 31230 Fundamentals of Deep Learning}
\centerline{\bf Problems For Information Theory}

\vfill
\vfill
Assume that probability distributions $P(y)$ are discrete with $\sum_y\;P(y) = 1$.

\bigskip


\bigskip
~{\bf Problem 1: Cross Entropy Upper Bounds Entropy} The KL-divergence between two discrete distributions is defined by
$$KL(P,Q) = E_{x \sim P} \ln \frac{P(x)}{Q(x)}$$
We will show later in the class that $KL(P,Q) \geq 0$ for any $P$ and $Q$.

The Cross Entropy $H(P,Q)$ is defined by

$$H(P,Q) = E_{x \sim P} - \ln Q(x)$$

When $P$ is a population distribution and $Q$ is a model distribution this is the cross entropy loss.
You are responsible for the derivation of

$$\argmin_Q H(P,Q) = P$$

This is the motivation for cross-entropy loss.


{\bf Problem 1. }  This problem is on softmax, entropy and cross entropy.  Let the variable $i$ range over the non-negative integers (0 to $\infty$).
Consider the following softmax distribution where $\beta> 0$ is an inverse temperature parameter.

$$P(i) = \softmax_i \left[-\beta i\right]$$

{\bf (a)} Give an expression for $P(i)$ in terms of $i$ without using a softmax or infinite sum.  You can use the equality that for $0 < \lambda < 1$ we have

$$\sum_{n=0}^{\infty} \lambda^n = \frac{1}{1-\lambda}$$

\solution{$$P(i) = \frac{e^{-\beta i}}{\sum_{i=0}^\infty e^{-\beta i}} = (1-e^{-\beta})e^{-\beta i}$$}

{\bf (b)} Solve for the entropy of this distribution. You can use the equality that for $0 < \lambda < 1$ we have

$$\sum_{n=0}^{\infty} \;n \lambda^n = \frac{\lambda}{(1-\lambda)^2}$$

\solution{

  \begin{eqnarray*}
    H(P) & = & E_i\;\left[-\ln P(i)\right] =  \sum_{i=0}^\infty P(i)\;\left( - \;\ln \left((1-e^{-\beta})e^{-\beta i}\right)\right) \\\\
    & = & \sum_{i=0}^\infty P(i)\;\left(-\ln \left(1-e^{-\beta}\right) + \beta i\right) \\
    & = & \left(-\ln \left(1-e^{-\beta}\right)\sum_{i=0}^\infty P(i)\right) + \sum_i (1-e^{-\beta})e^{-\beta i} \;\beta i \\
    & = & - \ln \left(1-e^{-\beta}\right) + \beta(1-e^{-\beta})\;\;\sum_{i=0}^\infty i(e^{-\beta})^i \\
    & = & - \ln \left(1-e^{-\beta}\right) + \beta(1-e^{-\beta})\;\;\frac{e^{-\beta}}{(1-e^{-\beta})^2} \\
    & = & - \ln \left(1-e^{-\beta}\right) + \frac{\beta e^{-\beta}}{(1-e^{-\beta})}
  \end{eqnarray*}
}

{\bf (c)} Consider a one-hot population distribution defined by $\pop(k) = 1$ and $\pop(i) = 0$ for $i \not = k$.
What is the cross-entropy $H(\pop,P)$ and KL-divergence $KL(\pop,P)$?  What is the cross-entropy $H(P,\pop)$ and the KL divergence $KL(P,\pop)$?

\solution{
  \begin{eqnarray*}
    H(\pop,P) & = & E_{i\sim \pop}\left[ - \ln P(i)\right] \\
    & = & -\ln P(k) \\
    & = & -\ln (\;(1-e^{-\beta})e^{-\beta k}\;) \\
    & = & \beta k - \ln(1-e^{-\beta}) \\
      \\
      KL(\pop,P) & = & H(\pop,P) - H(\pop) \\
      & =& \beta k - \ln (1-e^{-\beta}) - 0 \\
      \\
      H(P,\pop) & = & KL(P,\pop) = \infty
  \end{eqnarray*}
}

\bigskip
{\bf Problem 2. Joint Entropy and Conditional Entropy}
We define conditional entropy $H(y|x)$ as follows
$$H(y|x) = E_{x,y} \;-\log P(y|x).$$
Given a distribution $P(x,y)$ show
$$H(P) = H(x) + H(y|x).$$

\solution{
\begin{eqnarray*}
  H(P) & = &  E_{(x,y) \sim P}\;-\ln P(x,y) \\
  \\
  & = & E_{(x,y) \sim P}\;-\ln P(x)P(y|x) \\
  \\
  & = & E_{(x,y) \sim P}\;\left(-\ln P(x) - \ln P(y|x)\right) \\
  \\
  & = & \left(E_{(x,y) \sim P}\;-\ln P(x)\right) + \left(E_{(x,y) \sim P}\;- \ln P(y|x)\right) \\
  \\
  & = & H(x) + H(y|x)
\end{eqnarray*}
}

\bigskip
~{\bf Problem 3. Unmeasurability of KL divergence and Population Entropy} The problem
of population density estimation is defined by the following equation.
$$\Phi^* = \argmin_\Phi\;H(\mathrm{Pop},Q_\Phi) = E_{y \sim \mathrm{Pop}}\;-\ln\;Q_\Phi(y)$$
This equation is used for language modeling --- estimating the probability distribution over the population of English sentences that appear, say, in the New York Times.

\medskip
(a) Show the following.
$$\Phi^* = \argmin_\Phi\;H(\mathrm{Pop},Q_\Phi) = \argmin_\Phi \;KL(\mathrm{Pop},Q_\Phi)$$



\solution{
  $$\argmin_\Phi \;KL(\mathrm{Pop},Q_\Phi) = \argmin_\Phi \;H(\pop,Q_\Phi) - H(\pop)$$
  Since $H(\pop)$ does not depend on $\Phi$ the minima are the same.
}

\bigskip
(b) Explain why we can measure $H(\pop,Q_\Phi)$ but cannot measure $KL(\pop,Q_\Phi)$ for the structured object unconditional case (language modeling) and for the
the conditional (labeling) case (imagenet).

\solution{
  We assume that the model is such that $Q_\Phi(y)$ can be computed.  For example, an auto-regressive language model allows us to compute
  $Q_\Phi(y)$ for a sentence $y$ as a product of next-word probabilities.
  
  Assuming $Q_\Phi(y)$ can be computed, we can compute (a good approximation to) $E_{y \sim \pop}\; -\ln Q_\Phi(y)$ by sampling sentences $y_1$, $\ldots$ $y_n$ from $\pop$
  and computing
  $$\hat{H}(\pop,Q_\Phi) = \frac{1}{N} \sum_i -\ln Q_\Phi(y_i).$$
  The confidence interval for this estimate shrinks as $1/\sqrt{N}$.

  \medskip
  However, in the case of strcutured objects, such as sentences, while we can sample from $\pop$, we cannot compute $\pop(y)$.
  Therefore we have no way of computing or even approximating, $H(\pop)$.  So we cannot compute
  $$KL(\pop,Q_\Phi) = H(\pop,Q_\Phi) - H(\pop).$$
  \bigskip
  For the conditional case we have
  \begin{eqnarray*}
    KL(\pop(y|x),Q_\Phi(y|x)) & = & E_{x,y \sim \pop}\;\ln \frac{\pop(y|x)}{Q_\Phi(y|x)} \\
    \\
    H(\pop(y|x),Q_\Phi(y|x)) & = &  E_{x,y \sim \pop}\;- \ln Q_\Phi(y|x)
  \end{eqnarray*}
  We assume that $Q_\Phi(y|x)$ can be computed and that allows $H(\pop(y|x),Q_\Phi(y|x))$ to be computed (to a good approximation) by taking the average of a sample.
  However, we cannot compute $\pop(y|x)$, even for binary classification, because (in most applications) we will never sample the same $x$ twice.
}

\bigskip
{\bf Problem 5. Asymmetry of cross entropy and KL-divergene} Consider the objective
\begin{equation}
  \label{revH}
  P^* = \argmin_P\;H(P,Q)
\end{equation}
Define $y^*$ by
$$y^* = \argmax_y\;Q(y)$$
Let $\delta_y$ be the distribution such that $\delta_y(y) = 1$ and $\delta_y(y') = 0$ for $y' \not = y$.
Show that $\delta_{y^*}$ minimizes (\ref{revH}).

\solution{
  Consider an arbitrary distribution $P$.  We must show that $H(P,Q) \geq H(\delta_{y^*},Q)$.
  \begin{eqnarray*}
    Q(y) & \leq & Q(y^*) \\
    -\ln Q(y) & \geq & -\ln Q(y^*) \\
    E_{y \sim P} - \ln Q(y) & \geq & -\ln Q(y^*) \\
    H(P,Q) & \geq & -\ln Q(y^*) = H(\delta_{y^*},Q)
  \end{eqnarray*}
}

\bigskip
Next consider
\begin{equation}
  \label{revKL}
  P^* = \argmin_P\;KL(P,Q)
\end{equation}
Show that $Q$ is the minimizer of (\ref{revKL}).

\solution{
  This follows from
  \begin{eqnarray*}
    KL(P,P) & = & E_{y \sim P} \ln \frac{P(x)}{P(x)} = 0 \\
    KL(P,Q) & \geq & 0
  \end{eqnarray*}
}

\bigskip
Next consider a subset $S$ of the possible values and let $Q_S$ be the restriction of $Q$ to the set $S$.
\begin{eqnarray*}
  Q_S(y) & = & \frac{1}{Q(S)}\left\{\begin{array}{ll} Q(y) & \mbox{for $y \in S$} \\ 0 & \mbox{otherwise} \end{array}\right.
\end{eqnarray*}

Show that that $KL(Q_S,Q) = -\ln Q(S)$ , which will be quite small if $S$ covers much of the mass.

\solution{
  \begin{eqnarray*}
    KL(Q_S,Q) & = & E_{y \sim Q_S}\;\ln \frac{Q_S(y)}{Q(y)} \\
    & = & E_{y \sim Q_S} \; \ln \frac{Q(y)/Q(S)}{Q(y)} \\
    & = & E_{y \sim Q_S} \; - \ln Q(S) \\
    & = & -\ln Q(S)
  \end{eqnarray*}
}

\medskip
Show that, in contrast, $KL(Q,Q_S)$ is infinite unless $S$ covers all values with non-zero propability.

\solution{
  If there exists a value $\tilde{y}$ not in $S$ with $P(\tilde{y}) > 0$ then
  $$E_{y \sim P}\;- \ln P_S(y) \geq - P(\tilde{y}) \ln 0 = \infty$$
  }


When we optimize a model $Q_\Phi$ under the objective $KL(Q_\Phi,Q)$ we can get that $Q_\Phi$ covers only one high probability region (a mode) of $Q$ (a problem called mode collapse)
while optimizing $Q_\Phi$ under the objective $KL(Q,Q_\Phi)$ we will tend to get that $Q_\Phi$ covers all of $Q$.  The two directions are very different even though both
are minimized at $P = Q$.


\bigskip
{\bf Problem 6. Data Processing Inequality}
Prove the data processing inequality that for any function $f$ with $z = f(y)$ we have $H(z) \leq H(y)$.

\medskip
Warning: This data processing inequality does not apply to contiuous densities.  A function on a continuous density can either expand or shrink the distribution which
increases or decrease its differential entropy respectively.

\solution{
  \begin{eqnarray*}
    H(y,z) & = & H(y) + H(z|y) = H(y) \\
    & = & H(z) + H(y|z)
  \end{eqnarray*}
  The result now follows from the fact that $H(y|z) \geq 0$
}

\bigskip
{\bf Problem 7. Mutual Information} Consider a joint distribution $P(x,y)$ on discrete random variables $x$ and $y$.
We define the marginal distributions $P(x)$ and $P(y)$ as follows.
\begin{eqnarray*}
  P(x) & = & \sum_y\;P(x,y) \\
  \\
  P(y) & = & \sum_x\;P(x,y)
\end{eqnarray*}
Let $Q(x,y)$ be defined to be the product of marginals.
$$Q(x,y) = P(x)P(y).$$
We define mutual information by
$$I(x,y) = KL(P,Q)$$
which I will write as
$$I(x,y) = KL(P(x,y),Q(x,y))$$
We define conditional entropy $H(y|x)$ by
$$H(y|x) = E_{x,y \sim P(x,y)} \;-\ln P(y|x).$$

(a) Show
$$I(x,y) = H(y) - H(y|x) = H(x) - H(x|y)$$

\solution{
  \begin{eqnarray*}
    I(x,y) & = & E_{x,y \sim P(x,y)}\;\ln \frac{P(x,y)}{P(x)P(y)} \\
    & = & E_{x,y \sim P(x,y)}\;\ln \frac{P(x)P(y|x)}{P(x)P(y)} \\
    & = & E_{x,y \sim P(x,y)}\;\ln \frac{P(y|x)}{P(y)} \\
    & = & \left(E_{y \sim P(y)}\;- \ln P(y) \right) - \left(E_{x,y \sim P(x,y)}\; -\ln P(y|x)\right) \\
    & = & H(y) - H(y|x)
  \end{eqnarray*}
  The other equality is similar.
}

    
    
\bigskip
(b) Explain why (a) implies $H(x) \geq H(x|y)$.

\solution{
  This is because the information $I(x,y)$ is a KL divergence which is always non-negative.
}

\medskip
(c) By stating (b) conditioned on $z$ we have
$$H(x|z) \geq H(x|y,z).$$

Use this to show that the data process inequality applies to mutual information,
i.e., that for $z = f(y)$ we have $I(x,z) \leq I(x,y)$.

\solution{
  We first note that for discrete distributions where $z$ is a function of $y$ we have $P(x|y,z) = P(x|y)$ which implies that $H(x|y,z) = H(x|y)$.  so the above inequality can be written as
  $$H(x|z) \geq H(x|y).$$
  The result then follows from
  $$I(x,z) = H(x) - H(x|z)$$
  and
  $$I(x,y) = H(x) - H(x|y)$$
}

\bigskip
{\bf Problem 8. 20 pts}  Consider the distribution on non-negative integers
given by$$P(i) = \frac{1}{2^{i+1}}.$$

\medskip
{\bf (a)} Using $\sum_{i=0}^\infty ar^i = \frac{a}{1-r}$ show that
$\sum_{i= 0}^\infty\;P(i) = 1$.

\solution{
  $$\sum_{i = 0}^\infty \frac{1}{2^{i+1}} = \sum_{i =0}^\infty
  \frac{1}{2}\left(\frac{1}{2}\right)^i =
  \frac{\frac{1}{2}}{1-\frac{1}{2}} = 1$$ }

\medskip
{\bf (b)} Using $\sum_{i=0}^{\infty} \;i r^i = \frac{r}{(1-r)^2}$
compute the numerical value of the entropy $H_2(P)$ for this
distribution (with your answer in bits).

\solution{
  \begin{eqnarray*}
    H_2(P) & = & E_i\;\left[-\log_2 P(i)\right] = \sum_{i=0}^\infty P(i)\;\left(-\log_2 \frac{1}{2^{i+1}}\right) \\
    & = & \sum_{i=0}^\infty \frac{1}{2^{i+1}}\;(i+1) \\
    & = & \frac{1}{2}\sum_{i=0}^\infty \frac{i}{2^i} + \sum_{i = 0}^\infty P(i) \\
    & = & 1 + \frac{1}{2} \left(\frac{\frac{1}{2}}{(1-\frac{1}{2})^2}\right) = 2 \; \mathrm{bits} \\
  \end{eqnarray*}
}

\medskip
{\bf (c)} Give a code word (a bit string) $c(i)$ for each
non-negative integer $i$ such that the code is prefix-free (no code
word is a proper prefix of any other code word) and such that expected
code length $E_{i \sim P} |c(i)|$ equals the entropy in bits you
calculated in part (b).

\solution{$c(i)$ is the string consisting of $i$ ones followed by a zero.
  For example $c(0) = 0$ and $c(3) = 1110$.  We then have that $|c(i)|
  = i+1$ so that the expected length is
  $$\sum_{i=0}^\infty P(i) |c(i)| = \sum_{i=0}^\infty P(i) (i+1) =
  \sum_{i=0}^\infty P(i) \left(-\log_2 P(i)\right) = H_2(P) = 2\;\mathrm{bits}$$ }

\bigskip
{\bf Problem 9. 30 pts}  Problem 2 was on converting probabilities to
codes.  This problem is on converting codes to probabilities.
Consider any prefx-free code $c(i)$ for the non-negative integers $i$.
Give a sampling procedure that either returns an integer $i$ or
fails to terminate and where the probability of returning $i$ is
$2^{-|c(i)|}$.

\solution{
  \begin{tabbing}
  Initialize $c$ to be the empty string. \\ \\ until \=termination
  do:\\ \\ \>If $c = c(i)$ for some $i$ return $i$. \\ \\ \>With
  probabilty $1/2$ set c to $c0$ else set $c$ to $c1$
  \end{tabbing}
  }

\bigskip
{\bf Problem 10. 30 pts} Shannon's source coding
theorem states that for any prefix-free code we have
$$E_{x\sim P} |c(x)| \geq H_2(P)$$
and for any $P$ there exists a prefix-free code such
that
$$E_{x \sim P} |c(x)| \leq H_2(P) + 1.$$ In this problem we will prove the second inequality.
We consider the case of a countably infinite set where
each element has nonzero probability and consider the following procedure for constructing a code.

\begin{quotation}
  \noindent Enumerate the elements of ${\cal X}$ as $x_1$, $x_2$, $x_3$, $\ldots$ in order of decreasing probability. \newline
  \newline
  Initialize the code to be empty (no $x_i$ is asigned any code)\newline
  \newline
  For $i = 1,2,3,\ldots$ assign an unused code $c(x_i)$ to $x_i$ such
  that $|c(x_i)| =\lceil-\log_2 P(x_i)\rceil$ and such that no preifx of that code word has been previously assigned.
\end{quotation}

Suppose we have defined code words $c(x_1)$, $\ldots$, $c(x_i)$ and
are trying to find a code word for $x_{i+1}$.

\medskip

{\bf (a)} Explain why no unassigned code word of length $\lceil
-\log_2 P(x_{i+1})\rceil$ can be a prefix of any previously assigned
code word.

\solution{For $j < i+1$ we have $P(x_j) \geq P(x_{i+1})$. Hence non of
  the previously assigned code can be longer than $\lceil -\log_2
  P(x_{i+1})\rceil$.  }

\medskip

{\bf (b)} Explain why there must exist an unallocated code word
$c(x_{i+1})$ satisfying the specified conditions.  Hint: Show that the probability of non-termination
for the procedure of problem 7 is nonzero.

{\bf problem 11.}

\solution{First, the probability of nontermination in the procedure
  defined by problem 3 must be nonzero.  This is because probability
  assigned to $x_i$ by the code can be no larger than than $P(x_i)$
  and hence the probability of returning any $x_j$ with $0 \leq j \leq
  i$ is strictly less than one.  Second, when the sampling procedure fails to terminate it
  must encoutern an unallocated code word satisfying the given conditions.}

\bigskip
{\bf Problem 12. The ELBO}
We consider a model distribution $Q_\Phi(z,y)$ with marginal distribution
$$Q_\Phi(y) = \sum_z Q_\Phi(z,y).$$
We are interested in minimizing the unconditional (or unsupervised) cross-entropy of this model.
$$\Phi^* = \argmin_\Phi E_{y \sim \mathrm{Train}} - \ln Q_\Phi(y)$$
For many models of interest $Q_\Phi(z,y)$ can be efficiently computed as $Q_\Phi(z)Q_\Phi(y|z)$ but $Q_\Phi(y)$ is intractable to compute.
In a variational auto-encoder we train a second model $\tilde{Q}_\Psi(z|y)$ and use the following inequality
\begin{eqnarray*}
\ln Q_\Phi(y) & \geq & \mathrm{ELBO}\\
& = & E_{z \sim \tilde{Q}(z|y)} \;\ln \frac{Q_\Phi(z,y)}{\tilde{Q}_\Psi(z|y)}
\end{eqnarray*}
Rather than minimize the cross entropy we can maximize the ELBO (the Evidence Lower BOund) which corresponds to minimizing an upper bound on the cross entropy.
Maximization of the ELBO with respect to model parameters $\Phi$ and $\Psi$ define a variational auto encoder (VAE).
We will consider this in much more detail later in the class.  For now we just consider the formal equations.

\bigskip
{\bf a.} The ELBO can be written as
$$\mathrm{ELBO} = E_{z \sim \tilde{Q}(z|y)} \;\ln \frac{Q_\Phi(y)Q_\Phi(z|y)}{\tilde{Q}_\Psi(z|y)}.$$
Here we have that the ELBO is the expectation of a log of a product of three terms. Separate all three terms and express
the terms other than $\ln Q_\Phi(y)$ as entropies or cross entropies.

\solution{
  \begin{eqnarray*}
    ELBO & = & E_{z \sim \tilde{Q}_\Psi(z|y)}\;\ln \frac{Q_\Phi(y)Q_\Phi(z|y)}{\tilde{Q}_\Psi(z|y)} \\
    \\
    & = & \left(E_{z \sim \tilde{Q}_\Psi(z|y)}\;\ln Q_\Phi(y)\right) +
    \left(E_{z \sim \tilde{Q}_\Psi(z|y)}\;\ln Q_\Phi(z|y)\right)
    + \left(E_{z \sim \tilde{Q}_\Psi(z|y)}\;\ln \frac{1}{\tilde{Q}_\Psi(z|y)}\right) \\
    & = & \ln Q_\Phi(y) - H(\tilde{Q}_\Psi(z|y),Q_\Phi(z|y)) +  H(\tilde{Q}(z|y))
  \end{eqnarray*}
}

\bigskip
{\bf b.}  Now rewrite the ELBO by separating it into one the term for $Q_\Phi(y)$ and one term for the other two combined and write the combined term
as a KL divergence.  Explain why your expression implies that the ELBO is a lower bound on $\ln Q_\Phi(y)$.

\solution{
  \begin{eqnarray*}
    ELBO & = & E_{z \sim \tilde{Q}_\Psi(z|y)}\;\ln \frac{Q_\Phi(y)Q_\Phi(z|y)}{\tilde{Q}_\Psi(z|y)} \\
    \\
    & = & \left(E_{z \sim \tilde{Q}_\Psi(z|y)}\;\ln Q_\Phi(y)\right) +
    \left(E_{z \sim \tilde{Q}_\Psi(z|y)}\;\ln \frac{Q_\Phi(z|y)}{\tilde{Q}_\Psi(z|y)}\right) \\
    & = & \ln Q_\Phi(y) - KL(\tilde{Q}_\Psi(z|y),Q_\Phi(z|y))
  \end{eqnarray*}

  The lower bound property follows from the fact that KL divergence is non-negative.
}


\end{document}
