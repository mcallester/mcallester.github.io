\input ../SlidePreamble
\input ../preamble


\begin{document}

{\Huge

  \centerline{\bf TTIC 31230, Fundamentals of Deep Learning}
  \bigskip
  \centerline{David McAllester, Autumn 2023}
  \vfill
  \centerline{\bf Generative Adversarial Networks (GANs)}
\vfill
\vfill


\slide{Modeling Probability Distributions on Images}

Suppose we want to train a model of the probability distribution of natural images using cross-entropy loss.

\vfill
$$\Phi^* = \argmin_\Phi \;E_{y \sim \popd}\;-\ln p_\Phi(y)$$

\vfill
Images are continuous stuctured objects --- a continuous value at every pixel.

\vfill
It is difficult to build probability models for images (or other continuous structured values)
that both accurately model the distribution and also allow us to calculate $p_\Phi(y)$.

\slide{Generative Adversarial Networks (GANs)}

GANs represent $p_\Phi(y)$ implicitly by constructing an image generator and abandon the ability to compute $p_\Phi(y)$.

\vfill
The cross-entropy loss is replaced by an adversarial discriminator which tries to distinguish between generated images and real images.

\anaslide{Representing a Distribution with a Generator}

\bigskip
\centerline{$z$\hspace{5in}$y_\Phi(z)$}
\centerline{\includegraphics[width=6in]{\images/generator1}}

\bigskip
The random input $z$ defines a probability density on images $y_\Phi(z)$.  We will write this as $p_\Phi(y)$ for the image $y$.

\anaslide{Representing a Distribution with a Generator}

\bigskip
\centerline{$z$\hspace{5in}$y_\Phi(z)$}
\centerline{\includegraphics[width=6in]{\images/generator1}}

\bigskip
We want $p_\Phi(y)$ to model a natural image distribution such as the distribution over human faces.


\anaslide{Representing a Distribution with a Generator}

\bigskip
\centerline{$z$\hspace{5in}$y_\Phi(z)$}
\centerline{\includegraphics[width=6in]{\images/generator1}}

\bigskip
We can sample from $p_\Phi(y)$ by sampling $z$.  But we cannot compute $p_\Phi(y)$ for $y$ sampled from the population.

\slide{Increasing Spatial Dimension}
 
Reducing spatial dimention with strided convolution:
\begin{tabbing}
For \= $x,y,j,\Delta x,\Delta y, i$ \\
\\
\>$L_{{\ell+1}}[{\color{red}x},{\color{red}y},j]\; \pluseq \;W[\Delta x, \Delta y, i, j] L_{{\ell}}[{\color{red} s*x} + \Delta x, {\color{red} s*y} + \Delta y, i]$
\end{tabbing}

\vfill
Increasing spatial dimension with PyTorch ConvTranspose2d:
\begin{tabbing}
For \= $x,y,j,\Delta x,\Delta y, i$ \\
\\
\>$L_{{\ell+1}}[{\color{red} s*x} + \Delta x, {\color{red} s*y} + \Delta y, i]\; \pluseq \;W[\Delta x, \Delta y, i, j]L_\ell[{\color{red}x},{\color{red}y},j]$
\end{tabbing}

\slide{Generative Adversarial Networks (GANs)}

Let $y$ range over images.  We have a generator $p_\Phi$. For $i \in \{-1,1\}$ we define a probability distribution over pairs
$\tuple{i,y}$ by
\begin{eqnarray*}
\tilde{p}_\Phi(i = 1) & = & 1/2 \\
\tilde{p}_\Phi(y|i=1) & =&  \popd(y) \\
\tilde{p}_\Phi(y|i=-1) & = & p_\Phi(y)
\end{eqnarray*}

\vfill
We also have a discriminator $P_\disc(i|y)$ that tries to determine the source $i$ given the image $y$.

\vfill
The generator tries to fool the discriminator.
\begin{eqnarray*}
        \gen^* & = & \argmax_\gen\;\;\min_\disc\;E_{\tuple{i,y} \sim \tilde{p}_\gen}\;-\ln P_\disc(i|y)
\end{eqnarray*}

\slide{GANs}

The generator tries to fool the discriminator.

\vfill
\begin{eqnarray*}
\gen^* & = & \argmax_\gen\;\;\min_\disc\;E_{\tuple{i,y} \sim \tilde{p}_\gen}\;-\ln P_\disc(i|y)
\end{eqnarray*}

\vfill
{\bf Assuming universality (next slide)} of both the generator $p_\gen$ and the discriminator $P_\disc$ we have {\color{red} $p_{\gen^*} = \popd$}.

\vfill
Note that this involves only discrete cross-entropy.


\slide{The Universality Assumption}

DNNs are universally expressive (can model any function) and trainable (the desired function can be found by SGD).

\vfill
Universality assumption is clearly false but is useful.

\vfill
The success of GANs (to the extent tht they have been successful) is a tribute to the utility of the universality ssumption.


\slide{Jensen-Shannon Divergence}

{\huge
\begin{eqnarray*}
  \gen^* & = & \argmax_\gen\;\;\min_\disc\;E_{\tuple{i,y} \sim \tilde{p}_\gen}\;-\ln P_\disc(i|y) \\
  \\
  \\
  & = & \argmin_\gen \; KL\left(\popd,\frac{\popd+p_\gen}{2}\right)+\;KL\left(p_\gen,\frac{\popd+p_\gen}{2}\right) \\
  \\
  \\
  \gen^* & = & \argmin_\gen \; {\color{red} \mathrm{JSD}(\popd,p_\gen)}
\end{eqnarray*}
}


\slide{GAN Mode Collapse}

A major concern is ``mode collapse'' where the learned distribution omits a significant fraction of the population distribution.

\vfill
There is no quantitative performance measure that provides a meaningful guarantee against mode collapse.

\vfill
In practice GANS are evaluated on FID score.

\slide{The Fr\'{e}nchet Inception Score (FID)}

Consider two distributions $P$ and $Q$ on the same set (perhaps distributions or densities on images).

\vfill
Generative image models are (still) evaluated using a certain meanure of the ``distance'' between the population distribution
and the generation distribution.

\vfill
For GANs we cannot compute the probability of a generated image so we cannot use cross entropy or KL-divergence.

\slide{The Fr\'{e}nchet Inception Score (FID)}

The Fr\'{e}nchet distance $F(P,Q)$ can be measure puresly by sampling.

$$\mathrm{F}(p,q) = \inf_{\mu}\;\left(E_{(x,y)\sim \mu} \;||x-y||^2\right)^{\frac{1}{2}}$$

\vfill
This is also known as the 2-Wasserstein distance.

\slide{The Fr\'{e}nchet Inception Score (FID)}

But rather than measure the $L_2$ distance between images we measure the $L_2$ distance between
the ``inception feature vectors'' $I(x)$ and $I(y)$.

\vfill
For an image $x$ the feature vector $I(x)$ is computed from a certain layer in the inception image classification network.

\slidetwo{Generative Adversarial Nets}{Goodfellow et al., June 2014}
\centerline{\includegraphics[width = 9in]{\images/GAN2014}}
The rightmost column (yellow boarders) gives the nearest neighbor in the training data to the adjacent column.

\slidetwo{Unsupervised Representation Learning ... (DC GANS)}
{Radford et al., Nov. 2015}

\centerline{\includegraphics[width = 9in]{\images/GANDCa}}

\slidetwo{Unsupervised Representation Learning ... (DC GANS)}
{Radford et al., Nov. 2015}

\centerline{\includegraphics[width = 9in]{\images/ImageFeatures}}

\slide{Interpolated Faces}

[Ayan Chakrabarti, January 2017]

\centerline{\includegraphics[height = 4.5in]{\images/interp}}

\slide{Conditional GANS}
In the conditional case we have a population distribution over pairs $\tuple{x,y}$.

\vfill
For conditional GANs we have a generator $p_\gen(y|x)$ and a discriminator $P_\disc(i|x,y)$
where $i = 1$ if $y$ the real ``label'' for $x$ and $-1$ if $y$ is generated from $x$.

{\color{red} $$\gen^* = \argmax_\gen\;\;\min_\disc\;E_{\tuple{x,y,i} \sim \tilde{p}_\gen}\;-\ln P_\disc(i|x,y)$$}

\slidetwo{Image-to-Image Translation (Pix2Pix)}
{Isola et al., Nov. 2016}

We assume a corpus of ``image translation pairs'' such as images paired with semantic segmentations.

\centerline{\includegraphics[width = 8.0in]{\images/cGAN0}}

\slide{UNets}

Pix2Pix uses a U-Net.

\vfill
U-Net: Convolutional Networks for Biomedical Image Segmentation, Ronneberger, Fischer and Brox, May 2015.

\vfill
U-Nets are fundamental to current diffusion models such as DALL$\cdot$E.

\slide{Adversarial Discrimination as an Additional Loss}

$$\gen^* = \argmin_\gen\;E_{(x,y) \sim \popd}\;\; ||y- y_\gen(x)||_1\; {\color{red} + \; \lambda\; {\cal L}_{\mathrm{Discr}}(\gen)}$$

\vfill
$${\cal L}_\mathrm{Discr}(\gen) = \max_\disc \;E_{x,y,i \sim \tilde{p}_\gen}\; \ln P_\disc(i|y,x)$$

\slide{Discrimination as an Additional Loss}

{\huge
$$\begin{array}{lrcl}
\mathrm{L1:} & \gen^* & = & \argmin_\gen\;E_{(x,y) \sim \popd}\;\; ||y - y_\gen(x)||_1 \\
\\
\\
\mathrm{cGAN:} & \gen^* & = & \argmin_\gen\;{\cal L}_{\mathrm{Discr}}(\gen) \\
\\
\\
\mathrm{L1 + cGAN:} & \gen^* & = & \argmin_\gen\;E_{(x,y) \sim \popd}\;\; ||y - y_\gen(x)||_1\; +\; \lambda\; {\cal L}_\mathrm{Discr}(\gen)
\end{array}$$
}


\slidetwo{Image-to-Image Translation (Pix2Pix)}
{Isola et al., Nov. 2016}

\centerline{\includegraphics[height = 4.5in]{\images/cGAN1}}

\slide{Arial Photo to Map and Back}

\centerline{\includegraphics[width = 8.0in]{\images/cGAN2}}

\slidetwo{Unpaired Image-to-Image Translation (Cycle GANs)}{Zhu et al., March 2017}

We have two corpora of images, say images of zebras and unrelated images of horses, or photographs and unrelated paintings by Monet.

\vfill
We want to construct translations between the two classes.

\centerline{\includegraphics[width = 8.0in]{\images/Cycle2}}

\slide{Cycle Gans}

\centerline{\includegraphics[width = 11.0in]{\images/Cycle3}}

\slide{Cycle Gans}

\centerline{\includegraphics[width = 6.0in]{\images/Cycle4}}

\slidetwo{Unsupervised Machine Translation (UMT)}
         {Lample et al, Oct. 2017, also Artetxe et al., Oct. 2017}


In unsupervised machine translation the cycle loss is called {\bf back-translation}.

\slidetwo{Feature Alignment by Discrimination}{Text to Speech (Saito et al. Sept. 2017)}

\centerline{\includegraphics[width = 2.0in]{\images/Txt2spchGAN}}

\vfill
Minimum Generation Error (MGE) uses {\color{red} perceptual distortion} ---
a distance between the feature vector of the generated sound wave and the
feature vector of the original.

\vfill
{\color{red}Perceptual Naturalness} can be enforced by a feature discrimination loss.

\slidetwo{Adversarial Discriminative Domain Adaptation}{Tzeng et al. Feb. 2017}

\centerline{\includegraphics[width = 4.0in]{\images/AdvDomainAdapt}}

A feature discrimination loss can be used to align source and target features.

\slide{Progressive GANs}

\centerline{Progressive Growing of GANs, Karras et al., Oct. 2017}

\centerline{\includegraphics[height = 4.5in]{\images/GANproga}}

\slide{Progressive GANs}

\centerline{\includegraphics[height = 4.5in]{\images/GANprogb}}

\slide{Early GANs on ImageNet}

\centerline{\includegraphics[height = 4.5in]{\images/BadGAN}}

\slide{BigGans}
\centerline{Large Scale GAN Training, Brock et al., Sept. 2018}
\centerline{\includegraphics[width = 9in]{\images/GANclass}}

\vfill
This is a class-conditional GAN --- it is conditioned on the imagenet class label.

\vfill
This generates 512 X 512 images without using progressive training.

\slide{StyleGAN}
{\Large A Style-Based Generator Architecture for Generative Adversarial Networks, Karras et al., Dec. 2018}

\centerline{\includegraphics[height= 4.8in]{\images/Style1}}

\slide{StyleGAN: Architecture}

\centerline{\includegraphics[height= 5.2in]{\images/StyleArch}}

\slide{StyleGAN: Style Transfer}

\centerline{\includegraphics[height= 5.2in]{\images/Style2}}

\slide{StyleGAN: Noise Variation}

\centerline{\includegraphics[height= 5.2in]{\images/Style3}}

\slide{StyleGAN2 and StyleGAN3}

StyleGan2 appeared in December of 2019 with significant improvements.

\vfill
It was demonstrated to work on many classes of images, not just faces.

\vfill
StyleGAN3 appeared in June 2021.

\slide{Projecting Images into Latent Space}

Given an image, can we find a noise vector (a latent vector) that generate a close approximation of
the given image.  Can we invert the generator?

\vfill
This appears to be possible with StyleGAN2 but not with the original even though StyleGAN2 produces better images.

\vfill
By measuring the match between an image $y$ and $g(g^{-1}(y))$ we can determine whether $y$ was generated by StyleGAN2.

\slide{June 2023: StyleGAN Seems to ``Understand'' Images}

\centerline{\includegraphics[height= 5.2in]{\images/StyleGANKnows}}

\slide{END}

}
\end{document}

\slide{END}

}
\end{document}
