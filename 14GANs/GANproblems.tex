\documentclass{article}
\input ../preamble
\parindent = 0em
\parskip = 1ex

\newcommand{\solution}[1]{}
%\newcommand{\solution}[1]{\bigskip {\color{red} {\bf Solution}: #1}}

\begin{document}


\centerline{\bf TTIC 31230 Fundamentals of Deep Learning}

\bigskip

\centerline{\bf Problems for GANs.}

\bigskip
\bigskip
~{\bf Problem 1. Conditional GANs}  In a conditional GAN
we model a conditional distribution $\pop(y|x)$ defined by a population distribution on pairs $\tuple{x,y}$.
For conditional GANs we consider the probability distribution over triples
$\tuple{x,y,i}$ defined by
\begin{eqnarray*}
\tilde{P}_\Phi(i = 1) & = & 1/2 \\
\tilde{P}_\Phi(y|x,i=1) & =&  \popd(y|x) \\
\tilde{P}_\Phi(y|x,i=-1) & = & p_\Phi(y|x)
\end{eqnarray*}

\medskip
(a)  Write the conditional GAN adversarial objective function for this problem in terms of $\tilde{P}(x,y,i)$, $P_\Phi(y|x)$ and $P_\Psi(i|y,x)$.

\solution{
$$\Phi^* = \argmax_\Phi\;\min_\Psi \;E_{x,y,i \sim \tilde{P}(x,y,i)}\;\;-\ln P_\Psi(i|x,y)$$
}
 
\bigskip
{\bf Problem 2. GAN instability}

Consider the following adversarial objective where $x$ and $y$ are scalars (real numbers).

$$\max_x\;\min_y \;xy$$

\medskip
(a) Write the differential equation for gradient flow of this adversarial objective.

\solution{
  \begin{eqnarray*}
    \frac{dx}{dt} & = & y \\
    \\
    \frac{dy}{dt} & = & -x
  \end{eqnarray*}
}

\medskip
(b) Give a general solution to your differential equation. (Hint: It goes in a circle).  You solution should have parameters
allowing for any given initial value of $x$ and $y$.

\solution{

  \begin{eqnarray*}
    x & = & r_0\sin(t + \Theta_0) \\
    \\
    y & = & r_0\cos(t + \Theta_0)
  \end{eqnarray*}
}

\bigskip
{\bf Problem 3. Contrastive GANs.}

A GAN can be built with a ``contrastive'' discriminator.  Rather than
estimate the probability that $y$ is from the population, the
discriminator must select which of $y_1,\ldots,y_N$ is from the
population.

\medskip
More formally, for $N \geq 2$ let $\tilde{P}_\Phi^{(N)}$ be the distribution on tuples $\tuple{i,y_1,\ldots,y_N}$ defined by drawing one
``positive'' from $\pop$ and $N-1$ IID negatives from $P_\Phi$; then
inserting the positive at a random position among the negatives; and
returning $(i,y_1,\ldots,y_N)$ where $i$ is the index of the positive.

$$\Phi^* = \argmax_\Phi \min_\Psi \;E_{(i,y_1,\ldots,y_{N+1}) \sim \tilde{P}_\Phi^{(N)}}\; - \ln p_\Psi(i|y_1,\ldots,y_{N+1}) \;\;\;(1)$$

\medskip
Restate the above definition of $\tilde{P}_\Phi^{(N)}$ and the GAN adversarial objective for the case of conditional constrastive GANs.

\solution{
  $$\Phi^* = \argmax_\Phi \min_\Psi \;E_{(i,y_1,\ldots,y_{N+1},x) \sim \tilde{P}^{(N)}_\Phi} \ln - P_\Psi(i|y_1,\ldots,y_{N+1},x)$$
}

\bigskip 

{\bf Problem 4. Reshaping Noise in GANs.} A GAN generator is typically given a random noise vector $z \sim {\cal N}(0,I)$.  Give equations defining a method for computing $z'$ from $z$ such that
the distribution on $z'$ is a
mixture of two Gaussians each with a different mean and diagonal covariance matrix.  Hint: use a step-function threshold on the first component of $z$ to compute a binary value and use the other components
of $z$ to define the Gaussian variables.

\solution{
  \begin{eqnarray*}
    y & = & \mathbf{1}[z[0] \geq 0] \\
    \\
    z' & = & y(\mu_1 + \sigma_1 \odot z[1:d]) + (1-y)(\mu_2 + \sigma_2 \odot z[1:d])
  \end{eqnarray*}
}
    

\bigskip
{\bf Problem 5.}  This problem is on GAN language modeling.  A GAN takes noise as input and transforms it to an output.  We consider the case where the output is a string of symbols $w_1,\ldots, w_T$
where for simplicity we always generate a string of exactly length $T$ and where the words are integers with $w_t \in \{0,\ldots,I-1\}$ where $I$ is the size of the vocabulary.
The GAN parameters are just the parameters of a bigram model, i.e., the parameters are probability tables

\begin{eqnarray*}
  P[i] & = & P(w_1 = i) \\
  \\
  Q[i,j] & = & P(w_{t+1} = j\;|\; w_t = i)
\end{eqnarray*}

We take the noise input to the GAN to be a sequence of random real numbers $\epsilon_1,\ldots,\epsilon_T$ where each $\epsilon_t$ is drawn uniformly from the interval $[0,1]$.

{\bf (a)} Write a function $\hat{w}(P[I],\epsilon_1)$ which deterministically returns the first word given the noise value $\epsilon_1$ such that the probability over the draw of $\epsilon_1$
that $\hat{w}(P[I],\epsilon_1) = i$ is $P[i]$.

\solution{
  We can take $\hat{w}(P[I],\epsilon_1)$ to be the unique $i$ such that $\epsilon_1 \in \left[\left(\sum_{j<i} P[j]\right),\;\left(\sum_{j \leq i} \;P[j]\right)\right]$
}

{\bf (b)} Write a function $\hat{w}(Q[I,I],w_t,\epsilon_t)$ which deterministically returns the word $w_{t+1}$ given $w_t$ such that the probability over the draw of $\epsilon_t$
that $\hat{w}(Q[I,I],w_t,\epsilon_t) = j$ is $Q[w_t,j]$.

\solution{
  We can take $\hat{w}(Q[I,I],w_t,\epsilon_t)$ to be the unique $w_j$ such that $\epsilon_t \in \left[\left(\sum_{j<i} Q[w_t,j]\right),\;\left(\sum_{k \leq j} \;Q[w_t,j]\right)\right]$
}

{\bf (c)} There is a problem with this GAN.  For string generated by the GAN we need to back-propagate the discriminator loss into the GAN generator parameters.  Explain why this is problematic.
Is this always problematic when the generator output is discrete?

\solution{Yes, there is a problem whever $s$ is discrete. A discrete output will not change under differential updates to the GAN parameters.  Hence the gradient of the discriminator loss
  with respect to the generator parameters is zero.  This will happen for any GAN generatng a discrete output. While there are approaches one can try for discrete GANs, GANs are most effective for modeling
  continuous objects like sounds and images.  It does not help to have the GAN sample from a transformer model. To get a gradient on the generator parameters we need a gradient of the discriminator loss with
  respect to a continuous signal $s$ being generated by the generator.}


\bigskip
{\bf Problem 2. 25 pts}  This problem is on GAN language modeling.  A GAN takes noise as input and transforms it to an output.  We consider the case where the output is a string of symbols $w_1,\ldots, w_T$
where for simplicity we always generate a string of exactly length $T$ and where the words are integers with $w_t \in \{0,\ldots,I-1\}$ where $I$ is the size of the vocabulary.
The GAN parameters are just the parameters of a bigram model, i.e., the parameters are probability tables

\begin{eqnarray*}
  P[i] & = & P(w_1 = i) \\
  \\
  Q[i,j] & = & P(w_{t+1} = j\;|\; w_t = i)
\end{eqnarray*}

We take the noise input to the GAN to be a sequence of random real numbers $\epsilon_1,\ldots,\epsilon_T$ where each $\epsilon_t$ is drawn uniformly from the interval $[0,1]$.

{\bf (a)} Write a function $\hat{w}(P[I],\epsilon_1)$ which deterministically returns the first word given the noise value $\epsilon_1$ such that the probability over the draw of $\epsilon_1$
that $\hat{w}(P[I],\epsilon_1) = i$ is $P[i]$.

\solution{
  We can take $\hat{w}(P[I],\epsilon_1)$ to be the unique $i$ such that $\epsilon_1 \in \left[\left(\sum_{j<i} P[j]\right),\;\left(\sum_{j \leq i} \;P[j]\right)\right]$
}

{\bf (b)} Write a function $\hat{w}(Q[I,I],w_t,\epsilon_t)$ which deterministically returns the word $w_{t+1}$ given $w_t$ such that the probability over the draw of $\epsilon_t$
that $\hat{w}(Q[I,I],w_t,\epsilon_t) = j$ is $Q[w_t,j]$.

\solution{
  We can take $\hat{w}(Q[I,I],w_t,\epsilon_t)$ to be the unique $w_j$ such that $\epsilon_t \in \left[\left(\sum_{j<i} Q[w_t,j]\right),\;\left(\sum_{k \leq j} \;Q[w_t,j]\right)\right]$
}

{\bf (c)} There is a problem with this GAN.  For string generated by the GAN we need to back-propagate the discriminator loss into the GAN generator parameters.  Explain why this is problematic.
Is this always problematic when the generator output is discrete?

\solution{Yes, there is a problem whever $s$ is discrete. A discrete output will not change under differential updates to the GAN parameters.  Hence the gradient of the discriminator loss
  with respect to the generator parameters is zero.  This will happen for any GAN generatng a discrete output. While there are approaches one can try for discrete GANs, GANs are most effective for modeling
  continuous objects like sounds and images.  It does not help to have the GAN sample from a transformer model. To get a gradient on the generator parameters we need a gradient of the discriminator loss with
  respect to a continuous signal $s$ being generated by the generator.}

\ignore{
~{\bf Problem 4. Jensen-Shannon Divergence}
Consider a population distribution $\pop$ and subset $S$ of its support. Let $\pop_S$ be the restriction of $\pop$ to the set $S$.
\begin{eqnarray*}
  \pop_S(y) & = & \frac{1}{\pop(S)}\left\{\begin{array}{ll} \pop(y) & \mbox{for $y \in S$} \\ 0 & \mbox{otherwise} \end{array}\right.
\end{eqnarray*}
We also consider the Jensen-Shannon divergence
$$\mathrm{JS}(P,Q) = \frac{1}{2}\left(\mathrm{KL}\left(P,\frac{P+Q}{2}\right) + \mathrm{KL}\left(Q,\frac{P+Q}{2}\right)\right)$$
Where for distributions $P$ and $Q$ we define $P+Q$ by the equation $(P+ Q)(y) = P(y)+Q(y)$.

\medskip
(a) Solve for
$$\mathrm{KL}\left(\pop_S,\frac{\pop+\pop_S}{2}\right)$$
in terms of the probability mass $\pop(S)$ of the set $S$.  Your solution should have the form
$\ln 2 - \ln (1 + \epsilon)$ where $\epsilon$ is a function of $P(S)$ with $0 \leq \epsilon \leq 1$.

\solution{
  \begin{eqnarray*}
    \mathrm{KL}\left(\pop_S,\frac{\pop+\pop_S}{2}\right) & = & E_{y \sim \pop_S}\;\ln \frac{2\pop_S(y)}{\pop(y) + \pop_S(y)} \\
    \\
    & = & E_{y \sim \pop_S}\;\ln \frac{\frac{2\pop(y)}{\pop(S)}}{\pop(y) + \frac{\pop(y)}{\pop(S)}} \\
    \\
    & = & \ln \frac{2}{\pop(S) + 1} \\
    \\
    & = & \ln 2 - \ln (1 + P(S)) \\
  \end{eqnarray*}
}

(b) Solve for
$$\mathrm{KL}\left(\pop,\frac{\pop+\pop_S}{2}\right)$$
in terms of the probability mass $\pop(S)$ of the set $S$.  Your solution should have the form $\ln 2 - \epsilon \ln (\epsilon + 1)/\epsilon$
for $\epsilon$ a function of  $P(S)$ with $0 \leq \epsilon \leq 1$.
Hint: Write the KL-divergence as $P(S)E_{y \sim P(y|y\in S)} [\ldots] + (1-P(S))E_{y \sim \pop(y|y \not \in S)}\;[\ldots]$.

\solution{
  \begin{eqnarray*}
    & & \mathrm{KL}\left(\pop,\frac{\pop+\pop_S}{2}\right) \\
    \\
    & = & E_{y \sim \pop}\;\ln \frac{2\pop(y)}{\pop(y) + \pop_S(y)} \\
    \\
    & = & \pop(S)E_{y \sim \pop(y|y \in S)}\;\ln \frac{2\pop(y)}{\pop(y) + \frac{\pop(y)}{\pop(S)}}
    +(1-\pop(S))E_{y \sim \pop(y|y \not \in S)} \ln \frac{2\pop(y)}{\pop(y)} \\
    \\
    & = & \pop(S)\;\ln \frac{2\pop(S)}{\pop(S)+1} + (1- \pop(S)) \ln 2 \\
    \\
    & = & \ln 2 - \pop(S)\;\ln \frac{\pop(S)+1}{\pop(S)} 
  \end{eqnarray*}
}

\medskip
(c) For $\epsilon << 1$ we have $\ln(1+ \epsilon) \approx \epsilon$ and $\ln(1+ \epsilon)/\epsilon \approx \ln(1/\epsilon)$.  Apply these approximation
to get an approximate value for $\mathrm{JS}(\pop_S,\pop)$ for small values of $P(S)$.

\solution{
  \begin{eqnarray*}
    \mathrm{JS}(\pop_S,\pop,Q) & =  &\frac{1}{2}\left(\mathrm{KL}\left(\pop_S,\frac{\pop_S+\pop}{2}\right) + \mathrm{KL}\left(\pop,\frac{\pop_S+\pop}{2}\right)\right) \\
    \\
    & \approx & \frac{1}{2}\left(\ln 2 - \epsilon + \ln 2 - \epsilon \ln \frac{1}{\epsilon}\right) \\
    \\
    & = & \ln 2 - \frac{\epsilon}{2}\left(1 + \ln \frac{1}{\epsilon}\right) \\
    \\
    \epsilon & = & \pop(S)
  \end{eqnarray*}
}
}
\end{document}
