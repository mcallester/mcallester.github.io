\documentclass{article}
\input ../preamble
\parindent = 0em

%\newcommand{\solution}[1]{}
\newcommand{\solution}[1]{\bigskip {\color{red} {\bf Solution}: #1}}

\begin{document}


\centerline{\bf TTIC 31230 Fundamentals of Deep Learning}
\bigskip
\centerline{\bf RL Problems.}


\bigskip
{\bf Problem 1. REINFORCE for BLEU Translation Score.}
Consider training machine translation on a corpus of translation pairs $(x,y)$ where $x$ is, say, an English sentence $x_1,\ldots,\mathrm{EOS}$ and $y$ is a French sentence $y_1,\ldots,\mathrm{EOS}$
where EOS is the ``end of sentence'' tag.

\medskip
Suppose that we have a parameterized autoregressive model defining $P_\Phi(y_t|x,y_1,\ldots,y_{t-1})$ so that $P_\Phi (y_1,\ldots,y_{T}|x) = \prod_{t=1}^{T'}P_\Phi(y_t|x,y_1,\ldots,y_{t-1})$ where $y_T$ is EOS.

\medskip
For a sample $\hat{y}$ from $P_\Phi(y|x)$ we have a non-differentiable BLEU score $\mathrm{BLEU}(\hat{y},y) \geq 0$ that is not computed until the entire output $y$ is complete
and which we would like to maximize.

\medskip
(a) Give an SGD update equation for the parameters $\Phi$ for the REINFORCE algorithm for maximizing $E_{\hat{y} \sim P_\Phi(y|x)}$ for this problem.

\solution{
  For $\tuple{x,y}$ samples form the training corpus of translation pairs, and for $\hat{y}_1,\ldots,\hat{y}_T$ sampled from $P_\Phi(\hat{y}|x)$
  we update $\Phi$ by
  $$ \Phi \; \pluseq \; \eta\mathrm{BLEU}(\hat{y},y)\sum_{t = 1}^T \; \nabla_\Phi\;\ln P_\Phi(\hat{y}_t|x,\hat{y}_1,\ldots,\hat{y}_{t-1})$$
  Samples with higher BLEU scores have their probabilities increased.
  }

\medskip
(b) Suppose that somehow we reach a parameter setting $\Phi$ where $P_\Phi(y|x)$ assigns probability very close to 1 for a particular translation $\hat{y}$
so that in practice we will always sample the same $\hat{y}$.  Suppose that this translation $\hat{y}$ has less than optimal BLEU score.
Can the REINFORCE algorithm recover from this situation and consider other translations?  Explain your answer.

\solution{No.  The REINFORCE algorithm will not recover. The update will only increase the probability of the single translation which it always selects.
A deterministic policy has zero gradient and is stuck.}

\medskip
(c) Modify the REINFORCE update equations to use a value function approximation $V_\Phi(x)$ to reduce the variance in the gradient samples and where
$V_\Phi$ is trained by Bellman Error.
Your equations should include updates to train
$V_\Phi(x)$ to predict $E_{\hat{y} \sim P(y|x)}\;\mathrm{BLEU}(\hat{y},y)$.  (Replace the reward by the ``advantage'' of the particular translation).

\solution{
  For $\tuple{x,y}$ sampled form the training corpus of translation pairs, and for $\hat{y}_1,\ldots,\hat{y}_T$ sampled from $P_\Phi(\hat{y}|x)$
  we udate $\Phi$ by
  \begin{eqnarray*}
    \Phi & \pluseq & \eta(\mathrm{BLEU}(\hat{y},y)-V_\Phi(x))\sum_{t = 1}^T \; \nabla_\Phi\;\ln P_\Phi(\hat{y}_t|x,\hat{y}_1,\ldots,\hat{y}_{t-1}) \\
    \\
    \Phi & \minuseq & \eta \nabla_\Phi (V_\Phi(x) - \mathrm{BLEU}(\hat{y},y))^2  \;=\; 2\eta(V_\Phi(x) - \mathrm{BLEU}(\hat{y},y))\nabla_\Phi V_\Phi(x)
    \end{eqnarray*}
}

\bigskip
~{\bf Problem 2. Rapid Mixing for Asymptotic Avergage Reward.}

\medskip
We consider a case where we are interested in asymptotic average reward.

$$R(\pi) \; = \;\lim{T \rightarrow \infty}\;\frac{1}{T} \sum_{t=1}^T r_t$$

\vfill
For a given policy $\pi$ we have a Markov process over states --- a well defined state transition probability $P_\pi(s_{t+1}|s_t)$ defined by
$$P_\pi(s_{t+1}|s_t) \;=\;\sum_a \pi(a|s_1)P_\pi(s_2|s_1,a)$$
Under very mild conditions a Markov process has a well define stationary distribution on states which we will write $P_\pi(s)$.  This distribution is ``stationary''
in the sense that
$$\sum_{s_1} P_\pi(s_1)P_\pi(s_2|s_1) \;=\;P_\pi(s_2)$$

(a) Write the asymptotic average reward $R(\pi)$ in terms of the stationary distribution $P_\pi$, the policy $\pi(a|s)$ and the reward function $R(s,a)$

\solution{
  $$R(\pi) = E_{s \sim P_\pi(s),\;a \sim \pi(a|s)} \;R(s,a)$$
}

(b) Now for $\Delta t >1$ we define $P_\pi(s_{t+\Delta t}|s_t)$ recursively as by

$$P_\pi(s_{t+\Delta t}|s_t) = \sum_{s_{t+\Delta t - 1}}\;P_\pi(s_{t+\Delta t-1}|s_t)P_\pi(s_{t+\Delta t}|s_{t+\Delta t -1})$$

We now assume a ``mixing parameter'' $0 < \gamma < 1$ for $\pi$ defined by the property

$$\sum_{s_{t + \Delta t}}\;|P_\pi(s_{t+\Delta t}|s_t) - P_\pi(s_{t+\Delta t})| \leq \gamma^{\Delta t}$$

We now define an advantage function on state-action pairs to be the ``extra'' reward we get by taking action $a$ (rather than drawing from $\pi(a|s)$) summed over all time.

$$A(s,a) = E\;\sum_{t=0}^\infty (r_t - R(\pi)) \;\;|\;\;s_0 = s,\;a_0 = a$$

Assuming the reward is bounded by $r_\mathrm{max}$ and that we have the above mixing parameter $\gamma$, give a (finite) upper bound on the infinite sum $A(s,a)$.

\solution{
  \begin{eqnarray*}
    & & E\;r_{t} - R(\pi) \;\;|s_0 = s,\;a_0 = a, t> 0 \\
    \\
    & = & \left(\sum_{s_{t}} P_\pi(s_{t}|s_0) E_{a \sim \pi(a|s_{t})}\;R(s_{t},a)\right) - R(\pi) \\
    \\
    & = & \left(\sum_{s_{t}}\;(P_\pi(s_{t}) + P_\pi(s_{t}|s_0) - P_\pi(s_{t})) E_{a \sim \pi(a|s_{t})}\;R(s_{t},a)\right) - R(\pi) \\
    \\
    & = & R(\pi) +\left(\sum_{s_{t}}\; (P_\pi(s_{t}|s_0) - P_\pi(s_{t})) E_{a \sim \pi(a|s_{t})}\;R(s_{t},a)\right) -R(\pi)\\
    \\
    & = & \sum_{s_{t}}\; (P_\pi(s_{t}|s_0) - P_\pi(s_{t})) r_\mathrm{max} \\
    \\
    & \leq &  r_\mathrm{max} \gamma^t \\
    \\
    A(s,a) & \leq & r_\mathrm{max} \sum_{t=0}^\infty \gamma^t = \frac{r_\mathrm{max}}{1-\gamma}
  \end{eqnarray*}
}

It can be shown that

$$\nabla_\Phi R(\pi_\Phi) = E_{s \sim P_\pi(s),\;a\sim \pi(a|s)}\;\nabla_\Phi \ln \pi_\Phi(a|s) \;A(s,a)$$

You do not have to prove this.


\bigskip

{\bf Problem 3: How Advantage-Actor-Critic (A2C) Reduces Variance.}  This problem will consider a simple artificial example that demonstrates the power of the advantage actor-critic algorithm.
We start with policy gradient theorems for the episodic case.

$$\begin{array}{lrcl}
  \mathrm{REINFORCE:} &\nabla_\Phi R(\pi) & =  &E_{s_0,a_0,\ldots,s_T,a_T \sim \pi_\Phi}\;\;\sum_{t=0}^T (\nabla_\Phi \ln \pi_\Phi(a_t|s_t))\left(\sum_{t' = t}^T R(s_t,a_t)\right) \\
  \\
  \mathrm{A2C:} &\nabla_\Phi R(\pi) & =  &E_{s_0,a_0,\ldots,s_T,a_T \sim \pi_\Phi}\;\;\sum_{t=0}^T (\nabla_\Phi \ln \pi_\Phi(a_t|s_t))\;(Q^\pi(s_t,a_t) - V^\pi(s_t)) \\
  \\
  & V^\pi(s) & = & E_{s_0,a_0,\ldots,s_T,a_T \sim \pi_\Phi \;|s_0=s,}\;\;\sum_{t=0}^T R(s_t,a_t) \\
  \\
  & Q^\pi(s,a) & = & E_{s_0,a_0,\ldots,s_T,a_T \sim \pi_\Phi \;|s_0=s,\;a_0 = a}\;\;\sum_{t=0}^T R(s_t,a_t)
\end{array}$$

In practice we use approximators $V_\Psi(s)$ and $Q_\Theta(s,a)$
for $V^\pi(s)$ and $Q^\pi(s,a)$. But we will ignore that here and just consider $V^\pi(s)$ and $Q^\pi(s,a)$ as defined above for which the above equations are exactly true.

\medskip
We consider an MDP where we want to get to a goal state as quickly as possible.
We consider an MDP where we have two actions $a_1$ and $a_2$ and a policy is just
a biased coin flip between $a_1$ and $a_2$ independent of the state.  We also suppose that $a_1$ always fails to advance, that $a_2$ always advances by one, and that we reach a goal
as soon as we have advances $N$ times. When we reach the goal, but only when we reach the goal, we get reward $-T$ (or equivalently, cost $T$)
where $T$ is the number of actions taken to reach the goal.  Of course the best policy is to always pick $a_2$ which gives $N$ advancements in $N$ actions getting reward $-N$ (cost $N$).
We define a state $s$ to be the pair $(i,t)$ where $t$ is the number of actions taken so far and $i$ is the number of advancements made so far.

\medskip
(a) Define the state $s_{t+1}$ as a function of $s_t$ and $a_t$.
The state transition is deterministic so don't worry about formulating this as a probability.  Just say what the next state is in terms of the previous state and the action.

\solution{ Let $(i,t)$ be the state $s_t$.  If the action is $a_1$ then $s_{t+1} = (i,t+1)$ and if the action is $a_2$ and $s_{t+1} = (i+1,t+1)$.}

\medskip
(b) If the stochastic policy picks $a_2$ with probability $\lambda$ then the expected number actions taken to advance 1 step is $\sum_{t = 1}^\infty \lambda(1-\lambda)^{t-1}t = \frac{1}{\lambda}$.
Use this fact to give an expression for $V^\pi(i,t)$. (Remember that the reward, given only at the end, is $-T$).

\solution{From state $(i,t)$ reward occurs when we have made $N-i$ additional advances.  The expected time for each advance is $\frac{1}{\lambda}$.  So the expected value of $T$ given state $(i,t)$
  is $t +\frac{N-i}{\lambda}$ and so $V^\pi(i,t) = - (t + (N-i)/\lambda)$
  }

\medskip
(c) It can be shown that $Q^\pi(s,a) = R(s,a) + E_{s'\;| s,a} V^\pi(s')$.  Use this and your result from (b) to give an analytic expressions for $Q^\pi(s,a)$ and the advantage $A^\pi(s,a) = Q^\pi(s,a) - V^\pi(i,t)$
for $a = a_1$ and $a= a_2$.

\solution{
  In this example, $R(s,a) = 0$ except at the end where $R((N-1,t),a_2) = -(t+1)$.  So in all but the last step we have $Q^\pi(s,a) = V^\pi(s')$ where $s'$ is the next state.
  \begin{eqnarray*}
    Q^\pi((i,t)a_1) & = & V^\pi(i,t+1) \;=\; -(t+1  + (N-i)/\lambda)\\
    \\
    Q^\pi((i,t),a_2) & = & V^\pi(i+1,t+1) \;=\; -(t+1 + (N-i-1)/\lambda) \\
    \\
    V^\pi(i,t) & = & -(t + (N-i)/\lambda) \\
    \\
    A^\pi((i,t,),a_1) & = & -1 \\
    \\
    A^\pi((i,t,),a_1) & = & \frac{1}{\lambda} -1
  \end{eqnarray*}
  It turns out that these equations handle the last step as well.
  }

\medskip
(d) Policy gradient adjusts the probability $\lambda$ of selecting $a_2$.  It is possible that for some samples of runs the REINFORCE algorithm decreases $\lambda$
--- it moves the policy in the wrong direction.  For example, this happens when $\lambda = 1/2$ and there happens to be are more occurrences of $a_2$ than $a_1$ (note that the reward is always negative).
But the reward (or cost) is correlated with the number of occurrences of $a_1$ so the expected update is still correct.

\medskip
Given your answer to (c), is it possible that the A2C update ever reduces $\lambda$? Explain your answer.

\solution{In this example A2C always increases $\lambda$. The update on $\lambda$ is the sum of the updates for each time step.
  In this case each time step behaves independently because the advantage is determined by the action taken at that time.
  At each time step we have that if the action selected is $a_1$ then the advantage is negative which decreases the probability of $a_1$ and hence increases the probability of $a_2$.
  When the action selected is $a_2$ the advantage is positive and the probability of $a_2$ is again increased.}


\end{document}
