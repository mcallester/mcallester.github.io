\input ../../SlidePreamble
\input ../../preamble

\begin{document}

{\Huge

  \centerline{\bf TTIC 31230, Fundamentals of Deep Learning}
  \bigskip
  \centerline{David McAllester, Autumn 2020}
\vfill
  \centerline{\bf Reinforcement Learning}
  \vfill
  \centerline{\bf Value Iteration}
  \vfill
\vfill

\slide{Definition of Reinforcement Learning}

RL is defined by the following properties:

\vfill
\begin{itemize}
\item An environment with {\bf state}.

  \vfill
\item State changes are influenced by {\bf sequential decisions}.

  \vfill
\item  Reward (or loss) depends on {\bf making decisions} that lead to {\bf desirable states}.
\end{itemize}

\slide{Reinforcement Learning Examples}

\begin{itemize}
\item Board games (chess or go)

  \vfill
\item Atari Games (pong)

  \vfill
\item Robot control (driving)

  \vfill
\item Dialog

  \vfill
\item Life

\end{itemize}

\slide{Policies}

A policy is a way of behaving.

\vfill
Formally, a (nondeterministic) policy maps a state to a probability distribution over actions.

\vfill
\begin{eqnarray*}
    \pi(a_t|s_t) & & \mbox{probability of action $a_t$ in state $s_t$}
\end{eqnarray*}

\slide{Imitation Learning}

Construct a training set of state-action pairs $(s,a)$ from experts.

\vfill
Define stochastic policy $\pi_\Phi(s)$.

\vfill
$$\Phi^* = \argmin_\Phi \expectsub{(s,a) \sim \mathrm{Train}}{- \ln \pi_\Phi(a\;|\;s)}$$

\vfill
This is just cross-entropy loss where we think of $a$ as a ``label'' for $s$.

\slide{Dangers of Imperfect Imitation Learning}

Perfect imitation learning would reproduce expert behavior.

Imitation learning is {\bf off-policy} ---
the state distribution in the training data is different from that defined by the policy being learned.

\vfill
Imitating experts, such as expert fire eaters, can be dangersous.  ``Don't try this at home''.

\vfill
Also, it is difficult to exceed expert performance by imitating experts.  But this can happen.

\slide{Markov Decision Processes (MDPs)}

An MDP consists of a set ${\cal S}$ of states, a set ${\cal A}$ of allowed actions, a reward function $R$
and a next-state probability function $P_T$.  We will use the following notation.

\vfill
$s_t \in {\cal S}$ is the state at time $t$

\vfill
$a_t \in {\cal A}$ is the action taken at time $t$.

\vfill
$r_t = R(s_t,a_t) \in \mathbb{R}$ is the reward at time $t$

\vfill
$P_T(s_{t+1}|s_t,a_t)$ is the probability of $s_{t+1}$ given $s_t$ and $a_t$.

\vfill
The function $R(s,a)$ can allow for a cost of the action $a$.

\slide{Optimizing Reward}

In RL we maximize reward rather than minimize loss.

$$\pi^* = \argmax_\pi \;R(\pi)$$

\vfill
$$\begin{array}{rcll}
  R(\pi) & = & E_\pi\;\sum_{t=0}^T \;r_t &\mbox{episodic reward (go)} \\
\\
& \mbox{or} & E_\pi\;\sum_{t=0}^\infty \;\gamma^t r_t &\mbox{discounted reward (financial planning)} \\
\\
& \mbox{or} & \lim_{T \rightarrow \infty}\;\frac{1}{T} \;\sum_{t=0}^T \;r_t &\mbox{asymptotic average reward (driving)}
\end{array}$$


\slide{The Value Function}

For discounted reward:

\begin{eqnarray*}
  V^\pi(s) & = & E_\pi\;\sum_t \;\gamma^t r_t  \;\;|\; \pi, \; s_0 = s \\
  \\
  V^*(s) & = & \sup_\pi \;V^\pi(s) \\
  \\
  \pi^*(a|s) & = & \argmax_a\;R(s,a) + \gamma \expectsub{s' \sim P_T(s'|s,a)}{V^*(s')} \\
  \\
  V^*(s) & = & \max_a\; R(s,a) + \gamma \expectsub{s' \sim P_T(s'|s,a)}{V^*(s')}
\end{eqnarray*}

\slide{Value Iteration}

Suppose the state space and action space are finite.

\vfill
In that case we can do value iteration.

\begin{eqnarray*}
  V_0(s) & = & 0 \\
  \\
  V_{i+1}(s) & = & \max_a\; R(s,a) + \gamma \expectsub{s' \sim P_T(\cdot|s,a)}{V_i(s')}
\end{eqnarray*}

\vfill
If all rewards are non-negative then

$$V_{i+1}(s) \geq V_i(s)\;\;\;\;V_i(s) \leq V^*(s)\;\;\;\;\mathrm{so}\;\lim_{i \rightarrow \infty}\;V_i(s)\;\mathrm{exists}$$

\slide{Value Iteration}

Theorem: {\bf For discounted reward}
\vfill
$$V_\infty(s) \doteq \lim_{i \rightarrow \infty}\;V_i(s) = V^*(s)$$

\slide{Proof}

\begin{eqnarray*}
  \;\Delta & \doteq & \max_s\;\;V^*(s) - V_\infty(s) \\
  \\
  & = & \max_s \left(\begin{array}{l} \max_a R(s,a) + E_{s'|a} \gamma V^*(s') \\ - \max_a R(s,a) + E_{s'|a} \gamma V_\infty(s')\end{array}\right) \\
  \\
    & \leq & \max_s \max_a \left(\begin{array}{l} R(s,a) + E_{s'|a} \gamma V^*(s') \\ - R(s,a) + E_{s'|a} \gamma V_\infty(s')\end{array}\right) \\
  \\
  & = & \max_s \max_a \;E_{s'|a}\; \gamma (V^*(s') - V_\infty(s)) \\
  \\  
  & \leq & \gamma \Delta
\end{eqnarray*}


\slide{END}

}
\end{document}


