\input ../SlidePreamble
\input ../preamble

\begin{document}

{\Huge

  \centerline{\bf TTIC 31230, Fundamentals of Deep Learning}
  \bigskip
  \centerline{David McAllester}
  \vfill
  \centerline{Archiectural Elements That Improve SGD}
  \vfill
  \vfill
  \centerline{ReLu}
  \vfill
  \centerline{Initialization}
  \vfill
  \centerline{Batch Normalization}
  \vfill
  \centerline{Residual Networks}
  \vfill
  \centerline{Gated RNNs}

\slide{DNNs are Expressive and Trainable}

{\color{red} Universality Assumption:} DNNs are universally expressive (can model any function) and trainable (the desired function can be found by SGD).

\vfill
Universal trainability is clearly false but can still guide architecture design.

\vfill
Equally expressive architectures differ in trainability.

\vfill
We will consider architectural elements that improve SGD.

\vfill
Very deep networks are particularly difficult for SGD.

\slide{Activation Function Saturation}

$\mathrm{Relu}(x) = \max(x,0)$

\vfill
\centerline{\includegraphics[width= 4.0in]{../images/relu}}

\vfill
The Relu does not saturate at positive inputs.

\slide{Repeated Multiplication by Network Weights}

Consider

\vfill
$$y = \sum_i w[i]x[i]  \;\;\;= \;\;\; W[I]x[I]$$

\vfill
If the weights are large the activations will grow exponenetially in depth.

\vfill
If the weights are small the actvations will become exonentially small.

\slide{Repeated Multiplication by Network Weights}

Exploding activations cause exploding gradients.

\begin{eqnarray*}
y  & \pluseq & w[i]x[i] \\
\\
w.grad & \pluseq & y.grad \; x[i]
\end{eqnarray*}

\vfill
The size of $w[i].\grad$ is proportional to $x[i]$

\slide{He Initialization}

Initialize weights to preserve zero-mean unit variance distributions values.

$$y = \sum_i w[i]x[i]$$

\vfill
If we assume $x_i$ has zero mean and unit variance then we want $y$
to have zero mean and unit variance (over random training points).

\vfill
He initialization randomly draws $w[i]$ from

$${\cal N}(0,\sigma^2)\;\;\;\sigma = \sqrt{1/N}$$

\slide{He Initialization}

$$y = \sum_i w[i]x[i]$$

\vfill
$$w[i] \sim {\cal N}(0,\sigma^2)\;\;\;\sigma = \sqrt{1/N}$$

\vfill
Assuming independence we have that $y$ has zero mean and unit variance.

\slide{Batch Normalization}
Given a tensor $x[b,j]$ we define $\tilde{x}[b,j]$ as follows.

\begin{eqnarray*}
  \hat{\mu}[j] & = & \frac{1}{B} \sum_b\;x[b,j] \\
  \\
  \\
  \hat{\sigma}[j] & = & \sqrt{\frac{1}{B-1} \sum_b (x[b,j]-\hat{\mu}[j])^2} \\
  \\
  \\
  \tilde{x}[b,j]& = & \frac{x[b,j] - \hat{\mu}[j]}{\hat{\sigma}[j]}
\end{eqnarray*}


\vfill
At test time a single fixed estimate of $\mu[j]$ and $\sigma[j]$ is used.

\slide{Spatial Batch Normalization}

For CNNs we convert a tensor $L[b,x,y,n]$ to $\tilde{L}[b,x,y,n]$ as follows.

\begin{eqnarray*}
  \hat{L}[n] & = & \frac{1}{BXY} \sum_{b,x,y}\;L[b,x,y,n] \\
  \\
  \\
  \hat{\sigma}[n] & = & \sqrt{\frac{1}{BXY-1} \sum_{b,x,y} (L[b,x,y,n]-\hat{L}[n])^2} \\
  \\
  \\
  \tilde{L}[b,x,y,n]& = & \frac{L[b,x,y,n] - \hat{L}[n]}{\hat{\sigma}[n]}
\end{eqnarray*}

\slideplain{Layer Normalization}

For CNNs we convert a tensor $L[b,x,y,n]$ to $\tilde{L}[b,x,y,n]$ as follows.

\begin{eqnarray*}
  \hat{L}[b,n] & = & \frac{1}{XY} \sum_{b,x,y}\;L[b,x,y,n] \\
  \\
  \\
  \hat{\sigma}[b,n] & = & \sqrt{\frac{1}{XY-1} \sum_{b,x,y} (L[b,x,y,n]-\hat{L}[b,n])^2} \\
  \\
  \\
  \tilde{L}[b,x,y,n]& = & \frac{L[b,x,y,n] - \hat{L}[b,n]}{\hat{\sigma}[n]}
\end{eqnarray*}

\slideplain{Adding an Affine Transformation}

$$\breve{L}[b,x,y,n] = \gamma[n] \tilde{L}[b,x,y,n] + \beta[n]$$

\vfill
Here $\gamma[n]$ and $\beta[n]$ are parameters of the batch normalization.

\vfill
This allows the batch normlization to learn an arbitrary affine transformation (offset and scaling).

\vfill
The affine transformation can undo the normaliztion but using ReLu activations the normalized value remains independent of scaling the weights and
bias terms (thresholds) of the layer.

\slide{Batch Normalization}

Spatial Batch Normalization is generally used in CNNs.  Layer normalization is used in the transformer.

\vfill
It is intuitively justified in terms of ``internal covariate shift'':
as the inputs to a layer change the zero mean unit variance property underlying Xavier initialization are maintained.

\ignore{
\slide{Normalization Interacts with SGD}

Consider backpropagation through a weight layer.

\begin{eqnarray*}
  y.\mathrm{value}[\cdots] & \pluseq & w.\mathrm{value}[\cdots]\;x.\mathrm{value}[\dots] \\
  \\
  \\
  w.\mathrm{grad}[\cdots] & \pluseq & y.\mathrm{grad}[\cdots]\;x.\mathrm{value}[\cdots]
\end{eqnarray*}

\vfill
Replacing $x$ by $x/\hat{\sigma}$ seems related to RMSProp for the update of $w$.

\slide{The Simple Normalization Conjecture}

A simple normalization layer $y = \alpha (x + \beta)$ can be used in place of batch normalization as long $\beta$ is initialized to $-\hat{\mu}$
and $\alpha$ is initialized to $1/\hat{\sigma}$.

\vfill
Here $\hat{\mu}$ and $\hat{\sigma}$ should be computed only after earlier normalizations have been properly initialized.
}

\vfill
\eject
~ \vfill
\centerline{\bf Skip Connections}
\vfill
\vfill

\slide{Deep Residual Networks (ResNets) by Kaiming He 2015}

\vfill
\includegraphics[width= 2.5in]{../images/resnet}
\hfill \begin{minipage}[b]{4in}
  A ``skip connection'' is adjusted by a ``residual correction''

  \bigskip
  The skip connections connects input to output directly and hence preserves gradients.

  \bigskip
  ResNets were introduced in late 2015 (Kaiming He et al.) and revolutionized computer vision.
\end{minipage}

\anaslideplain{Simple Residual Skip Connections in CNNs (stride 1)}

\medskip
\begin{eqnarray*}
R_{\color{red} \ell+1}[B,X,Y,J] & = & \mathrm{Conv}(W_{\color{red} \ell+1}[X,Y,J,J],B_{\color{red} \ell+1}[J],L_{\color{red} \ell}[B,X,Y,J]) \\
\\
\mathrm{for}\;b,x,y,j\;\;\;\;\;\;\;\\
L_{\color{red} \ell+1}[b,x,y,j] & = & L_{\color{red}\ell}[b,x,y,j] + R_{\color{red} \ell+1}[b,x,y,j]
\end{eqnarray*}

\vfill I will use capital letter indices to denote entire tensors and lower case letters for particular indeces.

\anaslide{Simple Residual Skip Connections in CNNs (stride 1)}

\medskip
\begin{eqnarray*}
R_{\color{red} \ell+1}[B,X,Y,J] & = & \mathrm{Conv}(W_{\color{red} \ell+1}[X,Y,J,J],B_{\color{red} \ell+1}[J],L_{\color{red} \ell}[B,X,Y,J]) \\
\\
\mathrm{for}\;b,x,y,j\;\;\;\;\;\;\;\\
L_{\color{red} \ell+1}[b,x,y,j] & = & L_{\color{red}\ell}[b,x,y,j] + R_{\color{red} \ell+1}[b,x,y,j]
\end{eqnarray*}

\vfill Note that in the above equations $L_{\color{red} \ell}[B,X,Y,J]$ and $R_{\color{red} \ell+1}[B,X,Y,J]$ are the same shape.
\vfill
In the actual ResNet $R_{\color{red} \ell+1}$ is computed by two or three convolution layers.

\slideplain{Handling Spacial Reduction}

Consider $L_{\color{red} \ell}[B,X_{\color{red} \ell},Y_{\color{red} \ell},J_{\color{red} \ell}]$ and $R_{\color{red} \ell+1}[B,X_{\color{red} \ell+1},Y_{\color{red} \ell+1},J_{\color{red} \ell+1}]$
\begin{eqnarray*}
X_{\color{red} \ell+1} & = & X_{\color{red} \ell}/s \\
Y_{\color{red} \ell+1} & = & Y_{\color{red} \ell}/s \\
J_{\color{red} \ell+1} & \geq &  J_{\color{red} \ell}
\end{eqnarray*}


\vfill
In this case we construct $\tilde{L}_{\color{red} \ell}[B,X_{\color{red} \ell +1},Y_{\color{red} \ell+1},J_{\color{red}\ell +1}]$

\begin{eqnarray*}
\mathrm{for}\;b,x,y,j\;\;\tilde{L}_{\color{red} \ell}[b,x,y,j] & = & \left\{\begin{array}{ll} L_{\color{red} \ell}[b,s*x,s*y,j] & \mbox{for $j < J_{\color{red} \ell}$} \\ 0 & \mbox{otherwise} \end{array}\right.\\
\\
L_{\color{red} \ell+1}[B,X_{\color{red} \ell +1},Y_{\color{red} \ell+1},J_{\color{red}\ell +1}] & = & \tilde{L}_{\color{red} \ell}[B,X_{\color{red} \ell +1},Y_{\color{red} \ell+1},J_{\color{red}\ell +1}] \\
& & + R_{\color{red} \ell+1}[B,X_{\color{red} \ell +1},Y_{\color{red} \ell+1},J_{\color{red}\ell +1}]
\end{eqnarray*}

\slide{Resnet32}

\centerline{\includegraphics[height= 5.5in]{../images/ResnetStack} {\large [Kaiming He]}}


\slideplain{Deeper Versions use Bottleneck Residual Paths}
We reduce the number of features to ${\color{red} K < J}$ before doing the convolution.

{\huge
\begin{eqnarray*}
U[B,X,Y,{\color{red} K}] & = & \mathrm{Conv}'(\Phi^A_{\ell+1}{ [1,1,{\color{red} J},{\color{red} K}]},L_\ell[B,X,Y,{\color{red} J}]) \\
\\
V[B,X,Y,{\color{red} K}] & = & \mathrm{Conv}'(\Phi^B_{\ell+1}{[3,3,{\color{red} K},{\color{red} K}]},U[B,X,Y,{\color{red} K}]) \\
\\
R[B,X,Y,{\color{red} J}] & = & \mathrm{Conv}'(\Phi^R_{\ell+1}{ [1,1,{\color{red} K},{\color{red} J}]},V[B,X,Y,{\color{red} K}]) \\
\\
L_{ \ell+1} & = & L_\ell + R
\end{eqnarray*}
}

\vfill
Here $\mathrm{CONV}'$ may include batch normalization and/or an activation function.

\slide{A General Residual Connection}

$${\color{red} y = \tilde{x} + R(x)}$$

\vfill
Where $\tilde{x}$ is either $x$ or a version of $x$ adjusted to match the shape of $R(x)$.

\ignore{
\slide{DenseNet}

For {\color{red} $u[I]$} and {\color{red} $v[J]$} we let {\color{red} $(u;w)[I+J]$} denote vector concatenation.
\vfill
$$(u;v)[k] = \left\{\begin{array}{l} u[k]\;\mbox{for $k < I$} \\ v[k-I] \;\mbox{otherwise} \end{array}\right.$$

\vfill
$$\mbox{for}\;b,x,y\;\;L_{\color{red} \ell + 1}[b,x,y,J_{\color{red} \ell} + J_{\color{red} R}] = L_\ell[b,x,y,J_{\color{red} \ell}];R[b,x,y,J_{\color{red} R}]$$
}

\slide{Deep Residual Networks}

\vfill
\includegraphics[width= 2.5in]{../images/resnet}
\hfill \begin{minipage}[b]{4in} As with most of deep learning, not much is known about what resnets are actually doing.
  
  \bigskip
  \bigskip
  For example, different residual paths might update disjoint channels making the networks shallower than they look.

  \bigskip
  \bigskip
  They are capable of representing very general circuit topologies.
\end{minipage}

\slideplain{}
\vfill
\centerline{Recurrent Neural Networks (RNNs)}
\vfill
\vfill

\slide{Vanilla RNNs}



\centerline{\includegraphics[width=3.5in]{../images/RNN}}
\centerline{{\large [Christopher Olah]}}

We use two input linear threshold units.

{\huge
$${\color{red} h_t[b,j]} = \sigma\left(\left(\sum_i\;W^{h,h}[j,i]{\color{red} h_{t-1}[b,i]}\right) + \left(\sum_k W^{x,h}[j,k]{\color{red} x_t[b,k]}\right) - B[j]\right)$$
}
\vfill
$$\mathrm{Parameter}\;{\color{red} \Phi = (W^{h,h}[J,J],\;W^{x,h}[J,K],\;B[J])}$$


\slide{Exploding and Vanishing Gradients}

\vfill
If we avoid saturation of the activation functions then we get exponentially growing or shrinking eigenvectors of the weight matrix.

\vfill
Note that if the forward values are bounded by sigmoids or tanh then they cannot explode.

\vfill
However the gradients can still explode.

\slide{Exploding Gradients: Gradient Clipping}

\vfill
We can dampen the effect of exploding gradients by clipping them before applying SGD.

\vfill
$$W.\mathrm{grad'} = \left\{\begin{array}{l} W.\mathrm{grad} \;\;\;\mbox{if $||W.\mathrm{grad}|| \leq n_{\mathrm{max}}$} \\
                                                      \\ \\
                                                      n_{\mathrm{max}} \; W.\mathrm{grad} / ||W.\mathrm{grad}|| \;\; \mbox{otherwise}
\end{array} \right.$$

\vfill
See {\tt torch.nn.utils.clip\_grad\_norm}

\slide{Time as Depth}

\centerline{\includegraphics[width=3.5in]{../images/RNN}}
\centerline{{\large [Christopher Olah]}}

\vfill
We would like the RNN to {\color{red} remember and use} information from much earlier inputs.


\vfill
All the issues with depth now occur through time.

\vfill
However, for RNNs {\color{red} at each time step we use the same model parameters.}

\vfill
In CNNs {\color{red} at each layer uses its own model parameters.}

\slide{Skip Connections Through Time}

\centerline{\includegraphics[width=3.5in]{../images/RNN}}
\centerline{{\large [Christopher Olah]}}

\vfill
We would like to add {\color{red} skip connections through time}.

\vfill
However, We have to handle the fact that the same model parameters are used at every time step.

\slideplain{Update Gate RNN (UGRNN)}

{\huge
\begin{eqnarray*}
R_t[b,j] & = & \mathrm{tanh}\left(\left(\sum_i\;W^{h,R}[j,i]{\color{red} h_{t-1}[b,i]}\right) + \left(\sum_k W^{x,R}[j,k]{\color{red} x_t[b,k]}\right) - B^R[j]\right) \\
\\
G_t[b,j] & = & \sigma\left(\left(\sum_i\;W^{h,G}[j,i]{\color{red} h_{t-1}[b,i]}\right) + \left(\sum_k W^{x,G}[j,k]{\color{red} x_t[b,k]}\right) - B^G[j]\right) \\
\\
{\color{red} h_t[b,j]} & = & {\color{red} G_t[b,j]h_{t-1}[b,j] + (1-G_t[b,j])R_t[b,j]} \\
\\
h_t[b,J] & = & G_t[b,J] \odot h_{t-1}[b,J] + (1-G_t[b,J])\odot R_t[b,J]\;\;\mbox{(alternate notation)} \\
\\
{\color{red} \Phi} & {\color{red} =} & {\color{red} (W^{h,R},W^{x,R},B^R,W^{h,G},W^{x,G},B^G)}
\end{eqnarray*}
}

\slide{Gated Recurrent Unity (GRU) by Cho et al. 2014}

\centerline{\includegraphics[width=4.0in]{../images/GRU}}
\centerline{{\huge [Christopher Olah]}}

\vfill
The right half is a UGRNN.

\vfill
The GRU adds a gating on $h_{t-1}$ before the tanh.

\slide{Long Short Term Memory (LSTM)}
\centerline{\includegraphics[width=3.5in]{../images/LSTM}}
\centerline{{\large [figure: Christopher Olah]}}

\centerline{\Large [LSTM: Hochreiter\&Shmidhuber, 1997]}

\slide{UGRNN vs. GRUs vs. LSTMs}

\vfill
In class projects from previous years, GRUs consistently outperformed LSTMs.

\vfill
A systematic study [Collins, Dickstein and Sussulo 2016] states:

\begin{quotation}
  Our results point to the GRU as being the most learnable of gated RNNs for shallow architectures, followed by the UGRNN.
\end{quotation}

\slide{Improving Trainability}
\vfill
  \centerline{Initialization}
  \vfill
  \centerline{Batch Normalization}
  \vfill
  \centerline{Residual Networks}
  \vfill
  \centerline{Gated RNNs}

\slide{END}

}
\end{document}
