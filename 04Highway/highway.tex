\input ../SlidePreamble
\input ../preamble

\begin{document}

{\Huge

  \centerline{\bf TTIC 31230, Fundamentals of Deep Learning}
  \bigskip
  \centerline{David McAllester}
  \vfill
  \centerline{Architetural Elements That Improve SGD}
  \vfill
  \vfill
  \centerline{ReLu}
  \vfill
  \centerline{Initialization}
  \vfill
  \centerline{Normalization}
  \vfill
  \centerline{Residual Connections}
  \vfill
  \centerline{Gated RNNs}

\slide{ReLu}

$\mathrm{ReLu}(x) = \max(x,0)$

\vfill
\centerline{\includegraphics[width= 4.0in]{../images/relu}}

\vfill
The ReLu does not saturate at positive inputs.

\slidetwo{Problems with Depth:}{Repeated Multiplication by Network Weights}

Consider

\vfill
$$y = \sum_i w[i]x[i]  \;\;\;= \;\;\; W[I]x[I]$$

\vfill
If the weights are large the activations will grow exponentially in network depth.

\vfill
If the weights are small the actvations will become exonentially small.

\slidetwo{Problems with Depth:}{Repeated Multiplication by Network Weights}

Exploding activations cause exploding gradients.

\begin{eqnarray*}
y  & \pluseq & w[i]x[i] \\
\\
w.grad & \pluseq & y.grad \; x[i]
\end{eqnarray*}

\vfill
The size of $w[i].\grad$ is proportional to $x[i]$

\slide{He Initialization}

Initialize weights to preserve zero-mean unit variance distributions values.

$$y = \sum_i w[i]x[i]$$

\vfill
If we assume $x_i$ has zero mean and unit variance then we want $y$
to have zero mean and unit variance (over random training points).

\vfill
He initialization randomly draws $w[i]$ from

$${\cal N}(0,\sigma^2)\;\;\;\sigma = \sqrt{1/N}$$

\slide{He Initialization}

$$y = \sum_i w[i]x[i]$$

\vfill
$$w[i] \sim {\cal N}(0,\sigma^2)\;\;\;\sigma = \sqrt{1/N}$$

\vfill
Assuming independence we have that $y$ has zero mean and unit variance.

\slide{Batch Normalization}

For CNNs we convert a tensor $L[b,x,y,n]$ to $\tilde{L}[b,x,y,n]$ as follows.

\begin{eqnarray*}
  \hat{L}[x,y,n] & = & \frac{1}{B} \sum_{b}\;L[b,x,y,n] \\
  \\
  \\
  \hat{\sigma}[x,y,n] & = & \sqrt{\frac{1}{B-1} \sum_{b} (L[b,x,y,n]-\hat{L}[x,y,n])^2} \\
  \\
  \\
  \tilde{L}[b,x,y,n]& = & \frac{L[b,x,y,n] - \hat{L}[x,y,n]}{\hat{\sigma}[x,y,n]}
\end{eqnarray*}

\slideplain{Spatial Batch Normalization}


\begin{eqnarray*}
  \hat{L}[n] & = & \frac{1}{BXY} \sum_{b,x,y}\;L[b,x,y,n] \\
  \\
  \\
  \hat{\sigma}[n] & = & \sqrt{\frac{1}{BXY-1} \sum_{b,x,y} (L[b,x,y,n]-\hat{L}[n])^2} \\
  \\
  \\
  \tilde{L}[b,x,y,n]& = & \frac{L[b,x,y,n] - \hat{L}[n]}{\hat{\sigma}[n]}
\end{eqnarray*}

\slideplain{Layer Normalization}

\begin{eqnarray*}
  \hat{L}[b,n] & = & \frac{1}{XY} \sum_{x,y}\;L[b,x,y,n] \\
  \\
  \\
  \hat{\sigma}[b,n] & = & \sqrt{\frac{1}{XY-1} \sum_{x,y} (L[b,x,y,n]-\hat{L}[b,n])^2} \\
  \\
  \\
  \tilde{L}[b,x,y,n]& = & \frac{L[b,x,y,n] - \hat{L}[b,n]}{\hat{\sigma}[b,n]}
\end{eqnarray*}

\slideplain{Adding an Affine Transformation}

$$\breve{L}[b,x,y,n] = \gamma[n] \tilde{L}[b,x,y,n] + \beta[n]$$

\vfill
Here $\gamma[n]$ and $\beta[n]$ are parameters of the batch normalization.

\vfill
This allows the batch normlization to learn an arbitrary affine transformation (offset and scaling).

\vfill
The affine transformation can undo the normaliztion but using ReLu activations the normalized value remains independent of scaling the weights and
bias terms (thresholds) of the layer.

\vfill
\eject
~ \vfill
\centerline{\bf Residual Connections}
\vfill
\vfill

\slide{Deep Residual Networks (ResNets) by Kaiming He 2015}

\vfill
\includegraphics[width= 2.5in]{../images/resnet}
\hfill \begin{minipage}[b]{4in}
  A residual connections connects input to output directly and hence preserves gradients.

  \bigskip
  ResNets were introduced in late 2015 (Kaiming He et al.) and revolutionized computer vision.
\end{minipage}

\anaslideplain{Residual Connections in CNNs}
\newcommand{\nin}{n_{\mathrm{in}}}
\newcommand{\nout}{n_{\mathrm{out}}}

\medskip
\begin{eqnarray*}
& & \tilde{L}_{\color{red} \ell+1}[B,X,Y,N_{\mathrm{out}}] \\
\\
& = & \mathrm{Conv}(K_{\color{red} \ell+1}[N_{\mathrm{out}},\Delta X,\Delta Y,N_{\mathrm{in}}],B_{\color{red} \ell+1}[N_{\mathrm{out}}],L_{\color{red} \ell}[B,X,Y,N_{\mathrm{in}}])
\end{eqnarray*}

\begin{eqnarray*}
L_{\color{red} \ell+1}[B,X,Y,N_{\mathrm{out}}] & = & L_{\color{red}\ell}[B,X,Y,N_{\mathrm{in}}] + \tilde{L}_{\color{red} \ell+1}[B,X,Y,N_{\mathrm{out}}]
\end{eqnarray*}

\vfill Capital letters indicate that complete tensors.

\vfill These equations require that the spacial dimension remains the same (stide 1) and $N_{\mathrm{out}} = N_{\mathrm{in}}$.

\slideplain{Residual Connections in CNNs}

The residual connection typically skips over several layers, or in transformers, a complex multi-level unit.

\vfill
{\Large
\begin{eqnarray*}
& & \tilde{L}_{\color{red} \ell+1}[B,X,Y,N_{\mathrm{out}}] \\
& = & \mathrm{Conv}(K_{\color{red} \ell+1}[N,\Delta X,\Delta Y,N],B_{\color{red} \ell+1}[N],L_{\color{red} \ell}[B,X,Y,N]) \\
\\
& & \tilde{L}_{\color{red} \ell+2}[B,X,Y,N] \\
& = & \mathrm{Conv}(K_{\color{red} \ell+1}[N,\Delta X,\Delta Y,N],B_{\color{red} \ell+1}[N],L_{\color{red} \ell+1}[B,X,Y,N])
\end{eqnarray*}
}

\begin{eqnarray*}
L_{\color{red} \ell+2}[B,X,Y,N] & = & L_{\color{red}\ell}[B,X,Y,N] + \tilde{L}_{\color{red} \ell+2}[B,X,Y,N]
\end{eqnarray*}


\slideplain{Handling Spacial Reduction}

Spacial reduction and neuron expansion is done without convolution.

\vfill

$$L_{\color{red} \ell+1}[b,x,y,j]  =  \left\{\begin{array}{ll} L_{\color{red} \ell}[b,s*x,s*y,j] & \mbox{for $j < N_{\color{red} \ell}$} \\ 0 & \mbox{otherwise} \end{array}\right.$$

\vfill
Residual connections are still placed around all convolutions.

\slide{Resnet32}

\centerline{\includegraphics[height= 5.5in]{../images/ResnetStack} {\large [Kaiming He]}}


\slideplain{Deeper Versions use Bottleneck Residual Paths}
\newcommand{\Nout}{N_{\mathrm{out}}}
\newcommand{\Nin}{N_{\mathrm{in}}}

We reduce the number of neurons to ${\color{red} N_{\mathrm{bottle}} < N}$ before doing the convolution.

{\huge
\begin{eqnarray*}
U[B,X,Y,N_{\mathrm{bottle}}] & = & \mathrm{Conv}(K^U[N_{\mathrm{bottle}},1,1,\Nin],L_\ell[B,X,Y,N]) \\
\\
V[B,X,Y,N_{\mathrm{bottle}}] & = & \mathrm{Conv}(K^V[N_{\mathrm{bottle}},3,3,N_\mathrm{bottle}],U[B,X,Y,N_{\mathrm{bottle}}]) \\
\\
R[B,X,Y,N] & = & \mathrm{Conv}(K^R[N,1,1,N_{\mathrm{bottle}}],V[B,X,Y,N]) \\
\\
L_{ \ell+1} & = & L_\ell + R
\end{eqnarray*}
}

\slide{A General Residual Connection}

$${\color{red} y = x + R(x)}$$

\vfill
where $R(x)$ has the same shape as $x$.

\slide{}
\vfill
\centerline{Recurrent Neural Networks (RNNs)}
\vfill
\vfill

\slide{Vanilla RNNs}



\centerline{\includegraphics[width=3.5in]{../images/RNN}}

\vfill

{\huge
$${\color{red} h_{t+1}[b,n_{\mathrm{out}}]} = \sigma(W^{h,h}[n_{\mathrm{out}},N_{\mathrm{in}}]h_t[N_{\mathrm{in}}] + W^{h,x}[n_{\mathrm{out}},N_x]x_t[N_x] - B[n_{\mathrm{out}}])$$
}

\slide{Exploding and Vanishing Gradients}

\vfill
If we avoid saturation of the activation functions then we get exponentially growing or shrinking eigenvectors of the weight matrix.

\vfill
Note that if the forward values are bounded by sigmoids or tanh then they cannot explode.

\vfill
However the gradients can still explode.

\slide{Exploding Gradients: Gradient Clipping}

\vfill
We can dampen the effect of exploding gradients by clipping them before applying SGD.

\vfill
$$W.\mathrm{grad'} = \left\{\begin{array}{l} W.\mathrm{grad} \;\;\;\mbox{if $||W.\mathrm{grad}|| \leq \alpha$} \\
                                                      \\ \\
                                                      \alpha \; W.\mathrm{grad} / ||W.\mathrm{grad}|| \;\; \mbox{otherwise}
\end{array} \right.$$

\vfill
See {\tt torch.nn.utils.clip\_grad\_norm}

\slide{Time as Depth}

\centerline{\includegraphics[width=3.5in]{../images/RNN}}

\vfill
We would like the RNN to {\color{red} remember and use} information from much earlier inputs.


\vfill
All the issues with depth now occur through time.

\vfill
However, for RNNs {\color{red} at each time step we use the same model parameters.}

\vfill
In CNNs {\color{red} at each layer uses its own model parameters.}

\slide{Skip Connections Through Time}

\centerline{\includegraphics[width=3.5in]{../images/RNN}}

\vfill
We would like to add {\color{red} skip connections through time}.

\vfill
However, We have to handle the fact that the same model parameters are used at every time step.

\slideplain{Update Gate RNN (UGRNN)}
\centerline{\includegraphics[width=3.5in]{../images/RNN}}

{\huge
\vfill
\begin{eqnarray*}
R_t[b,\nout] & = & \mathrm{tanh}(W^{h,R}[\nout,\Nin]h_t[b,\Nin] + W^{x,R}[\nout,\Nin]x_t[b,\Nin] - B^R[\nout]) \\
\\
G_t[b,\nout] & = & \;\;\;\;\sigma(W^{h,G}[\nout,\Nin]h_t[b,\Nin] + W^{x,G}[\nout,\Nin]x_t[b,\Nin] - B^G[\nout]) \\
\\
h_{t+1}[b,n] & = & G_t[b,n]h_t[b,n] + (1-G_t[b,n])R_t[b,n] \\
\\
h_{t+1}[b,N] & = & G_t[b,N] \odot h_t[b,N] + (1-G_t[b,N])\odot R_t[b,N]
\end{eqnarray*}
}

\slide{Gated Recurrent Unity (GRU) by Cho et al. 2014}

\centerline{\includegraphics[width=4.0in]{../images/GRU}}

\slide{Long Short Term Memory (LSTM)}
\centerline{\includegraphics[width=3.5in]{../images/LSTM}}

\centerline{\Large [LSTM: Hochreiter\&Shmidhuber, 1997]}

\slide{UGRNN vs. GRUs vs. LSTMs}

\vfill
In class projects from previous years, GRUs consistently outperformed LSTMs.

\vfill
A systematic study [Collins, Dickstein and Sussulo 2016] states:

\begin{quotation}
  Our results point to the GRU as being the most learnable of gated RNNs for shallow architectures, followed by the UGRNN.
\end{quotation}

\slide{Improving Trainability}
  \centerline{ReLu}
  \vfill
  \centerline{Initialization}
  \vfill
  \centerline{Normalization}
  \vfill
  \centerline{Residual Connections}
  \vfill
  \centerline{Gated RNNs}

\slide{END}

}
\end{document}
