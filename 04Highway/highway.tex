\input ../SlidePreamble
\input ../preamble

\begin{document}

{\Huge

  \centerline{\bf TTIC 31230, Fundamentals of Deep Learning}
  \bigskip
  \centerline{David McAllester}
  \vfill
  \centerline{Architetural Elements That Improve SGD}
  \vfill
  \vfill
  \centerline{ReLu}
  \vfill
  \centerline{Initialization}
  \vfill
  \centerline{Normalization}
  \vfill
  \centerline{Residual Connections}

\slide{ReLu}

$\mathrm{ReLu}(x) = \max(x,0)$

\vfill
\centerline{\includegraphics[width= 4.0in]{../images/relu}}

\vfill
The ReLu does not saturate at positive inputs.

\slidetwo{Problems with Depth:}{Repeated Multiplication by Network Weights}

Consider

\vfill
$$y = \sum_i w[i]x[i]  \;\;\;= \;\;\; W[I]x[I]$$

\vfill
If the weights are large the activations will grow exponentially in network depth.

\vfill
If the weights are small the actvations will become exonentially small.

\slidetwo{Problems with Depth:}{Repeated Multiplication by Network Weights}

Exploding activations cause exploding gradients.

\begin{eqnarray*}
y  & \pluseq & w[i]x[i] \\
\\
w.grad & \pluseq & y.grad \; x[i]
\end{eqnarray*}

\vfill
The size of $w[i].\grad$ is proportional to $x[i]$

\slide{He Initialization}

Initialize weights to preserve zero-mean unit variance distributions values.

$$y = \sum_i w[i]x[i]$$

\vfill
If we assume $x_i$ has zero mean and unit variance then we want $y$
to have zero mean and unit variance (over random training points).

\vfill
He initialization randomly draws $w[i]$ from

$${\cal N}(0,\sigma^2)\;\;\;\sigma = \sqrt{1/N}$$

\slide{He Initialization}

$$y = \sum_i w[i]x[i]$$

\vfill
$$w[i] \sim {\cal N}(0,\sigma^2)\;\;\;\sigma = \sqrt{1/N}$$

\vfill
Assuming independence we have that $y$ has zero mean and unit variance.

\slide{Batch Normalization}

For CNNs we convert a tensor $L[b,x,y,n]$ to $\tilde{L}[b,x,y,n]$ as follows.

\begin{eqnarray*}
  \hat{L}[x,y,n] & = & \frac{1}{B} \sum_{b}\;L[b,x,y,n] \\
  \\
  \\
  \hat{\sigma}[x,y,n] & = & \sqrt{\frac{1}{B-1} \sum_{b} (L[b,x,y,n]-\hat{L}[x,y,n])^2} \\
  \\
  \\
  \tilde{L}[b,x,y,n]& = & \frac{L[b,x,y,n] - \hat{L}[x,y,n]}{\hat{\sigma}[x,y,n]}
\end{eqnarray*}

\slideplain{Spatial Batch Normalization}


\begin{eqnarray*}
  \hat{L}[n] & = & \frac{1}{BXY} \sum_{b,x,y}\;L[b,x,y,n] \\
  \\
  \\
  \hat{\sigma}[n] & = & \sqrt{\frac{1}{BXY-1} \sum_{b,x,y} (L[b,x,y,n]-\hat{L}[n])^2} \\
  \\
  \\
  \tilde{L}[b,x,y,n]& = & \frac{L[b,x,y,n] - \hat{L}[n]}{\hat{\sigma}[n]}
\end{eqnarray*}

\slideplain{Layer Normalization}

\begin{eqnarray*}
  \hat{L}[b,n] & = & \frac{1}{XY} \sum_{x,y}\;L[b,x,y,n] \\
  \\
  \\
  \hat{\sigma}[b,n] & = & \sqrt{\frac{1}{XY-1} \sum_{x,y} (L[b,x,y,n]-\hat{L}[b,n])^2} \\
  \\
  \\
  \tilde{L}[b,x,y,n]& = & \frac{L[b,x,y,n] - \hat{L}[b,n]}{\hat{\sigma}[b,n]}
\end{eqnarray*}

\slideplain{Adding an Affine Transformation}

$$\breve{L}[b,x,y,n] = \gamma[n] \tilde{L}[b,x,y,n] + \beta[n]$$

\vfill
Here $\gamma[n]$ and $\beta[n]$ are parameters of the batch normalization.

\vfill
This allows the batch normlization to learn an arbitrary affine transformation (offset and scaling).

\vfill
The affine transformation can undo the normaliztion but using ReLu activations the normalized value remains independent of scaling the weights and
bias terms (thresholds) of the layer.

\vfill
\eject
~ \vfill
\centerline{\bf Residual Connections}
\vfill
\vfill

\slide{Deep Residual Networks (ResNets) by Kaiming He 2015}

\vfill
\includegraphics[width= 2.5in]{../images/resnet}
\hfill \begin{minipage}[b]{4in}
  A residual connections connects input to output directly and hence preserves gradients.

  \bigskip
  ResNets were introduced in late 2015 (Kaiming He et al.) and revolutionized computer vision.
\end{minipage}

\anaslideplain{Residual Connections in CNNs}

\medskip
\begin{eqnarray*}
& & \tilde{L}_{\color{red} \ell+1}[B,X,Y,N_{\mathrm{out}}] \\
\\
& = & \mathrm{Conv}(K_{\color{red} \ell+1}[N_{\mathrm{out}},\Delta X,\Delta Y,N_{\mathrm{in}}],B_{\color{red} \ell+1}[N_{\mathrm{out}}],L_{\color{red} \ell}[B,X,Y,N_{\mathrm{in}}])
\end{eqnarray*}

\begin{eqnarray*}
L_{\color{red} \ell+1}[B,X,Y,N_{\mathrm{out}}] & = & L_{\color{red}\ell}[B,X,Y,N_{\mathrm{in}}] + \tilde{L}_{\color{red} \ell+1}[B,X,Y,N_{\mathrm{out}}]
\end{eqnarray*}

\vfill Capital letters indicate that complete tensors.

\vfill These equations require that the spacial dimension remains the same (stide 1) and $N_{\mathrm{out}} = N_{\mathrm{in}}$.

\slideplain{Residual Connections in CNNs}

The residual connection typically skips over several layers, or in transformers, a complex multi-level network.

\vfill
\begin{eqnarray*}
& & \tilde{L}_{\color{red} \ell+1}[B,X,Y,N_{\mathrm{out}}] \\
& = & \mathrm{Conv}(K_{\color{red} \ell+1}[N,\Delta X,\Delta Y,N],B_{\color{red} \ell+1}[N],L_{\color{red} \ell}[B,X,Y,N]) \\
\\
& & \tilde{L}_{\color{red} \ell+2}[B,X,Y,N] \\
& = & \mathrm{Conv}(K_{\color{red} \ell+1}[N,\Delta X,\Delta Y,N],B_{\color{red} \ell+1}[N],L_{\color{red} \ell+1}[B,X,Y,N])
\end{eqnarray*}

\vfill
\begin{eqnarray*}
L_{\color{red} \ell+2}[B,X,Y,N] & = & L_{\color{red}\ell}[B,X,Y,N] + \tilde{L}_{\color{red} \ell+2}[B,X,Y,N]
\end{eqnarray*}


\slideplain{Handling Spacial Reduction}

Spacial reduction and neuron expansion is done without convolution.

\vfill

$$L_{\color{red} \ell+1}[b,x,y,j]  =  \left\{\begin{array}{ll} L_{\color{red} \ell}[b,s*x,s*y,j] & \mbox{for $j < N_{\color{red} \ell}$} \\ 0 & \mbox{otherwise} \end{array}\right.$$

\vfill
Residual connections are still placed around all convolutions.

\slide{Resnet32}

\centerline{\includegraphics[height= 5.5in]{../images/ResnetStack} {\large [Kaiming He]}}


\slideplain{Deeper Versions use Bottleneck Residual Paths}

We reduce the number of neurons to ${\color{red} N_{\mathrm{bottle}} < N}$ before doing the convolution.

{\huge
\begin{eqnarray*}
U[B,X,Y,N_{\mathrm{bottle}}] & = & \mathrm{Conv}(K^U[N_{\mathrm{bottle}},1,1,\Nin],L_\ell[B,X,Y,N]) \\
\\
V[B,X,Y,N_{\mathrm{bottle}}] & = & \mathrm{Conv}(K^V[N_{\mathrm{bottle}},3,3,N_\mathrm{bottle}],U[B,X,Y,N_{\mathrm{bottle}}]) \\
\\
R[B,X,Y,N] & = & \mathrm{Conv}(K^R[N,1,1,N_{\mathrm{bottle}}],V[B,X,Y,N]) \\
\\
L_{ \ell+1} & = & L_\ell + R
\end{eqnarray*}
}

\slide{A General Residual Connection}

$${\color{red} y = x + R(x)}$$

\vfill
where $R(x)$ has the same shape as $x$.

\slide{Improving Trainability}
  \centerline{ReLu}
  \vfill
  \centerline{Initialization}
  \vfill
  \centerline{Normalization}
  \vfill
  \centerline{Residual Connections}

\slide{END}

}
\end{document}
