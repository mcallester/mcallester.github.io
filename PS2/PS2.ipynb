{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import edf\n",
    "import mnist_loader\n",
    "\n",
    "train_images, train_labels = mnist_loader.load_mnist(section = 'training', path = 'MNIST')\n",
    "test_images, test_labels = mnist_loader.load_mnist(section = 'testing', path = 'MNIST')\n",
    "\n",
    "train_images = train_images[:,None,2:-2,2:-2]\n",
    "test_images = test_images[:,None,2:-2,2:-2]\n",
    "\n",
    "plt.imshow(train_images[0,0], cmap='gray', interpolation = 'nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(batch_size, data, labels, xnode, ynode, probnode, lossnode=None):\n",
    "    num_samples = len(data)\n",
    "    total_err = 0.0\n",
    "    total_loss = 0.0\n",
    "    num_batches = num_samples//batch_size\n",
    "    p = np.random.permutation(len(train_images))\n",
    "    shuffled_train_images = train_images[p]\n",
    "    shuffled_train_labels = train_labels[p]\n",
    "    for i in range(num_batches):\n",
    "        start, end = i*batch_size, (i+1)*batch_size\n",
    "        xnode.value = shuffled_train_images[start:end]\n",
    "        ynode.value = shuffled_train_labels[start:end]\n",
    "        edf.Forward()\n",
    "        total_err += np.sum(np.not_equal(np.argmax(probnode.value, axis=1), ynode.value))\n",
    "        if lossnode:\n",
    "            total_loss += lossnode.value.mean()\n",
    "            edf.Backward(lossnode)\n",
    "            edf.SGD()\n",
    "        if i>0 and i%400 == 0:\n",
    "            print (\"\\t Batch {}/{}\".format(i, num_batches))\n",
    "    return 100*total_err/num_samples\n",
    "\n",
    "def train(num_epochs, batch_size, xnode, ynode, probnode, lossnode):\n",
    "    train_err_log = []\n",
    "    test_err_log = []\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"Epoch: {}/{}\".format(epoch+1, num_epochs))\n",
    "        train_err = run_epoch(batch_size, train_images, train_labels, xnode, ynode, probnode, lossnode)\n",
    "        train_err_log.append(train_err)\n",
    "        print (\"\\t Training Error {:.2f} %\".format(train_err))\n",
    "        test_err = run_epoch(batch_size, test_images, test_labels, xnode, ynode, probnode)\n",
    "        test_err_log.append(test_err)\n",
    "        print (\"\\t Test Error {:.2f} %\".format(test_err))\n",
    "    return train_err_log, test_err_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reshape(edf.CompNode):\n",
    "    def __init__(self, x, size):\n",
    "        edf.CompNodes.append(self)\n",
    "        self.x = x\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self):\n",
    "        self.value = self.x.value.reshape(self.x.value.shape[0], *self.size)\n",
    "        \n",
    "    def backward(self):\n",
    "        self.x.addgrad(self.grad.reshape(self.x.value.shape))\n",
    "\n",
    "class Affine(edf.CompNode):\n",
    "    def __init__(self, x, in_feats, out_feats):\n",
    "        edf.CompNodes.append(self)\n",
    "        X = edf.Xavier(in_feats)\n",
    "        self.w = edf.Parameter(np.random.uniform(-X,X,(in_feats,out_feats)))\n",
    "        self.x = x\n",
    "\n",
    "    def forward(self):\n",
    "        self.value = self.x.value @ self.w.value\n",
    "\n",
    "    def backward(self):\n",
    "        self.x.addgrad(self.grad @ self.w.value.transpose())\n",
    "        dw = (np.expand_dims(self.x.value, -1) * np.expand_dims(self.grad, -2)).reshape(-1, *self.w.value.shape)\n",
    "        self.w.addgrad(dw)\n",
    "        \n",
    "class ReLU(edf.CompNode):\n",
    "    def __init__(self, x):\n",
    "        edf.CompNodes.append(self)\n",
    "        self.x = x\n",
    "\n",
    "    def forward(self):\n",
    "        self.value = np.maximum(0, self.x.value)\n",
    "\n",
    "    def backward(self):\n",
    "        self.x.addgrad(np.greater(self.x.value, 0) * self.grad)\n",
    "        \n",
    "class Transpose(edf.CompNode):\n",
    "    def __init__(self, x, dim1, dim2):\n",
    "        edf.CompNodes.append(self)\n",
    "        self.x = x\n",
    "        self.dim1, self.dim2 = dim1, dim2\n",
    "\n",
    "    def forward(self):\n",
    "        self.value = self.x.value.swapaxes(self.dim1, self.dim2)\n",
    "        \n",
    "    def backward(self):\n",
    "        self.x.addgrad(self.grad.swapaxes(self.dim1, self.dim2))\n",
    "        \n",
    "class BatchedMatmul(edf.CompNode):\n",
    "    def __init__(self, x1, x2):\n",
    "        edf.CompNodes.append(self)\n",
    "        self.x1 = x1\n",
    "        self.x2 = x2\n",
    "\n",
    "    def forward(self):\n",
    "        self.value = self.x1.value @ self.x2.value\n",
    "        \n",
    "    def backward(self):\n",
    "        self.x1.addgrad(self.grad @ self.x2.value.swapaxes(-1,-2))\n",
    "        self.x2.addgrad(self.x1.value.swapaxes(-1,-2) @ self.grad)\n",
    "        \n",
    "class Sum(edf.CompNode):\n",
    "    def __init__(self, x1, x2):\n",
    "        edf.CompNodes.append(self)\n",
    "        self.x1 = x1\n",
    "        self.x2 = x2\n",
    "\n",
    "    def forward(self):\n",
    "        self.value = self.x1.value + self.x2.value\n",
    "        \n",
    "    def backward(self):\n",
    "        self.x1.addgrad(self.grad)\n",
    "        self.x2.addgrad(self.grad)\n",
    "        \n",
    "class Softmax(edf.CompNode):\n",
    "    def __init__(self, s):\n",
    "        edf.CompNodes.append(self)\n",
    "        self.s = s\n",
    "        \n",
    "    def forward(self):\n",
    "        smax = np.max(self.s.value,axis=-1,keepdims=True)\n",
    "        bounded = np.maximum(-10,self.s.value - smax)\n",
    "        es = np.exp(bounded) \n",
    "        self.value = es / np.sum(es,axis=-1,keepdims=True)\n",
    "\n",
    "    def backward(self):\n",
    "        p_dot_pgrad = np.matmul(np.expand_dims(self.value, -2), np.expand_dims(self.grad, -1)).squeeze(-1)\n",
    "        self.s.addgrad(self.value * (self.grad - p_dot_pgrad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_im2col_indices(x_shape, field_height, field_width, padding=1, stride=1):\n",
    "    N, C, H, W = x_shape\n",
    "    out_height = int((H + 2 * padding - field_height) / stride + 1)\n",
    "    out_width = int((W + 2 * padding - field_width) / stride + 1)\n",
    "    i0 = np.repeat(np.arange(field_height), field_width)\n",
    "    i0 = np.tile(i0, C)\n",
    "    i1 = stride * np.repeat(np.arange(out_height), out_width)\n",
    "    j0 = np.tile(np.arange(field_width), field_height * C)\n",
    "    j1 = stride * np.tile(np.arange(out_width), out_height)\n",
    "    i = i0.reshape(-1, 1) + i1.reshape(1, -1)\n",
    "    j = j0.reshape(-1, 1) + j1.reshape(1, -1)\n",
    "    k = np.repeat(np.arange(C), field_height * field_width).reshape(-1, 1)\n",
    "    return (k, i, j)\n",
    "\n",
    "def col2im_indices(cols, x_shape, field_height=3, field_width=3, padding=1, stride=1):\n",
    "    N, C, H, W = x_shape\n",
    "    H_padded, W_padded = H + 2 * padding, W + 2 * padding\n",
    "    x_padded = np.zeros((N, C, H_padded, W_padded), dtype=cols.dtype)\n",
    "    k, i, j = get_im2col_indices(x_shape, field_height, field_width, padding, stride)\n",
    "    cols_reshaped = cols.reshape(C * field_height * field_width, -1, N)\n",
    "    cols_reshaped = cols_reshaped.transpose(2, 0, 1)\n",
    "    np.add.at(x_padded, (slice(None), k, i, j), cols_reshaped)\n",
    "    if padding == 0: return x_padded\n",
    "    return x_padded[:, :, padding:-padding, padding:-padding]\n",
    "\n",
    "def im2col_indices(x, field_height, field_width, padding=1, stride=1):\n",
    "    p = padding\n",
    "    x_padded = np.pad(x, ((0, 0), (0, 0), (p, p), (p, p)), mode=\"constant\")\n",
    "    k, i, j = get_im2col_indices(x.shape, field_height, field_width, padding, stride)\n",
    "    cols = x_padded[:, k, i, j]\n",
    "    C = x.shape[1]\n",
    "    cols = cols.transpose(1, 2, 0).reshape(field_height * field_width * C, -1)\n",
    "    return cols\n",
    "\n",
    "class Conv(edf.CompNode):\n",
    "    def __init__(self, x, in_channels, out_channels, k, padding, stride):\n",
    "        edf.CompNodes.append(self)\n",
    "        self.in_channels, self.out_channels, self.k, self.padding, self.stride = in_channels, out_channels, k, padding, stride\n",
    "        gain = edf.Xavier(self.in_channels * self.k**2)\n",
    "        self.w = edf.Parameter(np.random.uniform(-gain,gain,(out_channels, in_channels, k, k)))\n",
    "        self.x = x\n",
    "\n",
    "    def forward(self):\n",
    "        N, _, height, width = self.x.value.shape\n",
    "        out_height = (height + 2 * self.padding - self.k) // self.stride + 1\n",
    "        out_width = (width + 2 * self.padding - self.k) // self.stride + 1\n",
    "        self.x_cols = im2col_indices(self.x.value, self.k, self.k, padding=self.padding, stride=self.stride)\n",
    "        W_col = self.w.value.reshape(self.out_channels, -1)\n",
    "        out = W_col @ self.x_cols\n",
    "        out = out.reshape(self.out_channels, out_height, out_width, N)\n",
    "        self.value = out.transpose(3, 0, 1, 2)\n",
    "\n",
    "    def backward(self):\n",
    "        dout_reshaped = self.grad.transpose(1, 2, 3, 0).reshape(self.out_channels, -1)\n",
    "        dw = dout_reshaped.dot(self.x_cols.T).reshape(self.w.value.shape)[None,:]\n",
    "        dx_cols = self.w.value.reshape(self.out_channels, -1).T.dot(dout_reshaped)\n",
    "        dx = col2im_indices(dx_cols, self.x.value.shape, self.k, self.k, self.padding, self.stride)\n",
    "        self.x.addgrad(dx)\n",
    "        self.w.addgrad(dw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "batch_size = 64\n",
    "edf.clear_compgraph()\n",
    "xnode = edf.Input()\n",
    "ynode = edf.Input()\n",
    "\n",
    "def CNN(x, y):\n",
    "    u = ReLU(Conv(x, 1, 4, 3, 1, 2))\n",
    "    u = ReLU(Conv(u, 4, 8, 3, 1, 2))\n",
    "    u = ReLU(Conv(u, 8, 16, 3, 1, 2))\n",
    "    u = Conv(u, 16, 10, 3, 0, 1)\n",
    "    u = Reshape(u, (10,))\n",
    "    probnode = edf.Softmax(u)\n",
    "    lossnode = edf.LogLoss(probnode, ynode)\n",
    "    return probnode, lossnode\n",
    "\n",
    "probnode, lossnode = CNN(xnode, ynode)\n",
    "\n",
    "num_epochs = 10\n",
    "edf.learning_rate = 0.5\n",
    "train_err_log, test_err_log = train(num_epochs, batch_size, xnode, ynode, probnode, lossnode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(edf.CompNode):\n",
    "    def __init__(self, x):\n",
    "        edf.CompNodes.append(self)\n",
    "        self.x = x\n",
    "\n",
    "    def forward(self):\n",
    "        mu = self.x.value.mean((1,2), keepdims=True)\n",
    "        self.std = self.x.value.std((1,2), keepdims=True)\n",
    "        self.value = (self.x.value - mu) / self.std\n",
    "        \n",
    "    def backward(self):\n",
    "        grad = (1/self.std) * (self.grad - self.grad.mean((1,2), keepdims=True) - self.grad * (self.grad * self.value).mean((1,2), keepdims=True))\n",
    "        self.x.addgrad(grad)\n",
    "        \n",
    "def SelfAttention(x, in_dim, att_dim):\n",
    "    q = Affine(x, in_dim, att_dim)\n",
    "    k = Affine(x, in_dim, att_dim)\n",
    "    v = Affine(x, in_dim, att_dim)\n",
    "    \n",
    "    att = BatchedMatmul(q, Transpose(k, 1, 2))\n",
    "    att_softmax = Softmax(att)\n",
    "    h = BatchedMatmul(att_softmax, v)\n",
    "    \n",
    "    out = Affine(h, att_dim, in_dim)\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TransformerLayer(x, in_dim, att_dim):\n",
    "    u = Sum(SelfAttention(LayerNorm(x), in_dim, att_dim), x)\n",
    "    out = Sum(Affine(ReLU(Affine(LayerNorm(u), in_dim, 2*in_dim)), 2*in_dim, in_dim), u)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10\n",
      "\t Batch 400/937\n",
      "\t Batch 800/937\n",
      "\t Training Error 10.32 %\n",
      "\t Test Error 6.46 %\n",
      "Epoch: 2/10\n",
      "\t Batch 400/937\n",
      "\t Batch 800/937\n",
      "\t Training Error 4.97 %\n",
      "\t Test Error 4.02 %\n",
      "Epoch: 3/10\n",
      "\t Batch 400/937\n",
      "\t Batch 800/937\n",
      "\t Training Error 3.84 %\n",
      "\t Test Error 3.17 %\n",
      "Epoch: 4/10\n",
      "\t Batch 400/937\n",
      "\t Batch 800/937\n",
      "\t Training Error 3.24 %\n",
      "\t Test Error 2.81 %\n",
      "Epoch: 5/10\n",
      "\t Batch 400/937\n",
      "\t Batch 800/937\n",
      "\t Training Error 2.90 %\n",
      "\t Test Error 2.81 %\n",
      "Epoch: 6/10\n",
      "\t Batch 400/937\n",
      "\t Batch 800/937\n",
      "\t Training Error 2.62 %\n",
      "\t Test Error 2.21 %\n",
      "Epoch: 7/10\n",
      "\t Batch 400/937\n",
      "\t Batch 800/937\n",
      "\t Training Error 2.31 %\n",
      "\t Test Error 1.91 %\n",
      "Epoch: 8/10\n",
      "\t Batch 400/937\n",
      "\t Batch 800/937\n",
      "\t Training Error 2.15 %\n",
      "\t Test Error 1.73 %\n",
      "Epoch: 9/10\n",
      "\t Batch 400/937\n",
      "\t Batch 800/937\n",
      "\t Training Error 2.06 %\n",
      "\t Test Error 1.96 %\n",
      "Epoch: 10/10\n",
      "\t Batch 400/937\n",
      "\t Batch 800/937\n",
      "\t Training Error 1.96 %\n",
      "\t Test Error 1.87 %\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1234)\n",
    "batch_size = 64\n",
    "edf.clear_compgraph()\n",
    "xnode = edf.Input()\n",
    "ynode = edf.Input()\n",
    "\n",
    "dim = 32\n",
    "patch_size = 8\n",
    "seq_len = (24 // patch_size)**2\n",
    "\n",
    "u = Conv(xnode, 1, dim, patch_size, 0, patch_size)\n",
    "u = Reshape(u, (dim, seq_len))\n",
    "u = Transpose(u, 1, 2)\n",
    "\n",
    "u = TransformerLayer(u, dim, dim)\n",
    "u = TransformerLayer(u, dim, dim)\n",
    "\n",
    "u = Reshape(u, (seq_len*dim,))\n",
    "scores = Affine(u, seq_len*dim, 10)\n",
    "\n",
    "probnode = edf.Softmax(scores)\n",
    "lossnode = edf.LogLoss(probnode, ynode)\n",
    "\n",
    "num_epochs = 10\n",
    "edf.learning_rate = 0.05\n",
    "train_err_log, test_err_log = train(num_epochs, batch_size, xnode, ynode, probnode, lossnode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
