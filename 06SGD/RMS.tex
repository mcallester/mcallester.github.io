\input ../../SlidePreamble
\input ../../preamble

\newcommand{\solution}[1]{\bigskip {\bf Solution}: #1}

\begin{document}

{\Huge
  \centerline{\bf TTIC 31230, Fundamentals of Deep Learning}
  \bigskip
  \centerline{David McAllester, Autumn 2020}
  \vfill
  \centerline{\bf Stochastic Gradient Descent (SGD)}
  \vfill
  \centerline{\bf RMSProp and Adam}

\slide{RMSProp and Adam}

RMSProp and Adam are ``adaptive'' SGD methods --- they use different learning rates for different model parameters where the parameter-specific learning rate is
computed from statistics of the data.

\vfill
Adam is variant of RMSProp with momentum and ``debiasing''.

\slide{RMSProp}

RMSProp was introduced in Hinton's class lecture slides.

\vfill
RMSProp is based on a running average of $\hat{g}[i]^2$ for each scalar model parameter $i$.

\begin{eqnarray*}
{\color{red} s_t[i]} & {\color{red} =} & {\color{red} \left(1-\frac{1}{N_s}\right) s_{t-1}[i] + \frac{1}{N_s} \hat{g}_t[i]^2}\;\;\;\mbox{$N_s$ typically 100 or 1000} \\
\\
{\color{red} \Phi_{t+1}[i]} & {\color{red} =} & {\color{red} \Phi_t[i] - \frac{\eta}{\sqrt{s_t[i]} + \epsilon}\;\; \hat{g}_t[i]}
\end{eqnarray*}

\slide{RMSProp}
The second moment of a scalar random variable $x$ is $E\;x^2$

\vfill
The variance $\sigma^2$ of $x$ is $E \;(x - \mu)^2$ with $\mu = E\;x$.

\vfill
RMSProp uses an estimate $s[i]$ of the second moment of the random scalar $\hat{g}[i]$.

\vfill
If the mean $g[i]$ is small then $s[i]$ approximates the variance of $\hat{g}[i]$.

\vfill
There is a ``centering'' option in PyTorch RMSProp that switches from the second moment to the variance.

\slide{RMSProp Motivation}

If we assume constant noise covariance and locally positive definite quadratic loss in the neighborhood of the stationary distribution then, in a coordinate system
where the Hessian is the identity matrix, vanilla SGD has a stationary distribution of the form
$$p(\Phi) \propto \exp\left(-\sum_i \frac{{\Phi_i^2}}{\alpha\eta\sigma_i^2}\right)$$

\vfill
where, because we have used the Hessian-normalized coordinate system, we have 
$${\cal L} = ||\Phi||^2$$

\slide{RMSProp Motivation}

If we set $\eta_i = \eta_0/\sigma_i^2$ this becomes

$$p(\Phi) \propto \exp\left(\frac{-{\cal L}}{\alpha\eta_0}\right)$$

\vfill
The Gibbs stationary distribution seems desirable for exploration.

\slide{RMSProp Motivation}

However, computing the Hessian is extremely challenging and RMSprop works in the given coordinates.

\vfill
Also, centered RMSProp divides by $\sigma_i$ rather than $\sigma_i^2$.

\slide{RMSProp is Theoretically Mysterious}

{\color{red} $$\Phi[i] \;\minuseq \; \eta\;\frac{\hat{g}[i]}{\sigma[i]}\;\;(1)\hspace{5em}\Phi[i] \;\minuseq \; \eta\;\frac{\hat{g}[i]}{\sigma^2[i]}\;\;(2)$$}

\vfill
Although (1) seems to work better, (2) is better motivated theoretically.  To see this we can consider units.

\vfill
If parameters have units of ``weight'', and loss is in bits, then (2) type checks with $\eta$ having units of bits --- the numerical value of $\eta$
has no dependence on the choice of the weight unit.

\vfill
Consistent with the dimensional analysis, many theoretical analyses support (2) over (1) contrary to apparent empirical performance.

\slide{Adam --- Adaptive Momentum}

Adam combines momentum and RMSProp.

\vfill
PyTorch RMSProp also supports momentum.  However, it presumably uses the standard momentum learning rate parameter which couples the temperature to both
the learning rate and the momentum parameter.  Without an understanding of the coupling to temperature, hyper-parameter optimization is then difficult.

\vfill
Adam uses a momentum parameter that is naturally decoupled from temperature.

\vfill
Adam also uses ``bias correction''.

\slide{Bias Correction}

Consider a standard moving average.

\begin{eqnarray*}
\tilde{x}_0 & = & 0 \\
\\
\tilde{x}_t & = & \left(1-\frac{1}{N}\right)\tilde{x}_{t-1} + \left(\frac{1}{N}\right)x_t
\end{eqnarray*}

\vfill
For $t < N$ the average $\tilde{x}_t$ will be strongly biased toward zero.

\slide{Bias Correction}

The following running average maintains the invariant that $\tilde{x}_t$ is exactly the average of $x_1,\ldots,x_t$.

\begin{eqnarray*}
\tilde{x}_t & = & \left(\frac{t-1}{t}\right)\tilde{x}_{t-1} + \left(\frac{1}{t}\right)x_t \\
\\
\\
& = & \left(1-\frac{1}{t}\right)\tilde{x}_{t-1} + \left(\frac{1}{t}\right)x_t
\end{eqnarray*}

\vfill
We now have $\tilde{x}_1 = x_1$ independent of any $x_0$.

\vfill
But this fails to track a moving average for $t >> N$.

\slide{Bias Correction}

The following avoids the initial bias toward zero while still tracking a moving average.

\begin{eqnarray*}
\tilde{x}_t & = & \left(1-\frac{1}{\min(N,t)}\right)\tilde{x}_{t-1} + \left(\frac{1}{\min(N,t)}\right)x_t
\end{eqnarray*}

\vfill
The published version of Adam has a more obscure form of bias correction which yields essentially the same effect.

\slide{Adam (simplified)}

\begin{eqnarray*}
  \tilde{g}_{t}[i] & = & \left(1-\frac{1}{\min(t,N_g)}\right)\tilde{g}_{t-1}[i] + \frac{1}{\min(t,N_g)} \hat{g}_t[i] \\
  \\
  \\
  s_{t}[i] & = & \left(1-\frac{1}{\min(t,N_s)}\right)s_{t-1}[i] + \frac{1}{\min(t,N_s)} \hat{g}_t[i]^2 \\
  \\
  \\
\Phi_{t+1}[i] & =  & \Phi_t[i] - \frac{\eta}{\sqrt{s_{t}[i]} + \epsilon}\;\;\tilde{g}_{t}[i]
\end{eqnarray*}

\slide{Decoupling $\eta$ from $\epsilon$}

$$\Phi_{t+1}[i] =  \Phi_t - \frac{\eta}{\sqrt{s_{t}[i]} + \epsilon}\;\;\tilde{g}_{t}[i]$$

\vfill
The optimal $\epsilon$ is often large.
For large $\epsilon$ it is useful to set $\eta = \epsilon\eta_0$ in which case we get

$$\Phi_{t+1}[i] =  \Phi_t - \frac{\eta_0}{1 + \frac{1}{\epsilon}\sqrt{s_{t}[i]}}\;\;\tilde{g}_{t}[i]$$

\vfill
We then get standard SGD  as $\epsilon \rightarrow \infty$ holding $\eta_0$ fixed.

\slide{Making Adam Independent of $B$}

Making Adam independent of the batch size $B$ is difficult.

\vfill
Rather than
$$s_t[i] = \left(1-\frac{1}{N_s}\right) s_{t-1}[i] + \frac{1}{N_s} \hat{g}_t[i]^2$$
\vfill
we would like
\begin{eqnarray*}
s_t[i] & = &  \left(1-\frac{1}{N_s}\right) s_{t-1}[i] + \frac{1}{N_s} \left(\frac{1}{B} \sum_b \hat{g}_{t,b}[i]^2\right)
\end{eqnarray*}

\slide{Making Adam Independent of $B$}

In PyTorch this is difficult because ``optimizers'' are defined as a function of $\hat{g}_t$.

\vfill
$\hat{g}_t[i]$ is not sufficient for computing $\sum_b \;\hat{g}_{t,b}[i]^2$.

\vfill
To compute $\sum_b \;\hat{g}_{t,b}[i]^2$ we need to maintain a batch index in the gradiant attribute of parameters.

\slide{END}

} \end{document}

