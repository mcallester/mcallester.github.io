\input ../SlidePreamble
\input ../preamble

\newcommand{\solution}[1]{\bigskip {\bf Solution}: #1}

\begin{document}

{\Huge
  \centerline{\bf TTIC 31230, Fundamentals of Deep Learning}
  \bigskip
  \centerline{David McAllester, Winter 2020}
  \vfill
  \centerline{\bf Stochastic Gradient Descent (SGD)}
  \vfill
  \vfill
  \centerline{\bf The Classical Convergence Theorem}
  \vfill
  \centerline{\bf The Learning Rate as Temperature}
  \vfill
  \centerline{\bf Temperature, Batch Size, Momentum, and Adam}

\slide{Vanilla SGD}

$$\Phi \;\minuseq\; \eta \hat{g}$$

\vfill
\begin{eqnarray*}
  \hat{g} & = & E_{(x,y) \sim \mathrm{Batch}}\;\nabla_\Phi\;\mathrm{loss}(\Phi,x,y) \\
  \\
  \\
  g & = & E_{(x,y) \sim \mathrm{Pop}}\;\nabla_\Phi\;\mathrm{loss}(\Phi,x,y) \\
\end{eqnarray*}

\vfill
\centerline{$\eta$ is the ``learning rate'' hyper-parameter (a parameter not in $\Phi$).}


\slide{Issues}

\vfill
\begin{itemize}
\item {\bf Gradient Noise:} The accuracy of $\hat{g}$ as an estimate of $g$.

  \vfill
\item {\bf Gradient Drift:(second order structure)} The fact that $g$ changes as the parameters change.

  \vfill
\item {\bf Convergence.} Convergence requires $\eta_t \rightarrow 0$.

  \vfill
  \item {\bf Exploration.} Reducing $\eta$ slowly allows better exploration of model parameters.
\end{itemize}

\slide{An Example of Gradient Noise}

Suppose that $y$ is a scalar, and consider

\begin{eqnarray*}
 \mathrm{loss}(\beta,y) & = & \frac{1}{2}(\beta - y)^2 \\
 \\
  g & = &  \nabla_\beta\;E_{y \sim \mathrm{Pop}}\;\frac{1}{2}(\beta - y)^2 \\
  \\
  & = & \beta - E_{y \sim \mathrm{Pop}} \; y \\
  \\
  \hat{g} & = &\beta - E_{y \sim \mathrm{Batch}} \;y
\end{eqnarray*}

\vfill
Even if $\beta$ is optimal, for a finite batch we will have $\hat{g} \not = 0$.

\slide{The Classical Convergence Theorem}

$$\Phi \;\minuseq \; \eta_t \nabla_\Phi\;\mathrm{loss}(\Phi,x_t,y_t)$$

\vfill
For ``sufficiently smooth'' non-negative loss with

\vfill
$$\eta_t \geq 0\;\;\;\;\;\;\;\;\lim_{t \rightarrow \infty} \;\eta_t = 0\;\;\;\;\;\;\;\;\sum_t \eta_t = \infty \;\;\;\;\;\;\sum_t \;\eta_t^2 < \infty$$

\vfill
we have that the training loss $E_{(x,y) \sim \mathrm{Train}}\; \mathrm{loss}(\Phi,x,t)$ converges to a limit and any limit point of the sequence $\Phi_t$
is a stationary point in the sense that {\huge  $\nabla_\Phi \; E_{(x,y) \sim \mathrm{Train}} \;\mathrm{loss}(\Phi,x,t) = 0$}.

\vfill
{\Large
\vfill
{\bf Rigor Police:} One can construct cases where $\Phi$ diverges to infinity, converges to a saddle point, or even converges to a limit cycle.

}

\slide{Physicist's Proof of the Convergence Theorem}

Since $\lim_{t \rightarrow 0} \;\eta_t = 0$ we will eventually get to arbitrarilly small learning rates.

\vfill
For sufficiently small learning rates any meaningful update of the parameters will be based on an arbitrarily large sample
of gradients at essentially the same parameter value.

\vfill
An arbitrarily large sample will become arbitrarily accurate as an estimate of the full gradient.

\vfill
But since $\sum_t \eta_t = \infty$, no matter how small the learning rate gets, we still can make arbitrarily large motions in parameter space.


\vfill
{\Large
\vfill
For a rigorous proof see Neuro-Dynamic Programming, Bertsekas and Tsitsiklis, 1996.}

\slidetwo{SGD as a form of MCMC}{Learning Rate as a Temperature Parameter}

\centerline{\includegraphics[height= 4in]{\images/AnnealingSGD}}
\centerline{\Large Gao Huang et. al., ICLR 2017}


\slide{Temperature}

Physical temperature is a relationship between the energy and probability.

\vfill
$$P(x) = \frac{1}{Z} \;e^{\frac{-E(x)}{kT}} \;\;\;\;\;\;\;Z = \sum_x\; e^{\frac{-E(x)}{kT}}$$

\vfill
This is called the Gibbs or Boltzman distribution.

\vfill
$E(x)$ is the energy of physical microstate state $x$.


\vfill
$k$ is Boltzman's constant.

\vfill
$Z$ is called the partition function.

\slide{Temperature}

Boltzman's constant can be measured using the ideal gas law.

\begin{eqnarray*}
pV & = & NkT \\
\\
p & = & \mathrm{pressure} \\
V & = & \mathrm{volume} \\
N & = & \mbox{the number of molecules} \\
T & = & \mathrm{temperature} \\
k & = & \mbox{Boltzman's constant}
\end{eqnarray*}

\vfill
We can measure $p$, $V$, $N$ and $T$ and solve for $k$.


\slide{Temperature}

The Gibbs distribution is typically written as

$$P(x) = \frac{1}{Z}\;e^{-\beta E(x)}$$

\vfill

$\beta = \frac{1}{kT}$ is the (inverse) temperature parameter.

\vfill
``Hot'' is when $\beta$ is small and ``cold'' is when $\beta$ is large (confusing).

\slidetwo{Loss as Energy}{Learning Rate as Temperature}

A finite learning rate defines an equalibrium probability distribution (or density) over the model parameters.

\vfill
Each value of the model parameters has an associated loss.

\vfill
The distribution over model parameters defines a distribution over loss.

\slidetwo{Loss as Energy}{Learning Rate as Temperature}

\centerline{\includegraphics[width = 7in]{\images/annealing}}

\vfill
Equalibrium energy (loss) distributions at three different temperatures (learning rates).

\vfill
\centerline{\parbox{7in}
{\Large
Plots are from the ResNet paper.  Left plot is for CNNs without residual skip connections, the right plot is ResNet. Thin lines are training error, thick lines are validation error. In all cases $\eta$ is reduced twice, each time by a factor of 2.
}}

\slidetwo{Loss as Energy}{Learning Rate as Temperature}

\centerline{\includegraphics[width = 4in]{\images/annealing}}

\vfill
The learnng rate is always reduced over time.  The profile of learning rate reduction is called {\bf the learning rate schedule}.  An older convention (shown here) is to reduce the learning rate by a factor of 2 in steps.

\vfill
Modern transformer training first quickly and smoothly ramps up the learning rate (the warm-up phase) up and then slowly and smoothly ramps it down.

\slide{Batch Size and Temperature}

Vanilla SGD with minibatching typically uses the following update which defines the meaning of $\eta$.

\begin{eqnarray*}
\Phi_{t+1} & \;\minuseq\; & \eta \hat{g}_t \\
\\
\hat{g}_t & = & \frac{1}{B} \sum_b \hat{g}_{t,b}
\end{eqnarray*}

\vfill
Here $\hat{g}_{b}$ is the average gradient over the batch.

\vfill
Under this update {\bf increasing the batch size (while holding $\eta$ fixed) reduces the temperature.}

\slide{Making Temperature Independent of $B$}

For batch size 1 with learning rate $\eta_0$ we have

\begin{eqnarray*}
\Phi_{t+1} & = &  \Phi_{t} - \eta_0\;\nabla_\Phi {\cal L}(t,\Phi_{t}) \\
\\
\Phi_{t+B} & = &  \Phi_{t} - \sum_{b=0}^{B-1} \;\eta_0\;\nabla_\Phi {\cal L}(t+b,\Phi_{\color{red} t+b-1}) \\
\\
& \approx & \Phi_t - \eta_0 \sum_b \nabla_\Phi {\cal L}(t+b,\Phi_{\color{red} t}) \\
\\
& = & \Phi_t - B\eta_0\; \hat{g}_t
\end{eqnarray*}

\vfill
For batch updates $\Phi_{t+1} = \Phi_t - B\eta_0\; \hat{g}_t$ the temperature is essentially determined by $\eta_0$ independent of $B$.

\slideplain{Making Temperature Independent of $B$}

In 2017 it was discovered that setting $\eta = B\eta_0$ allows very large (highly parallel)
batches.

\vfill
{\bf Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour}, Goyal et al., 2017.


\slideplain{Momentum}

The standard (PyTorch) momentum SGD equations are

\begin{eqnarray*}
  {\color{red} v_t} & {\color{red} =} & {\color{red} \mu v_{t-1} + \eta * \hat{g}_t} \;\;\;\mbox{$\mu$ is typically .9 or .99}\\
  \\
  {\color{red} \Phi_{t+1}} & {\color{red} =} & {\color{red} \Phi_t -  v_t} \\
\end{eqnarray*}

\vfill
Here $v$ is velocity, $0 \leq \mu < 1$ represents friction drag and $\eta \hat{g}$ is the acceleration generated by the gradient force.

\slide{Momentum}

The theory of momentum is generally given in terms of second order structure and total gradients (GD rather than SGD).

\vfill
But second order analyses are controversial for SDG in very large dimension.

\vfill
Still, momentum is widely used in practice.

\slide{Momentum and Temperature}

\begin{eqnarray*}
  {\color{red} v_t} & {\color{red} =} & {\color{red} \mu v_{t-1} + \eta * \hat{g}_t} \;\;\;\mbox{$\mu$ is typically .9 or .99}\\
  \\
  {\color{red} \Phi_{t+1}} & {\color{red} =} & {\color{red} \Phi_t -  v_t} \\
\end{eqnarray*}

\vfill
We will use a first order analysis to argue that by setting

{\color{red} $$\eta = (1-\mu)B\eta_0$$}

\vfill
the temperature will be essentially determined by $\eta_0$ independent of the choice of the momentum parameter $\mu$ or the batch size $B$.

\slide{Momentum and Temperature}

{\color{red} $$\eta = (1-\mu)B\eta_0$$}

\vfill
Emprical evidence for this setting of $\eta$ is given in

\vfill
{\bf Don't Decay the Learning Rate, Increase the Batch Size}, Smith et al., 2018

\slide{Momentum as an Exponential Moving Average (EMA)}
Consider a sequence $x_1$, $x_2$, $x_3$, $\ldots$.
\vfill
For $t \geq N$, consider the average of the $N$ most recent values.
$$\overline{x}_t = \frac{1}{N} \;\; \sum_{k = 0}^{N-1}\; x_{t-k}$$

\vfill
This can be approximated efficiently with
\begin{eqnarray*}
\tilde{x}_0 & = & 0 \\
\\
\tilde{x}_t & = & \left(1-\frac{1}{N}\right)\tilde{x}_{t-1} + \left(\frac{1}{N}\right)x_t
\end{eqnarray*}

\slide{Deep Learning Convention for EMAs}

In deep learning an exponential moving average

$$\tilde{x}_t = \left(1-\frac{1}{N}\right)\tilde{x}_{t-1} + \left(\frac{1}{N}\right)x_t$$

\vfill
is written as

$$\tilde{x}_t = \beta\tilde{x}_{t-1} + (1-\beta)x_t$$

\vfill
where
$$\beta = 1 - 1/N$$

\vfill
Typical values for $\beta$ are .9, .99 or .999 corresponding to $N$ being 10, 100 or 1000.

\vfill
It will be convenient here to use $N$ rather than $\beta$.

\slide{Momentum as an EMA}

\begin{eqnarray*}
v_t & = & \mu v_{t-1} + {\color{red} \eta \hat{g}_t} \\
\\
& = & \left(1-\frac{1}{N}\right) v_{t-1} + {\color{red} \frac{1}{N}(N\eta \hat{g}_t)}
\end{eqnarray*}

\vfill
We see that $v_t$ is an EMA of $N \eta \hat{g}$.

\slide{Momentum as an EMA}

\centerline{\color{red} $v_t$ is an EMA of $N \eta \hat{g}$.}

\vfill
Alternatively, we can consider an EMA of the gradient.
{\color{red} $$\tilde{g}_t = \left(1-\frac{1}{N}\right)\tilde{g}_{t-1} + \left(\frac{1}{N}\right) \hat{g}_t$$}

\vfill
The moving average of $N\eta\hat{g}$ is the same as $N\eta$ times the moving average of $\hat{g}$.  Hence

\vfill
{\color{red} $$v_t = N \eta \tilde{g}_t$$}

\slide{Momentum as an EMA}

We have now shown that the standard formulation of momentum can be written as

\vfill
\begin{eqnarray*}
\tilde{g}_t & = & \left(1-\frac{1}{N}\right)\tilde{g}_{t-1} + \left(\frac{1}{N}\right) \hat{g}_t \\
\\
\\
\\
\Phi_{t+1} & = &  \Phi_t - N\eta\tilde{g}_t
\end{eqnarray*}

\slide{Total Effect Rule}

We will adopt the rule of thumb that the temperature is determined by the total effect of a single training gradient $g_{t,b}$.


\vfill
Also that ``temperature'' corresponds to the converged loss at fixed learning rate.

\slide{Total Effect Rule}

The effect of $g_{t,b}$ on the batch average $\hat{g}_t$ is $\left(\frac{1}{B}\right)g_{t,b}$.

\vfill
$$\mathrm{Using}\;\;\;\;\; \sum_{i = 0}^\infty \;\frac{1}{N}\left(1 - \frac{1}{N}\right)^i = 1$$

\vfill
we get that the effect of $\hat{g}_t$ on $\sum_{t=0}^\infty \tilde{g}_t$ equals $\hat{g}_t$.

\vfill
So for $\Phi_{t+1} =  \Phi_t - N\eta\tilde{g}_t$ the total effect of $g_{t,b}$ is $(N/B)\eta\; g_{t,b}$.


\slide{Total Effect Rule}

For $\Phi_{t+1} =  \Phi_t - N\eta\tilde{g}_t$ the total effect of $g_{t,b}$ is $(N/B)\eta\; g_{t,b}$.

\vfill
By taking $\eta = \frac{B}{N} \eta_0$ we get that the total effect, and hence the temperature, is determined by $\eta_0$ independent of the choice of $N$ and $B$.

\vfill
For the standard momentum paramenter $\mu = (1 - 1/N)$ this becomes

\vfill
{\color{red} $$\eta = (1-\mu)B \eta_0$$}

\vfill
where $\eta_0$ determines temperature independent of $\mu$ and $B$.

\slide{RMSProp and Adam}

RMSProp and Adam are ``adaptive'' SGD methods --- the effective learning rate is computed from statistics of the data rather than set as a fixed hyper-parameter (although
the adaptation algorithm itself still has hyper-parameters).

\vfill
Different effective learning rates are used for different model parameters.

\vfill
Adam is typically used in NLP while Vanilla SGD is typically used in vision.  This may be related to the fact that batch normalization is used in vision but not in NLP.

\slide{RMSProp}

RMSProp is based on a running average of $\hat{g}[i]^2$ for each scalar model parameter $i$.

\begin{eqnarray*}
{\color{red} s_t[i]} & {\color{red} =} & {\color{red} \left(1-\frac{1}{N_s}\right) s_{t-1}[i] + \frac{1}{N_s} \hat{g}_t[i]^2}\;\;\;\mbox{$N_s$ typically 100 or 1000} \\
\\
{\color{red} \Phi_{t+1}[i]} & {\color{red} =} & {\color{red} \Phi_t[i] - \frac{\eta}{\sqrt{s_t[i]} + \epsilon}\;\; \hat{g}_t[i]}
\end{eqnarray*}

\slide{RMSProp}
The second moment of a scalar random variable $x$ is $E\;x^2$

\vfill
The variance $\sigma^2$ of $x$ is $E \;(x - \mu)^2$ with $\mu = E\;x$.

\vfill
RMSProp uses an estimate $s[i]$ of the second moment of the random scalar $\hat{g}[i]$.

\vfill
For $g[i]$ small $s[i]$ approximates the variance of $\hat{g}[i]$.

\vfill
There is a ``centering'' option in PyTorch RMSProp that switches from the second moment to the variance.

\slide{RMSProp Motivation}
\begin{eqnarray*}
{\color{red} \Phi_{t+1}[i]} & {\color{red} =} & {\color{red} \Phi_t[i] - \frac{\eta}{\sqrt{s_t[i]} + \epsilon}\;\; \hat{g}_t[i]}
\end{eqnarray*}

\vfill
One interpretation of RMSProp is that a low variance gradient has less statistical uncertainty and hence needs less averaging before making the update.

\slide{RMSProp is Theoretically Mysterious}

{\color{red} $$\Phi[i] \;\minuseq \; \eta\;\frac{\hat{g}[i]}{\sigma[i]}\;\;(1)\hspace{5em}\Phi[i] \;\minuseq \; \eta\;\frac{\hat{g}[i]}{\sigma^2[i]}\;\;(2)$$}

\vfill
Although (1) seems to work better, (2) is better motivated theoretically.  To see this we can consider units.

\vfill
If parameters have units of ``weight'', and loss is in bits, then (2) type checks with $\eta$ having units of inverse bits --- the numerical value of $\eta$
has no dependence on the choice of the weight unit.

\vfill
Consistent with the dimensional analysis, many theoretical analyses support (2) over (1) contrary to apparent empirical performance.
\slide{Adam --- Adaptive Momentum}

Adam combines momentum and RMSProp (although PyTorch RMSProp also supports momentum).

\vfill
Adam also uses ``bias correction'' which probably accounts for it's popularity over RMSProp in practice.

\slide{Bias Correction}

Consider a standard moving average.

\begin{eqnarray*}
\tilde{x}_0 & = & 0 \\
\\
\tilde{x}_t & = & \left(1-\frac{1}{N}\right)\tilde{x}_{t-1} + \left(\frac{1}{N}\right)x_t
\end{eqnarray*}

\vfill
For $t < N$ the average $\tilde{x}_t$ will be strongly biased toward zero.

\slide{Bias Correction}

The following running average maintains the invariant that $\tilde{x}_t$ is exactly the average of $x_1,\ldots,x_t$.

\begin{eqnarray*}
\tilde{x}_t & = & \left(\frac{t-1}{t}\right)\tilde{x}_{t-1} + \left(\frac{1}{t}\right)x_t \\
\\
\\
& = & \left(1-\frac{1}{t}\right)\tilde{x}_{t-1} + \left(\frac{1}{t}\right)x_t
\end{eqnarray*}

\vfill
We now have $\tilde{x}_1 = x_1$ independent of any $x_0$.

\vfill
But this fails to track a moving average for $t >> N$.

\slide{Bias Correction}

The following avoids the initial bias toward zero while still tracking a moving average.

\begin{eqnarray*}
\tilde{x}_t & = & \left(1-\frac{1}{\min(N,t)}\right)\tilde{x}_{t-1} + \left(\frac{1}{\min(N,t)}\right)x_t
\end{eqnarray*}

\vfill
The published version of Adam has a more obscure form of bias correction which yields essentially the same effect.

\slide{Adam (simplified)}

\begin{eqnarray*}
  \tilde{g}_{t}[i] & = & \left(1-\frac{1}{\min(t,N_g)}\right)\tilde{g}_{t-1}[i] + \frac{1}{\min(t,N_g)} \hat{g}_t[i] \\
  \\
  \\
  s_{t}[i] & = & \left(1-\frac{1}{\min(t,N_s)}\right)s_{t-1}[i] + \frac{1}{\min(t,N_s)} \hat{g}_t[i]^2 \\
  \\
  \\
\Phi_{t+1}[i] & =  & \Phi_t - \frac{\eta}{\sqrt{s_{t}[i]} + \epsilon}\;\;\tilde{g}_{t}[i]
\end{eqnarray*}

\slide{Decoupling Hyperparametera}

The following reparameterization should be helpful for Adam.

\begin{eqnarray*}
N_g & = & min(1,N^0_g/B) \\
\\
\eta & = & \epsilon B \eta_0
\end{eqnarray*}

\vfill
Empirically, tuning $\epsilon$ is important.

\vfill
Some coupling remains between $\eta_0$ and $\epsilon$.

\vfill
$N_s$ should also be adapted to $B$ but this is problematic --- see the next slide.

\slide{Making $s_t[i]$ batch size invariant}

rather than
$$s_t[i] = \left(1-\frac{1}{N_s}\right) s_{t-1}[i] + \frac{1}{N_s} \hat{g}_t[i]^2$$
\vfill
we would like
\begin{eqnarray*}
s_t[i] & = &  \left(1-\frac{1}{N_s}\right) s_{t-1}[i] + \frac{1}{N_s} \left(\frac{1}{B} \sum_b \hat{g}_{t,b}[i]^2\right) \\
\\
\\
N_s & = & \min\left(1,N^0_s/B\right)\;\;\;\mbox{$N^0_s$ optimal for $B=1$}
\end{eqnarray*}

\slide{Making $s_t[i]$ Batch Size Invariant}

In PyTorch this is difficult because ``optimizers'' are defined as a function of $\hat{g}_t$.

\vfill
$\hat{g}_t[i]$ is not sufficient for computing $\sum_b \;\hat{g}_{t,b}[i]^2$.


\vfill
To compute $\sum_b \;\hat{g}_{t,b}[i]^2$ we need to modify the backward method of all PyTorch objects!

\slide{Summary}

We have considered Vanilla SGD, Momentum, RMSProp and Adam.

\vfill
Vanilla tends to be used in vision while Adam tends to be used in NLP.

\vfill
Reparameterization of the PyTorch hyperparameters can decouple hyperparameters simplifying hyperparameter search.


\slide{END}

} \end{document}

