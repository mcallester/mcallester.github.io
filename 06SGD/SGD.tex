\input ../SlidePreamble
\input ../preamble

\newcommand{\solution}[1]{\bigskip {\bf Solution}: #1}

\begin{document}

{\Huge
  \centerline{\bf TTIC 31230, Fundamentals of Deep Learning}
  \bigskip
  \centerline{David McAllester, Autumn 2023}
  \vfill
  \centerline{\bf Stochastic Gradient Descent (SGD)}
  \vfill
  \vfill
  \centerline{\bf The Classical Convergence Theorem}
  \vfill
  \centerline{\bf The Learning Rate as Temperature}
  \vfill
  \centerline{\bf Temperature, Batch Size, Momentum, and Adam}

\slide{Vanilla SGD}

$$\Phi_{t+1} = \Phi_t - \eta_t \hat{g}$$

\vfill
\begin{eqnarray*}
  \hat{g} & = & E_{(x,y) \sim \mathrm{Batch}}\;\nabla_\Phi\;{\cal L}(\Phi,x,y) \\
  \\
  \\
  g & = & E_{(x,y) \sim \mathrm{Pop}}\;\nabla_\Phi\;{\cal L}(\Phi,x,y) \\
\end{eqnarray*}

\vfill
\centerline{$\eta_t$ is the learning rate for time $t$.}


\slide{Issues}

\vfill
\begin{itemize}
\item {\bf Gradient Noise:} The accuracy of $\hat{g}$ as an estimate of $g$.

  \vfill
\item {\bf Gradient Drift:(second order structure)} The fact that $g$ changes as the parameters change.

  \vfill
\item {\bf Convergence:} Convergence requires $\eta_t \rightarrow 0$.

  \vfill
  \item {\bf Exploration:} Reducing $\eta$ slowly allows better exploration of model parameters.
\end{itemize}

\slide{The Classical Convergence Theorem}

$$\Phi \;\minuseq \; \eta_t \nabla_\Phi\;{\cal L}(\Phi,x_t,y_t)$$

\vfill
For ``sufficiently smooth'' non-negative loss with

\vfill
$$\eta_t \geq 0\;\;\;\;\;\;\;\;\lim_{t \rightarrow \infty} \;\eta_t = 0\;\;\;\;\;\;\;\;\sum_t \eta_t = \infty \;\;\;\;\;\;\sum_t \;\eta_t^2 < \infty$$

\vfill
we have that the training loss $E_{(x,y) \sim \mathrm{Train}}\; {\cal L}(\Phi,x,t)$ converges to a limit and any limit point of the sequence $\Phi_t$
is a stationary point in the sense that {\huge  $\nabla_\Phi \; E_{(x,y) \sim \mathrm{Train}} \;{\cal L}(\Phi,x,t) = 0$}.

\vfill
{\Large
\vfill
{\bf Rigor Police:} One can construct cases where $\Phi$ diverges to infinity, converges to a saddle point, or even converges to a limit cycle.

}

\slide{Physicist's Proof of the Convergence Theorem}

Since $\lim_{t \rightarrow 0} \;\eta_t = 0$ we will eventually get to arbitrarilly small learning rates.

\vfill
For sufficiently small learning rates any meaningful update of the parameters will be based on an arbitrarily large sample
of gradients at essentially the same parameter value.

\vfill
An arbitrarily large sample will become arbitrarily accurate as an estimate of the full gradient.

\vfill
But since $\sum_t \eta_t = \infty$, no matter how small the learning rate gets, we still can make arbitrarily large motions in parameter space.


\vfill
{\Large
\vfill
For a rigorous proof see Neuro-Dynamic Programming, Bertsekas and Tsitsiklis, 1996.}

\slidetwo{SGD as a form of MCMC}{Learning Rate as a Temperature Parameter}

\centerline{\includegraphics[height= 4in]{\images/AnnealingSGD}}
\centerline{\Large Gao Huang et. al., ICLR 2017}


\slide{Temperature}

Physical temperature is a relationship between the energy and probability.

\vfill
$$P(x) = \frac{1}{Z} \;e^{\frac{-E(x)}{kT}} \;\;\;\;\;\;\;Z = \sum_x\; e^{\frac{-E(x)}{kT}}$$

\vfill
This is called the Gibbs or Boltzman distribution.

\vfill
$E(x)$ is the energy of physical microstate state $x$.


\vfill
$k$ is Boltzman's constant.

\vfill
$Z$ is called the partition function.

\slide{Temperature}

Boltzman's constant can be measured using the ideal gas law.

\begin{eqnarray*}
pV & = & NkT \\
\\
p & = & \mathrm{pressure} \\
V & = & \mathrm{volume} \\
N & = & \mbox{the number of molecules} \\
T & = & \mathrm{temperature} \\
k & = & \mbox{Boltzman's constant}
\end{eqnarray*}

\vfill
We can measure $p$, $V$, $N$ and $T$ and solve for $k$.


\slide{Temperature}

The Gibbs distribution is typically written as

$$P(x) = \frac{1}{Z}\;e^{-\beta E(x)}$$

\vfill

$\beta = \frac{1}{kT}$ is the (inverse) temperature parameter.

\vfill
``Hot'' is when $\beta$ is small and ``cold'' is when $\beta$ is large (confusing).

\slidetwo{Loss as Energy}{Learning Rate as Temperature}

A finite learning rate defines an equalibrium probability distribution (or density) over the model parameters.

\vfill
Each value of the model parameters has an associated loss.

\vfill
The distribution over model parameters defines a distribution over loss.

\slidetwo{Loss as Energy}{Learning Rate as Temperature}

\centerline{\includegraphics[width = 7in]{\images/annealing}}

\vfill
Equalibrium energy (loss) distributions at three different temperatures (learning rates).

\vfill
\centerline{\parbox{7in}
{\Large
Plots are from the ResNet paper.  Left plot is for CNNs without residual skip connections, the right plot is ResNet. Thin lines are training error, thick lines are validation error. In all cases $\eta$ is reduced twice, each time by a factor of 2.
}}

\slidetwo{Loss as Energy}{Learning Rate as Temperature}

\centerline{\includegraphics[width = 4in]{\images/annealing}}

\vfill
The learnng rate is always reduced over time.  The profile of learning rate reduction is called {\bf the learning rate schedule}.  An older convention (shown here) is to reduce the learning rate by a factor of 2 in steps.

\vfill
Modern transformer training first quickly and smoothly ramps up the learning rate (the warm-up phase) up and then slowly and smoothly ramps it down.

\slide{Batch Size and Temperature}

Vanilla SGD with minibatching typically uses the following update which defines the meaning of $\eta$.

\begin{eqnarray*}
\Phi_{t+1} & \;\minuseq\; & \eta \hat{g}_t \\
\\
\hat{g}_t & = & \frac{1}{B} \sum_b \hat{g}_{t,b}
\end{eqnarray*}

\vfill
Here $\hat{g}_{b}$ is the average gradient over the batch.

\vfill
Under this update {\bf increasing the batch size (while holding $\eta$ fixed) reduces the temperature.}

\slide{Making Temperature Independent of $B$}

For batch size 1 with learning rate $\eta_0$ we have

\begin{eqnarray*}
\Phi_{t+1} & = &  \Phi_{t} - \eta_0\;\nabla_\Phi {\cal L}(t,\Phi_{t}) \\
\\
\Phi_{t+B} & = &  \Phi_{t} - \sum_{b=0}^{B-1} \;\eta_0\;\nabla_\Phi {\cal L}(t+b,\Phi_{\color{red} t+b-1}) \\
\\
& \approx & \Phi_t - \eta_0 \sum_b \nabla_\Phi {\cal L}(t+b,\Phi_{\color{red} t}) \\
\\
& = & \Phi_t - B\eta_0\; \hat{g}_t
\end{eqnarray*}

\vfill
For batch updates $\Phi_{t+1} = \Phi_t - B\eta_0\; \hat{g}_t$ the temperature is essentially determined by $\eta_0$ independent of $B$.

\slideplain{Making Temperature Independent of $B$}

In 2017 it was discovered that setting $\eta = B\eta_0$ allows very large (highly parallel)
batches.

\vfill
{\bf Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour}, Goyal et al., 2017.



\slide{EMA Momentum}

Momentum in general is equivalent to using an exponential moving average (EMA) of the gradient.

\vfill
EMA momentum is parameterized directly as an EMA.

\vfill
Traditional momentum (momentum in Vanilla SGD in PyTorch) defines the parameters differently.

\vfill
The Adam optimizer uses the EMA parameterization which is cleaner and which we describe first.

\slide{Exponential Moving Average (EMA)}
Consider a sequence $x_1$, $x_2$, $x_3$, $\ldots$.
\vfill
For $t \geq N$, the {\bf moving average} of the $N$ most recent values is
$$\overline{x}_t = \frac{1}{N} \;\; \sum_{k = 0}^{N-1}\; x_{t-k}$$

\vfill
The corresponding {\bf exponential moving average} is
\begin{eqnarray*}
\tilde{x}_0 & = & 0 \\
\\
\tilde{x}_t & = & \left(1-\frac{1}{N}\right)\tilde{x}_{t-1} + \left(\frac{1}{N}\right)x_t
\end{eqnarray*}

\slide{Exponential Moving Average (EMA)}
\begin{eqnarray*}
\tilde{x}_t & = & \left(1-\frac{1}{N}\right)\tilde{x}_{t-1} + \left(\frac{1}{N}\right)x_t \\
\\
\\
&= & \frac{1}{N}\sum_{s=1}^t \left[\left(1-\frac{1}{N}\right)^{t-s} x_s\right] \;\;\approx\;\; \frac{1}{N} \sum_{s=1}^t\;e^{-(s-t)/N} \;x_s 
\end{eqnarray*}

\vfill
$$\sum_{i=0}^\infty \left(1-\frac{1}{N}\right)^i = N$$

\slide{The Conventional Formulation of EMAs}

$$\tilde{x}_t = \left(1-\frac{1}{N}\right)\tilde{x}_{t-1} + \left(\frac{1}{N}\right)x_t$$

\vfill
is written as

$$\tilde{x}_t = \beta\tilde{x}_{t-1} + (1-\beta)x_t$$

\vfill
where
$$\beta = 1 - 1/N$$

\vfill
But we can use any $\beta \in [0,1)$.

\vfill
In deep learning models typical values for $\beta$ are .9, .99 or .999 corresponding to $N$ being 10, 100 or 1000.

\slide{EMA Momentum:}

\begin{eqnarray*}
  \tilde{g}_0 & = & 0 \\
  \\
  \tilde{g}_{t} & = & \left(1-\frac{1}{N}\right)\tilde{g}_{t-1} + \frac{1}{N} \hat{g}_t \\
  \\
  \\
  & = & \beta \tilde{g}_{t-1} + (1-\beta)\hat{g}_t \\
  \\             
  \\
  \Phi_{t+1} & =  & \Phi_t - \eta\tilde{g}_{t}
\end{eqnarray*}


\slide{EMA Momentum: Temperature is Independent of $N$}

The temperature is determined by the total effect of a single training gradient $\hat{g}$.
With EMA momentum the totat contribution of a single $\hat{g}_t$ is

$$\frac{1}{N} \sum_{i = 0}^\infty \left(1 - \frac{1}{N}\right)^i = 1$$

\vfill
Hence the gradient estimate $\hat{g}$ has in fluence $\eta$ for any value of $N$.

\vfill
If $N$ is small the model does not change significantly in $N$ updates and
temperature is independent of momentum parameter $N$.


\slide{Momentum}

The theory of momentum is generally given in terms of second order structure (gradient drift).

\vfill
Taking second order structure into account (where the model changes over $N$ iterations) momentum can effect the training.

\vfill
But the EMA parameterization reduces the effect of N on temperature.

\slideplain{Vanilla SGD Momentum Parameterization}

The standard (PyTorch) momentum SGD equations are

\begin{eqnarray*}
  {\color{red} v_t} & {\color{red} =} & {\color{red} \mu v_{t-1} + \eta * \hat{g}_t} \;\;\;\mbox{$\mu$ is typically .9 or .99}\\
  \\
  {\color{red} \Phi_{t+1}} & {\color{red} =} & {\color{red} \Phi_t -  v_t} \\
\end{eqnarray*}

\vfill
Here $v$ is velocity, $0 \leq \mu < 1$ represents friction drag and $\eta \hat{g}$ is the acceleration generated by the gradient force.

\slidetwo{Decoupling Learning rate, Batch Size, and Momentum}
{(Vanilla SGD parameterization of Momentum)}

\begin{eqnarray*}
  {\color{red} v_t} & {\color{red} =} & {\color{red} \mu v_{t-1} + \eta * \hat{g}_t} \;\;\;\mbox{$\mu$ is typically .9 or .99}\\
  \\
  {\color{red} \Phi_{t+1}} & {\color{red} =} & {\color{red} \Phi_t -  v_t}
\end{eqnarray*}

\vfill
{\color{red} $$\eta = (1-\mu)B\eta_0$$}

\slide{Momentum and Temperature}

{\color{red} $$\eta = (1-\mu)B\eta_0$$}

\vfill
Emprical evidence for this setting of $\eta$ is given in

\vfill
{\bf Don't Decay the Learning Rate, Increase the Batch Size}, Smith et al., 2018

\slide{Adam}

Adaptive SGD with Momentum (Adam).

\vfill
Adam is derived from RMSProp which first appeared in lecture slides by Hinton.

\vfill
``Adaptive'' means that Different learning rates are used for different parameters.

\slide{Adam}

Adam uses the EMA parameterization of momentum.

\vfill
PyTorch RMSProp also has momentum but with the vanilla parameterization.
\vfill
This choice of momentum  parameterization may be an important reason Adam is prefered in practice.

\slide{Adam (Without Bias Correction)}

\begin{eqnarray*}
{ \tilde{g}_t[i]} & { =} & { \left(1-\frac{1}{N_s}\right) \tilde{g}_{t-1}[i] + \frac{1}{N_s} \hat{g}_t[i]}\;\;\;\mbox{$N_s$ typically 100 or 1000} \\
\\
\\
{ s_t[i]} & { =} & { \left(1-\frac{1}{N_s}\right) s_{t-1}[i] + \frac{1}{N_s} \hat{g}_t[i]^2}\;\;\;\mbox{$N_s$ typically 100 or 1000} \\
\\
\\
{ \Phi_{t+1}[i]} & { =} & { \Phi_t[i] - \frac{\eta}{\sqrt{s_t[i]} + \epsilon}\;\; \tilde{g}_t[i]}
\end{eqnarray*}

\slide{Adam as Gradient Normalization}

One can think of Adam as gradient normalization.

\vfill
While normalization layers normalize the values, gradient normalization normalizes the gradients.

\slide{Adam (Centered)}

\begin{eqnarray*}
{ \tilde{g}_t[i]} & { =} & { \left(1-\frac{1}{N_s}\right) tilde{g}_{t-1}[i] + \frac{1}{N_s} \hat{g}_t[i]}\;\;\;\mbox{$N_s$ typically 100 or 1000} \\
\\
\\
{ s_t[i]} & { =} & { \left(1-\frac{1}{N_s}\right) s_{t-1}[i] + \frac{1}{N_s} \hat{g}_t[i]^2}\;\;\;\mbox{$N_s$ typically 100 or 1000} \\
\\
\\
{ \Phi_{t+1}[i]} & { =} & { \Phi_t[i] - \frac{\eta}{\sqrt{s_t[i] {\color{red} - \tilde{g}[i]^2}} + \epsilon}\;\; \tilde{g}_t[i]}
\end{eqnarray*}

\slide{A Noise Analysis of Centered Adam}

\begin{eqnarray*}
{\color{red} \Phi_{t+1}[i]} & {\color{red} =} & {\color{red} \Phi_t[i] - \frac{\eta}{\sigma[i] + \epsilon}\;\; \tilde{g}_t[i]}
\end{eqnarray*}

\vfill
One interpretation of Adam involves the gradient noise variance $\sigma[i]$.

\vfill
A low-noise measurement estimate of the gradient is more certain.

\vfill
A more certain gradient estimate needs less averaging.

\vfill
Less averaging is equivalent to a larger learning rate.

\slide{Problem with the Analysis}

{\color{red} $$\Phi[i] \;\minuseq \; \eta\;\frac{\tilde{g}[i]}{\sqrt{E \hat{g}^2}}\;\;(1)\hspace{5em}\Phi[i] \;\minuseq \; \eta\;\frac{\tilde{g}[i]}{\sigma^2[i]}\;\;(2)$$}

\vfill
The noise analysis yields (2) but (1) works better.

\vfill
The ``gradient normalization'' interpretation seems more relevant to practice.

\vfill
However, the theory of gradient normalization seems unclear.

\slide{Bias Correction}

Consider a standard moving average.

\begin{eqnarray*}
\tilde{x}_0 & = & 0 \\
\\
\tilde{x}_t & = & \left(1-\frac{1}{N}\right)\tilde{x}_{t-1} + \left(\frac{1}{N}\right)x_t
\end{eqnarray*}

\vfill
For $t < N$ the average $\tilde{x}_t$ will be strongly biased toward zero.

\slide{Bias Correction}

The following running average maintains the invariant that $\tilde{x}_t$ is exactly the average of $x_1,\ldots,x_t$.

\begin{eqnarray*}
\tilde{x}_t & = & \left(\frac{t-1}{t}\right)\tilde{x}_{t-1} + \left(\frac{1}{t}\right)x_t \\
\\
\\
& = & \left(1-\frac{1}{t}\right)\tilde{x}_{t-1} + \left(\frac{1}{t}\right)x_t
\end{eqnarray*}

\vfill
We now have $\tilde{x}_1 = x_1$ independent of any $x_0$.

\vfill
But this fails to track a moving average for $t >> N$.

\slide{Bias Correction}

The following avoids the initial bias toward zero while still tracking a moving average.

\begin{eqnarray*}
\tilde{x}_t & = & \left(1-\frac{1}{\min(N,t)}\right)\tilde{x}_{t-1} + \left(\frac{1}{\min(N,t)}\right)x_t
\end{eqnarray*}

\vfill
The published version of Adam has a more subtle form of bias correction which yields the same effect.

\slide{Adam}

\begin{eqnarray*}
  \tilde{g}_{t}[i] & = & \left(1-\frac{1}{\min(t,N_g)}\right)\tilde{g}_{t-1}[i] + \frac{1}{\min(t,N_g)} \hat{g}_t[i] \\
  \\
  \\
  s_{t}[i] & = & \left(1-\frac{1}{\min(t,N_s)}\right)s_{t-1}[i] + \frac{1}{\min(t,N_s)} \hat{g}_t[i]^2 \\
  \\
  \\
\Phi_{t+1}[i] & =  & \Phi_t - \frac{\eta}{\sqrt{s_{t}[i]} + \epsilon}\;\;\tilde{g}_{t}[i]
\end{eqnarray*}

\slide{Decoupling Hyperparameters}

The following reparameterization should be helpful for Adam.

\begin{eqnarray*}
N_g & = & \min(1,N^0_g/B) \\
\\
N_s & = & \min(1,N^0_s/B) \\
\\
\epsilon & = & \epsilon_0\sqrt{B} \\
\\
\eta & = & \epsilon B \eta_0
\end{eqnarray*}

\slide{Stochastic Gradient Descent (SGD)}

  \centerline{\bf The Classical Convergence Theorem}
  \vfill
  \centerline{\bf The Learning Rate as Temperature}
  \vfill
  \centerline{\bf Temperature, Batch Size, Momentum, and Adam}

\slide{END}

} \end{document}

