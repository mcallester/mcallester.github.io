\input ../SlidePreamble
\input ../preamble

\newcommand{\solution}[1]{\bigskip {\bf Solution}: #1}

\begin{document}

{\Huge
  \centerline{\bf TTIC 31230, Fundamentals of Deep Learning}
  \bigskip
  \centerline{David McAllester, Autumn 2023}
  \vfill
  \centerline{\bf Stochastic Gradient Descent (SGD)}
  \vfill
  \vfill
  \centerline{\bf Temperature, Batch Size, Momentum, and Adam}

\slide{Vanilla SGD}

\vfill
\begin{eqnarray*}
  \hat{g} & = & E_{(x,y) \sim \mathrm{Batch}}\;\nabla_\Phi\;{\cal L}(\Phi,x,y)
\end{eqnarray*}

$$\Phi_{t+1} = \Phi_t - {\color{red} \eta_t}\; \hat{g}$$


\vfill
\centerline{$\eta_t$ is the learning rate for time $t$.}


\slide{The Learning Rate Schedule}

At the beginning a large learning rate reduces the loss faster. Near the end a small learning rate reaches a lower loss.

\vfill
How the learning rate changes over time is called {\bf the learning rate schedule}.

\vfill
In the early days of deep learning the learning rate was reduced by a factor of 2 in steps (as above).

\vfill
\centerline{\includegraphics[width = 5in]{\images/annealing}}

\slide{Temperature}

\centerline{\includegraphics[width = 5in]{\images/annealing}}

{\huge
\vfill
As the learning rate is decreased (here by factors of 2) the asymptotic loss decreases.

\vfill
We will say that two setting of hyper-parameters (for example $\eta$ and the batch size $B$) run at the same ``temperature'' if
they result in the same asymptotic loss value.

\vfill
Two setting of hyper-parameters (such as the learning rate $\eta$ and batch size $B$) will behave similarly if they have the same {\bf temperature schedule}
(as opposed to learning rate schedule).
}

\slide{Physical Temperature}

{\huge
Physical temperature is a relationship between the energy of a system state and its probability.

\vfill
$$P(x) = \frac{1}{Z} \;e^{\frac{-E(x)}{kT}} \;\;\;\;\;\;\;Z = \sum_x\; e^{\frac{-E(x)}{kT}}$$

\vfill
This is called the Gibbs or Boltzman distribution.

\vfill
$E(x)$ is the energy of physical microstate state $x$.


\vfill
$k$ is Boltzman's constant.

\vfill
$Z$ is called the partition function.

\vfill
We will eventually show that under certain (somewhat special) conditions the asymptotic probability distribution over model parameters is a Gibbs distribution
where the energy is the loss.
}

\slide{Temperature}

The Gibbs distribution is typically written as

$$P(x) = \frac{1}{Z}\;e^{-\beta E(x)}$$

\vfill

$\beta = \frac{1}{kT}$ is the (inverse) temperature parameter.

\vfill
``Hot'' is when $\beta$ is small and ``cold'' is when $\beta$ is large (confusing).

\slide{Batch Size and Temperature}

PyTorch vanilla SGD uses the following update which defines a meaning of the learning rate $\eta$.

\begin{eqnarray*}
\Phi_{t+1} & \;\minuseq\; & \eta \hat{g}_t \\
\\
\hat{g}_t & = & \frac{1}{B} \sum_b \hat{g}_{t,b}
\end{eqnarray*}

\vfill
Here $\hat{g}_{t}$ is the average gradient over the batch.

\vfill
For PyTorch vanilla SGD both the batch size and the learning rate influence temperature (not good).

\slide{Making Temperature Independent of Batch Size}

{\huge
For batch size 1 with learning rate $\eta_0$ we have

\begin{eqnarray*}
\Phi_{t+1} & = &  \Phi_{t} - \eta_0\;\nabla_\Phi {\cal L}(t,\Phi_{t}) \\
\\
\Phi_{t+B} & = &  \Phi_{t} - \sum_{b=0}^{B-1} \;\eta_0\;\nabla_\Phi {\cal L}(t+b,\Phi_{\color{red} t+b-1}) \\
\\
& \approx & \Phi_t - \eta_0 \sum_b \nabla_\Phi {\cal L}(t+b,\Phi_{\color{red} t}) \\
\\
& = & \Phi_t - B\eta_0\; \hat{g}_t
\end{eqnarray*}

\vfill
{\bf For batch updates $\Phi_{t+1} = \Phi_t - B\eta_0\; \hat{g}_t$ the temperature is essentially determined by $\eta_0$ independent of $B$.}
}

\slideplain{Making Temperature Independent of $B$}

In 2017 it was discovered that setting $\eta = B\eta_0$ allows very large (highly parallel)
batches.

\vfill
{\bf Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour}, Goyal et al., 2017.


\slide{Momentum}

Momentum is another hyper-parameter in Deep Learning.

\vfill
Momentum uses a moving average of the gradient.

\vfill
Consider a sequence $x_1$, $x_2$, $x_3$, $\ldots$.
\vfill
For $t \geq N$, the {\bf rolling average} of the $N$ most recent values is
$$\overline{x}_t = \frac{1}{N} \;\; \sum_{k = 0}^{N-1}\; x_{t-k}$$

\vfill
This is awkward to compute and is influenced by both the value added and the value removed from the average.

\slide{Exponential Moving Average (EMA)}

\vfill
Consider a sequence $x_1$, $x_2$, $x_3$, $\ldots$.

\vfill
The corresponding {\bf exponential moving average (EMA)} is
\begin{eqnarray*}
\tilde{x}_0 & = & 0 \\
\\
\tilde{x}_t & = & \left(1-\frac{1}{N}\right)\tilde{x}_{t-1} + \left(\frac{1}{N}\right)x_t
\end{eqnarray*}

\vfill
This behaves like the rolling average of the last N values but is easy to compute and does not involve removing any particular old value.

\slide{Exponential Moving Average (EMA)}
\begin{eqnarray*}
\tilde{x}_t & = & \left(1-\frac{1}{N}\right)\tilde{x}_{t-1} + \left(\frac{1}{N}\right)x_t \\
\\
\\
&= & \frac{1}{N}\sum_{i=0}^{t-1} \left[\left(1-\frac{1}{N}\right)^i x_{t-i}\right]
\end{eqnarray*}

\vfill
$$\sum_{i=0}^\infty \left(1-\frac{1}{N}\right)^i = N$$

\slide{The Conventional Formulation of EMAs}

$$\tilde{x}_t = \left(1-\frac{1}{N}\right)\tilde{x}_{t-1} + \left(\frac{1}{N}\right)x_t$$

\vfill
is written as

$$\tilde{x}_t = \beta\tilde{x}_{t-1} + (1-\beta)x_t$$

\vfill
where
$$\beta = 1 - 1/N$$

\vfill
But we can use any $\beta \in [0,1)$.

\vfill
In deep learning we typically take $\beta$ to be .9, .99 or .999 corresponding to $N$ being 10, 100 or 1000.

\slide{EMA Momentum}

\begin{eqnarray*}
  \tilde{g}_0 & = & 0 \\
  \\
  \tilde{g}_{t} & = & \left(1-\frac{1}{N}\right)\tilde{g}_{t-1} + \frac{1}{N} \hat{g}_t,\;\;\;N \geq 1 \\
  \\
  \\
  & = & \beta \tilde{g}_{t-1} + (1-\beta)\hat{g}_t,\;\;\;\beta \in [0,1) \\
  \\             
  \\
  \Phi_{t+1} & =  & \Phi_t - \eta\tilde{g}_{t}
\end{eqnarray*}

\slide{EMA Momentum: Temperature is Independent of $N$}

Unless the batch size is extremely large (larger than possible for academics),
the temperature is determined by the total effect of a single training gradient $\hat{g}$.

\vfill
With EMA momentum the total contribution of a single $\hat{g}_t$ is

{\huge
\begin{eqnarray*}
& & \frac{1}{N} \sum_{i = 0}^\infty \left(1 - \frac{1}{N}\right)^i \\
\\
& = & (1-\beta)  \sum_{i = 0}^\infty \beta^i \\
\\
& = & 1
\end{eqnarray*}
}

\vfill
Hence the temperature is determined by $\eta$ independent of $N$.

\slideplain{Heavy Ball Momentum}

The PyTorch implementation of vanilla SGD uses ``heavy ball momentum''.

\begin{eqnarray*}
  { v_t} & { =} & { \mu v_{t-1} + \eta * \hat{g}_t}\\
  \\
 & { =} & { \mu v_{t-1} + (1-\mu)\frac{\eta}{1-\mu} * \hat{g}_t}\\
  \\
  { \Phi_{t+1}} & { =} & { \Phi_t -  v_t} \\
\end{eqnarray*}

In PyTorch vanilla SGD the temperature depends on all three hyper parameters --- $\mu$,  $B$, and $\eta$ (not good).

\slide{Heavy Ball Momentum}
For PyTorch vanilla SGD we can set $\eta$ by

\vfill
$$\eta = (1-\mu)B\eta_0$$

\vfill
For academic batch sizes this setting of $\eta$ makes the temperature depend on $\eta_0$ independent of $\mu$ and $B$.

\slide{Adaptive SGD with Momentum (Adam)}

Adam introduces even more hyper-parameters.

\vfill
Adam is derived from RMSProp which first appeared in lecture slides by Hinton.

\vfill
``Adaptive'' means that Different learning rates are used for different parameters.

\slide{Adam}

Adam uses the EMA momentum.

\vfill
PyTorch RMSProp uses heavy ball parameterization (not good).

\vfill
The choice of momentum  parameterization may be an important reason Adam is prefered over RMSprop in practice --- hyper-parameter tuning
is much easier when temperature is determined by a single parameter.

\slide{Adam (Without Bias Correction)}

\begin{eqnarray*}
{ \tilde{g}_t[i]} & { =} & \beta_g\;\tilde{g}_{t-1}[i] + (1-\beta_g)\hat{g}_t[i]\;\;\;\mbox{$\beta_g$ typically .9 or .99} \\
\\
\\
{ s_t[i]} & { =} & { \beta_s s_{t-1}[i] + (1-\beta_s)\hat{g}_t[i]^2}\;\;\;\mbox{$\beta_s$ typically .99 or .999} \\
\\
\\
{ \Phi_{t+1}[i]} & { =} & { \Phi_t[i] - \eta\;\frac{\tilde{g}_t[i]}{\sqrt{s_t[i]} + \epsilon}}
\end{eqnarray*}

\slide{Adam as Gradient Normalization}

We can set $\beta_g = 0$ without influencing temperature.  Hence Adam should be approximately equivalent to


\begin{eqnarray*}
{ s_t[i]} & { =} & { \beta_s s_{t-1}[i] + (1-\beta_s)\hat{g}_t[i]^2} \\
\\
\\
{ \Phi_{t+1}[i]} & { =} & { \Phi_t[i] - \eta\;\frac{g_t[i]}{\sqrt{s_t[i]} + \epsilon}}
\end{eqnarray*}

\vfill
While normalization layers normalize the values, Adam normalizes the gradients.

\slide{Bias Correction}

Consider a standard EMA.

\begin{eqnarray*}
\tilde{x}_0 & = & 0 \\
\\
\tilde{x}_t & = & \left(1-\frac{1}{N}\right)\tilde{x}_{t-1} + \left(\frac{1}{N}\right)x_t
\end{eqnarray*}

\vfill
For $t < N$ the average $\tilde{x}_t$ will be strongly biased toward zero.

\slide{Bias Correction}

The following running average maintains the invariant that $\tilde{x}_t$ is exactly the average of $x_1,\ldots,x_t$.

\begin{eqnarray*}
\tilde{x}_t & = & \left(\frac{t-1}{t}\right)\tilde{x}_{t-1} + \left(\frac{1}{t}\right)x_t \\
\\
\\
& = & \left(1-\frac{1}{t}\right)\tilde{x}_{t-1} + \left(\frac{1}{t}\right)x_t
\end{eqnarray*}

\vfill
We now have $\tilde{x}_1 = x_1$ independent of any $x_0$.

\vfill
But this fails to track a moving average for $t >> N$.

\slide{Bias Correction}

The following avoids the initial bias toward zero while still tracking a moving average.

\begin{eqnarray*}
\tilde{x}_t & = & \left(1-\frac{1}{\min(N,t)}\right)\tilde{x}_{t-1} + \left(\frac{1}{\min(N,t)}\right)x_t
\end{eqnarray*}

\vfill
The PyTorch version of Adam has a more subtle form of bias correction which yields the same effect.

\slide{Adam}

\begin{eqnarray*}
  \tilde{g}_{t}[i] & = & \left(1-\frac{1}{\min(t,N_g)}\right)\tilde{g}_{t-1}[i] + \frac{1}{\min(t,N_g)} \hat{g}_t[i] \\
  \\
  \\
  s_{t}[i] & = & \left(1-\frac{1}{\min(t,N_s)}\right)s_{t-1}[i] + \frac{1}{\min(t,N_s)} \hat{g}_t[i]^2 \\
  \\
  \\
\Phi_{t+1}[i] & =  & \Phi_t - \frac{\eta}{\sqrt{s_{t}[i]} + \epsilon}\;\;\tilde{g}_{t}[i]
\end{eqnarray*}

\slide{Decoupling Hyper-Parameters}

{\huge
The following reparameterization should be helpful for hyper-parameter tuning for Adam.  Here the independent
hyperparameters are $N^0_g, N^0_s$, $B$, $\epsilon_0$ and $\eta_0$.  Temperature should depend primarily on $\eta_0$ independent of
the other parameters and the procedure gracefully converges to vanilla SGD in the limit as $\epsilon_0 \rightarrow \infty$.

\begin{eqnarray*}
N_g & = & (1-\beta_g) = \min(1,N^0_g/B) \\
\\
N_s & = & (1-\beta_s) = \min(1,N^0_s/B) \\
\\
\epsilon & = & \epsilon_0\sqrt{B} \\
\\
\eta & = & \epsilon B \eta_0
\end{eqnarray*}
}
\slide{Stochastic Gradient Descent (SGD)}

  \centerline{\bf The Classical Convergence Theorem}
  \vfill
  \centerline{\bf The Learning Rate as Temperature}
  \vfill
  \centerline{\bf Temperature, Batch Size, Momentum, and Adam}

\slide{END}

} \end{document}


\slide{The Classical Convergence Theorem}

$$\Phi \;\minuseq \; \eta_t \nabla_\Phi\;{\cal L}(\Phi,x_t,y_t)$$

\vfill
For ``sufficiently smooth'' non-negative loss with

\vfill
$$\eta_t \geq 0\;\;\;\;\;\;\;\;\lim_{t \rightarrow \infty} \;\eta_t = 0\;\;\;\;\;\;\;\;\sum_t \eta_t = \infty \;\;\;\;\;\;\sum_t \;\eta_t^2 < \infty$$

\vfill
we have that the training loss $E_{(x,y) \sim \mathrm{Train}}\; {\cal L}(\Phi,x,t)$ converges to a limit and any limit point of the sequence $\Phi_t$
is a stationary point in the sense that {\huge  $\nabla_\Phi \; E_{(x,y) \sim \mathrm{Train}} \;{\cal L}(\Phi,x,t) = 0$}.

\vfill
{\Large
\vfill
{\bf Rigor Police:} One can construct cases where $\Phi$ diverges to infinity, converges to a saddle point, or even converges to a limit cycle.

}

\slide{Physicist's Proof of the Convergence Theorem}

Since $\lim_{t \rightarrow 0} \;\eta_t = 0$ we will eventually get to arbitrarilly small learning rates.

\vfill
For sufficiently small learning rates any meaningful update of the parameters will be based on an arbitrarily large sample
of gradients at essentially the same parameter value.

\vfill
An arbitrarily large sample will become arbitrarily accurate as an estimate of the full gradient.

\vfill
But since $\sum_t \eta_t = \infty$, no matter how small the learning rate gets, we still can make arbitrarily large motions in parameter space.


\vfill
{\Large
\vfill
For a rigorous proof see Neuro-Dynamic Programming, Bertsekas and Tsitsiklis, 1996.}
