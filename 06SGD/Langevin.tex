\input ../SlidePreamble
\input ../preamble

\begin{document}

{\Huge
  \centerline{\bf TTIC 31230, Fundamentals of Deep Learning}
  \bigskip
  \centerline{David McAllester, Autumn 2020}
  \vfill
  \centerline{\bf Stochastic Gradient Descent (SGD)}
  \vfill
  \centerline{\bf and Stochastic Differential Equations (SDEs)}
  \vfill
  \vfill
  \vfill

\slide{Modeling the SGD as a Stochastic Process}

If we randomly select training points then SGD is a stochastic process.

\vfill
Can we analytically solve for stationary distributions?

\vfill
Is the stationary distribution Gibbs --- is it $\frac{1}{Z}e^{-\frac{{\cal L}}{kT}}$ where the temperature $T$ is determined by the learning rate?


\slide{Modeling the SGD as a Stochastic Process}

It is possible to model both the stationary distribution and non-stationary stochastic dynamics
with a {\color{red} continuous time} stochastic differential equation such as {\color{red} Brownian motion}
or {\color{red} Langevin Dynamics}.

\vfill
Langevin Dynamics is the special case where the stationary distribution is Gibbs.

\vfill
We will show here that in general the stationary distribution of SGD is not Gibbs and hence does not correspond to Langevin dynamics.

\slide{Holding $\eta$ Fixed}


\vfill
Consider SGD with batch size 1.

$$\Phi_{i+1} = \Phi_i - \eta\hat{g}_i$$

\vfill
Unlike gradient flow, we now hold $\eta > 0$ fixed.

\vfill
We will still take ``time'' to be the sum of the learning rates over the updates.

\vfill
For $N$ steps of SGD we define $\Delta t = N \eta$

\slide{Holding $\eta$ Fixed}

We consider $\Delta t$ large enough so that $\Delta t$ corresponds to many SGD updates.

\vfill
We consider $\Delta t$ small enough so that the gradient estimate distribution does not change over the interval $\Delta t$.

\slide{Applying the Central Limit Theorem}

If the mean gradient $g(\Phi)$ is approximately constant over the interval $\Delta t = N \eta$ we have

$$\Phi(t + \Delta t)  \approx \Phi(t) -g(\Phi)\Delta t + \eta \sum_{i=1}^N (g(\Phi) - \hat{g}_i)$$

\vfill
The random variables in the last term have zero mean.

\vfill
By the central limit theorem a sum (not the average) of $N$ random vectors will approximate a Gaussian distribution where the variance
grows like $N$.

\slide{Modeling Noise}

The mean of $\hat{g}$ is the true gradient $g(\Phi)$.  

\begin{eqnarray*}
\Delta t & = & \eta N \\
\Phi(t + \Delta t) &  = & \Phi(t) - \sum_{j=1}^N \eta\hat{g}_i \\
\\
 &  = & \Phi(t) -g(\Phi)\Delta t + \eta \sum_{j=1}^N (g(\Phi) - \hat{g}_i)
\end{eqnarray*}

\slide{Modeling Noise}

\begin{eqnarray*}
\Delta t & = &  \Phi(t) -g(\Phi)\Delta t + \eta \sum_{j=1}^N (g(\Phi) - \hat{g}_i)
\end{eqnarray*}


\vfill
By the central limit theorem $\sum_{j=1}^N (g(\Phi) - \hat{g}_i)$ is approximately Gaussian.

\slide{In One Dimension}

We first consider the case of one parameter (a one dimensional optimization problem) so that $\hat{g}$ is a scalar.

\vfill
To model the noise as Gaussian we take $\epsilon \sim {\cal N}(0,\sigma^2)$ where $\sigma^2$ is the variance of $\hat{g}$.

\begin{eqnarray*}
\Phi(t + \Delta t) & \approx & \Phi(t) -g(\Phi)\Delta t + \eta \epsilon \sqrt{N} \\
\end{eqnarray*}

\slide{In One Dimension}

\begin{eqnarray*}
\Phi(t + \Delta t) & \approx & \Phi(t) -g(\Phi)\Delta t +  \eta \epsilon \sqrt{N} \\
\\
& = & \Phi(t) -g(\Phi)\Delta t +  \eta \epsilon \sqrt{\frac{\Delta t}{\eta}} \\
\\
& = & \Phi(t) -g(\Phi)\Delta t +  \sqrt{\eta} \epsilon \sqrt{\Delta t} \\
\\
& = & \Phi(t) -g(\Phi)\Delta t +  \epsilon' \sqrt{\Delta t} \\
\end{eqnarray*}

\vfill
With $\epsilon' \sim {\cal N}(0,\eta \sigma^2)$.


\slide{The Stochastic Differential Equation (SDE)}


\begin{eqnarray*}
\Phi(t + \Delta t) & \approx & \Phi(t) -g(\Phi)\Delta t +  \epsilon \sqrt{\Delta t} \\
\epsilon & \sim & {\cal N}(0,\eta\sigma^2)
\end{eqnarray*}

\vfill
We can take this last equation to hold in the limit of arbitrarily small $\Delta t$ in which case we get a continuous time stochastic process.  This process can be written as

{\color{red} $$d\Phi =  -g(\Phi)dt + \epsilon \sqrt{dt}\;\;\;\;\;\; \epsilon \sim {\cal N}(0,\eta\sigma^2)$$}

\slide{Higher Dimension}

In the higher dimensional case we get

{\color{red} $$d\Phi =  -g(\Phi)dt + \epsilon \sqrt{dt}\;\;\;\;\;\; \epsilon \sim {\cal N}(0,\eta\Sigma)$$}

\vfill
Where $\Sigma$ is the covariance matrix of $\hat{g}$.

\vfill
In one dimension $\Sigma$ is $\sigma^2$.

\vfill
For $g(\Phi) = 0$ and $\Sigma = I$ we get Brownian motion.

\vfill
But in general we do not have $\Sigma = I$.


\slide{END}
\end{document}
