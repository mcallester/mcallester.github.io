\input ../../SlidePreamble
\input ../../preamble

\begin{document}

{\Huge

  \centerline{\bf TTIC 31230, Fundamentals of Deep Learning}
  \bigskip
  \centerline{David McAllester, Autumn 2020}

\vfill
  \centerline{\bf AGI: Bootstrapping}
  \vfill
  \vfill

\slide{Bootstrapping}

Programs were first written in binary machine code.

\vfill
An assembler was written in binary.

\vfill
A Fortran interpreter was written in assembler.

\vfill
A Fortran compiler was written in Fortran.

\vfill
$\vdots$

\vfill
Will we eventually just describe programs in informal English.
\slide{Bootstrapping}

\vfill
\begin{quotation}
Let an ultraintelligent machine be defined as a machine that can far surpass all the intellectual activities of any person however clever. Since the design of machines is one of these intellectual activities, an ultraintelligent machine could design even better machines; there would then unquestionably be an ‘intelligence explosion,’ and the intelligence of humanity would be left far behind. Thus the first ultraintelligent machine is the last invention that humanity need ever make, provided that the machine is docile enough to tell us how to keep it under control.
\end{quotation}

\vfill
\rightline{I.J. Good, 1969}


\slide{Representing Functions by Programs}

High level scripting languages such as Python seem to be the most productive programming languages for human programmers.

\vfill
Does Python represent a particularly effective universal learning bias?

\vfill
Productivity in programming seems to be greatly enhanced by functional expressions (functional programming) and object-oriented programming
(objects, classes an inheretitance).

\vfill
This seems crucial if we want to somehow achieve I. J. Good's intelligence explosion.

\slide{The Turing Tarpit}

But in theory the choice of programming language does not matter.

\vfill
For any two Turing universal languages, say Python and Assmbler, there exists an interpreter $I$ for Python written in Assembler
where write $I(h)$ for the assember interpreter $I$ applied to Python program $h$.  We then get

$$|I(h)|_\mathrm{Assembler} = |h|_\mathrm{Python} + |I|_\mathrm{Assembler}$$

\vfill
Bootstrapping layers of language can make the interpreter small.

\slide{The Turing Tarpit}

$$|I(h)|_\mathrm{Assembler} = |h|_\mathrm{Python} + |I|_\mathrm{Assembler}$$

\vfill
Up to the additive constant of the interpreter, assembler gives just as good a learning bias as Python.

\vfill
Yet we know that the choice of language does matter --- Python is clearly better than assembler.

\vfill
Presumably this is because the search over Python is easier than the search over assembler.

\slide{Searching over Programs: Levin Search, 1973}

\includegraphics[height=2.0 in]{\images/Levin}
Leonid Levin observed that one can construct a universal solver.  The solver takes as input a solution tester
and returns as output a solution whenever a solution exists.

  \vfill
Levin's solver is universal in the sense that it is not more than a constant factor slower than any other solver for the given tester.

\vfill
It follows that for any problem in NP, the universal solver provides a P-time algorithm whenever such an algorithm exists.


\slide{Levin's Universal Solver}
  
\vfill
We time share all programs giving time slice $2^{-|h|}$ to program $h$ where $|h|$
is the length in bits of $h$.

\vfill
The run time of the universal solver is at most
$$O(2^{-|h|}(h(n) + T(n)))$$
where $h(n)$ is the time required to run program $h$ on a problem of size $n$ and $T(n)$ is the time required to check the solution.

\vfill
Here $2^{-|h|}$ is independent of $n$ and is technically a constant.

\slidetwo{Bootstrapping the Search}
{The Baldwin Effect: Learning Facilitates Adaptation}

\includegraphics[width = 1.2in]{\images/hinton}

In a 1987 paper entitled ``How Learning Can Guide Evolution'', Goeffrey Hinton and Steve Nowlan brought attention to a paper by Baldwin (1896).

\vfill
The basic idea is that Learning facilitates evolution --- if arm control is learned then arm structure is easier to change.

\slide{Meta-Baldwin}

The Baldwin effect should apply to brain modules as well, such as vision or the motor cortex.

\vfill
Learning should facilitate the evolution of brain modules.

\vfill
Meta-Baldwin: Learning should then facilitate the evolution of learning.

\slide{Bootstrapping the Search}
\vfill
\includegraphics[width= 1in]{\images/schmidhuber}
Schmidhuber proposes a learning algorithm for learning an optimal universal solver.
The Optimally Ordered Problem Solver (OOPS), Schmidhuber, 2002.

\vfill
The ideas are similar to Meta-Baldwin --- we learn to search over programs, or learn the learner.

\vfill
\vfill
But as in Levin search, Schmidhuber's optimal problem solver still does an exponential search over programs.  This
has not produces useful results to date.

\slide{END}

}
\end{document}
