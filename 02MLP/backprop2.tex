\input ../SlidePreamble
\input ../preamble

\begin{document}

{\Huge
  
  \centerline{\bf TTIC 31230, Fundamentals of Deep Learning}
  \bigskip
  \centerline{David McAllester, Winter 2022}
  \vfill
  \vfill
  \centerline{\bf Backpropagation with Arrays and Tensors}
  \vfill
  \vfill


\slide{Program Values as Objects}

\vfill
In a framework the program (or deep model) variables are objects in the sense of object oriented programming or Python.

\vfill
Each object $x$ stores its input objects in its instance variables and has an instance variable $x.\mathrm{value}$ storing its value.

\vfill
The instance variable $x.\mathrm{value}$ is filled by sending $x$ a forward message after its inputs have computed their values.

\vfill
Each object $x$ has an instance variable $x.\grad$ storing $\partial {\cal L}/\partial x$.

\vfill
$x.\grad$ is filled by the backward methods of objects $y$ that
use $x$ as an input.  The backward method for $y$ is called after $y.\grad$ has been filled and adds into $x.\grad$ for each input $x$.

\slide{Scalar Products}

Consider a scalar product $z = xy$.

\vfill
The forward method for $z$ computes.

$$z.\mathrm{value} = x.\mathrm{value}*y.\mathrm{value}$$

\vfill
The backward method for $z$ computes

\begin{eqnarray*}
x.\grad & \pluseq & z.\grad * y.\mathrm{value} \\
\\
y.\grad & \pluseq & z.\grad * x.\mathrm{value}
\end{eqnarray*}

\slide{Handling Arrays}

Consider an inner product between vectors
$$z = x^\top y$$

\vfill
In this case case $z.\mathrm{forward}$ does
$$z.\mathrm{value} = 0$$
$$\mathrm{for}\;i\;\;z.\mathrm{value} \;\pluseq\;x.\mathrm{value}[i]*y.\mathrm{value}[i]$$

\vfill
The backward method for $z$  treats each $\pluseq$ instruction seperately and does.
$$\mathrm{for}\;i\;\;x.\grad[i] \;\pluseq\;y.\mathrm{value}[i]*z.\grad$$
$$\mathrm{for}\;i\;\;y.\grad[i] \;\pluseq\;x.\mathrm{value}[i]*z.\grad$$

\slide{Handling Arrays}
Now consider multiplying a vector $x$ by a matrix $W$.
$$y = Wx$$

\vfill
In this case case $y.\mathrm{forward}$ does
$$\mathrm{for}\;j\;\;y.\mathrm{value}[j] = 0$$
$$\mathrm{for}\;i,j\;\;y.\mathrm{value}[j] \;\pluseq\;W.\mathrm{value}[j,i]*x.\mathrm{value}[i]$$

\vfill
The backward procedure $y.\mathrm{backward}$ treats each individual $\pluseq$ as a scalar product and does
$$\mathrm{for}\;i,j\;\;x.\grad[i] \;\pluseq\;W.\mathrm{value}[j,i]*y.\grad[j]$$
$$\mathrm{for}\;i\;\;W.\grad[j,i] \;\pluseq\;x.\mathrm{value}[i]*y.\grad[j]$$

\slide{A Linear Threshold Layer}

\begin{eqnarray*}
   s & = & \sigma\left(Wh - B \right)
\end{eqnarray*}

$$\begin{array}{lrcl}
\mathrm{for}\;j &  \tilde{s}[j] & = & 0 \\
\\
\mathrm{for}\;j,i &  \tilde{s}[j] & \pluseq &  W[j,i]h[i] \\
\\
\mathrm{for}\; j & s[j] & = & \sigma(\tilde{s}[j] - B[j])
\end{array}$$

\vfill
Backpropagation is also done with loops treating each individual assignment and $\pluseq$ instruction.

\slide{General Tensor Operations}

In practice all deep learning source code can be written using scalar assignments and loops over scalar assignments.
For example:

$$\begin{array}{rrcl}
\mathrm{for}\; h,i,j,k & Y[h,i,j] & \;\pluseq\; & A[h,i,k]\;B[h,j,k] \\
\end{array}$$

\vfill
has backpropagation loops

$$\begin{array}{rrcl}
\mathrm{for}\;h,i,j,k & A.\grad[h,i,k] & \pluseq & Y.\grad[h,i,j] \;B.\mathrm{value}[h,j,k] \\
\mathrm{for}\;h,i,j,k & B.\grad[h,j,k] & \pluseq & Y.\grad[h,i,j]\;A.\mathrm{value}[h,i,k]
\end{array}$$

\slide{END}
}

\end{document}
