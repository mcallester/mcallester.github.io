\input ../SlidePreamble
\input ../preamble

\begin{document}

{\Huge
  
  \centerline{\bf TTIC 31230, Fundamentals of Deep Learning}
  \bigskip
  \centerline{David McAllester, Winter 2022}
  \vfill
  \vfill
  \centerline{\bf Backpropagation with Arrays and Tensors}
  \vfill
  \vfill

\slide{Program Values as Objects}

Consider a scalar product ($x$, $y$ and $z$ are each just real numbers).
$$z = xy$$

\vfill
In a framework the values of the variables $x$, $y$ $z$ are objects in the sence of object oriented programming or Python.

\vfill
In computing $z.\mathrm{value}$ we assume that $x.\mathrm{value}$ and $y.\mathrm{value}$ are known. In the base case these are are just inputs or parameters.

\slide{Program Values as Objects}
$$z = xy$$

\vfill
The forward pass calls the procedure $z.\mathrm{forward}$ on each computed value $z$.

\vfill
The object $z$ holds its own inputs in its attributes.

\vfill
Since $z$ computed from a product the procedure $z.\mathrm{forward}$ assigns

\vfill
$$z.\mathrm{value} = x.\mathrm{value}*y.\mathrm{value}$$

\slide{Backprop with Objects}
$$z = xy$$

\vfill
Each object $x$ has an attribure $x.\grad$ which holds the gradient of the loss with respect to $x$.

We want
$$z.\grad = \frac{\partial {\cal L}}{\partial z}$$

\vfill
Backpropagation calls $z.\mathrm{backward}$ on each computed value $z$ in the reverse order.

\vfill
For $z = xy$ we have that $z.\mathrm{backward}$ does
\begin{eqnarray*}
x.\grad & \pluseq & y.\mathrm{value}*z.\grad \\
y.\grad & \pluseq & x.\mathrm{value}* z.\grad
\end{eqnarray*}

\slide{Handling Arrays}
Consider an inner product between vectors
$$z = x^\top y$$

\vfill
In this case case $z.\mathrm{forward}$ does
$$z.\mathrm{value} = 0$$
$$\mathrm{for}\;i\;\;z.\mathrm{value} \;\pluseq\;x.\mathrm{value}[i]*y.\mathrm{value}[i]$$

\vfill
The backward procedure $z.\mathrm{backward}$ treats each $\pluseq$ instruction seperately and does.
$$\mathrm{for}\;i\;\;x.\grad[i] \;\pluseq\;y.\mathrm{value}[i]*z.\grad$$
$$\mathrm{for}\;i\;\;y.\grad[i] \;\pluseq\;x.\mathrm{value}[i]*z.\grad$$

\slide{Handling Arrays}
Now consider multiplying a vector $x$ by a matrix $W$.
$$y = Wx$$

\vfill
In this case case $y.\mathrm{forward}$ does
$$\mathrm{for}\;j\;\;y.\mathrm{value}[j] = 0$$
$$\mathrm{for}\;i,j\;\;y.\mathrm{value}[j] \;\pluseq\;W.\mathrm{value}[j,i]*x.\mathrm{value}[i]$$

\vfill
The backward procedure $y.\mathrm{backward}$ treats each individual $\pluseq$ as a scalar product and does
$$\mathrm{for}\;i,j\;\;x.\grad[i] \;\pluseq\;W.\mathrm{value}[j,i]*y.\grad[j]$$
$$\mathrm{for}\;i\;\;W.\grad[j,i] \;\pluseq\;x.\mathrm{value}[i]*z.\grad[j]$$

\slide{A Linear Threshold Layer}

\begin{eqnarray*}
   s & = & \sigma\left(W^1h - B^1 \right)
\end{eqnarray*}

\vfill
$$\begin{array}{lrcl}
\mathrm{for}\;j &  \tilde{s}[j] & = & 0 \\
\\
\mathrm{for}\;j,i &  \tilde{s}[j] & \pluseq &  W^1[j,i]h[i] \\
\\
\mathrm{for}\; j & s[j] & = & \sigma(\tilde{s}[j] - B^1[j])
\end{array}$$

\vfill
backpropagation is also done with loops treating each individual assigningments and $\pluseq$ instructions.

\slide{General Tensor Operations}

In practice all deep learning source code can be written unsing scalar assignments and loops over scalar assignments.
For example:

$$\begin{array}{rrcl}
\mathrm{for}\; h,i,j,k & \tilde{Y}[h,i,j] & \;\pluseq\; & A[h,i,k]\;B[h,j,k] \\
\mathrm{for}\; h,i,j & Y[h,i,j] & = & \sigma(\tilde{Y}[h,i,j])
\end{array}$$

\vfill
has backpropagation loops

$$\begin{array}{rrcl}
\mathrm{for}\;h,i,j & \tilde{Y}.\grad[h,i,j] & \pluseq & Y.\grad[h,i,j]\;\;\sigma'(\tilde{Y}.\grad[h,i,j]) \\
\mathrm{for}\;h,i,j,k & A.\grad[h,i,k] & \pluseq & \tilde{Y}.\grad[h,i,j] \;B[h,j,k] \\
\mathrm{for}\;h,i,j,k & B.\grad[h,j,k] & \pluseq & \tilde{Y}.\grad[h,i,j]\;A[h,i,k]
\end{array}$$

\slide{END}
}

\end{document}
