\input ../../SlidePreamble
\input ../../preamble

\begin{document}

{\Huge

  \centerline{\bf TTIC 31230, Fundamentals of Deep Learning}
  \bigskip
  \centerline{David McAllester, Autumn 2020}


  \vfill
  \centerline{\bf AlphaZero Background Algorithms}
  \vfill
  \vfill          

\slide{AlphaGo Fan (October 2015)}

AlphaGo Defeats Fan Hui, European Go Champion.

\vfill
\centerline{\includegraphics[width=4in]{\images/alphago}}

\slide{AlphaGo Lee (March 2016)}

\vfill
\centerline{\includegraphics[width=8in]{\images/alphagolee}}

\slide{AlphaGo Zero vs. Alphago Lee (April 2017)}

{\bf AlphaGo Lee:}

\begin{itemize}
\item Trained on both human games and self play.
  
\item Trained for Months.

\item Run on many machines with 48 TPUs for Lee Sedol match.
\end{itemize}

{\bf AlphaGo Zero:}
\begin{itemize}
\item Trained on self play only.
  
\item Trained for 3 days.

\item Run on one machine with 4 TPUs.

\item Defeated AlphaGo Lee under match conditions 100 to 0.
\end{itemize}

\slide{AlphaZero Defeats Stockfish in Chess (December 2017)}

AlphaGo Zero was a fundamental algorithmic advance for general RL.

\vfill
The general RL algorithm of AlphaZero is essentially the same as that of AlphaGo Zero.


\slide{}

\centerline{\bf Some Background}

\vfill

\slide{Early Computer Chess}

First computer chess algorithm (min-max tree search) --- Claude Shannon, 1949

\vfill
$\alpha$-$\beta$ pruning --- various originators (including John McCarthy) circa 1960.

\vfill
$\alpha$-$\beta$ pruning was the backbone of all computer chess before AlphaGo 2015.

\vfill
The game of Go is clearly not approachable by these methods.

\slidetwo{Monte-Carlo Tree Search (MCTS)}
{Brugmann (1993)}

First major advance in computer Go.

\vfill
To estimate the value of a position (who is ahead and by how much)
run a cheap stochastic policy to generate a sequence of moves (a rollout) and see who wins.

\vfill
Select the move with the best rollout value.

\slidetwo{(One Armed) Bandit Problems}
{Robbins (1952)}

Bandit problems were studied in an independent line of research.

\vfill
Consider a set of choices.  The standard example is a choice between different slot machines with different but unknown expected payout.
The ``phrase one-armed bandit'' refers to a slot machine.

\vfill
But another example of choices might be the moves in a game.

\slidetwo{Bandit Problems}

\vfill
Consider a set of choices where each choice gets a stochastic reward.

\vfill
We can select a choice and get a reward as often as we like.

\vfill
We would like to determine which choice is best using a limited number of trials.

\slidetwo{The Upper Confidence Bound (UCB) Algorithm}
{Lai and Robbins (1985)}

For each action choice (bandit) $a$, construct a confidence interval for its average reward
based on $n$ trials for that acrtion.

\vfill
$$\mu(a) \in \hat{\mu}(a) \pm 2\sigma(a)/\sqrt{n(a)}$$

\vfill
Always select

$$\argmax_a \;\; \hat{\mu}(a) + 2\sigma(a)/\sqrt{n(a)}$$

\slidetwo{The Upper Confidence Tree (UCT) Algorithm}
{Kocsis and Szepesvari (2006), Gelly and Silver (2007)}

The UCT algorithm grows a tree by running ``simulations''.

\vfill
Each simulation descends into the tree to a leaf node, expands that leaf, and returns a value.

\vfill
In the UCT algorithm each move choice at each position is treated as a bandit problem.

\vfill
We select the child (bandit) with the lowest upper bound as computed from simulations selecting that child.


\slidetwo{Bootstrapping from Game Tree Search}
{Vaness, Silver, Blair and Uther, NeurIPS 2009}

In bootstrapped tree search we do a tree search to compute a min-max value $V_{\mathrm{mm}}(s)$
using tree search with a static evaluator $V_\Phi(s)$.  We then try to fit the static value to the min-max value.

\vfill
$$\Delta \Phi = - \eta \nabla_\Phi \left(V_\Phi(s) - V_{\mathrm{mm}}(s)\right)^2$$

\vfill
This is similar to minimizing a Bellman error between $V_\Phi(s)$ and a rollout estimate of the value of $s$ but where the rollout
estimate is replaced by a min-max tree search estimate.

\slide{END}

}
\end{document}
