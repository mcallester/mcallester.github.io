\input ../../SlidePreamble
\input ../../preamble


\begin{document}

{\Huge

  \centerline{\bf TTIC 31230, Fundamentals of Deep Learning}
  \bigskip
  \centerline{David McAllester, Autumn 2020}
  \vfill
  \vfill
  \centerline{\bf Variational Auto-Encoders (VAEs)}
  \vfill
  \centerline{\bf Posterior Collapse}
  \vfill
  \centerline{\bf $\beta$-VAEs}
  \vfill
  \centerline{\bf Encoder Autonomy}
  \vfill

  \vfill
  \vfill

\slide{VAE}
{\huge
\begin{eqnarray*}
P_\Psi(y,z) & = & \pop(y)P_\Psi(z|y)\;\;\;\;\mbox{The sampling distribution on $y$, $z$} \\
\\
z & = & z_\Psi(y,\epsilon) \;\;\;\mbox{$\epsilon$ parameter independent noise} \\
\\
\\
\mbox{VAE:}\;\;\;\Psi^*,\Phi^*  & = & \argmin_{\Psi,\Phi}\; \parens{E_{y,z} \;\;{\ln \frac{P_\Psi(z|y)}{\hat{P}_\Phi(z)}}}  + \parens{E_{y,z}\;-\ln {\hat{P}_\Phi(y|z)}}
\end{eqnarray*}}

\slide{VAE}
{\huge
\begin{eqnarray*}
P_\Psi(y,z) & = & \pop(y)P_\Psi(z|y) \\
\\
\mbox{VAE:}\;\;\;\Psi^*,\Phi^*  & = & \argmin_{\Psi,\Phi}\; \parens{E_{y,z} \;\;{\ln \frac{P_\Psi(z|y)}{\hat{P}_\Phi(z)}}}  + \parens{E_{y,z}\;-\ln {\hat{P}_\Phi(y|z)}} \\
\\
& = & \argmin_{\Psi,\Phi}\; {\color{red} \hat{I}_{\Psi,\Phi}(y,z)} \;+\; {\color{red} \hat{H}_{\Psi,\Phi}(y|z)}
\end{eqnarray*}

{\color{red}
$$I_\Psi(y,z) \leq \hat{I}_{\Psi,\Phi}(y,z) \;\;\;\;\;\; H_\Psi(y|z) \leq \hat{H}_{\Psi,\Phi}(y|z)$$

$$H(y) = I_\Psi(y,z) + H_\Psi(y|z)$$}
}

Inequalities hold with equality under universal expressiveness.

\slide{Posterior (Encoder) Collapse}

$$\Psi^*,\Phi^* = \argmin_{\Psi,\Phi}\;\hat{I}_{\Psi,\Phi}(y,z) + \hat{H}_{\Psi,\Phi}(y|z)$$

\vfill
Consider a trivial encoder with $P_\Psi(z^*|y) =1$ and $\hat{P}_\Phi(z^*) = 1$ for a fixed value $z^*$ independent of $y$ yielding
{\color{red} $\hat{I}_{\Psi,\Phi}(y,z) = 0$}.

\vfill
Under universal expressiveness we have $\hat{P}_{\Phi^*}(y|z) = \pop(y)$ yielding {\color{red} $\hat{H}_{\Psi,\Phi}(y|z) = H(y)$}.

\vfill
Therefore, under universal expressiveness {\bf there exists an optimal solution where
the posterior (encoder) $P_\Psi(z|y)$ collapses}.

\slide{The $\beta$-VAE}

{\huge
\begin{eqnarray*}
P_\Psi(y,z) & = & \pop(y)P_\Psi(z|y)\;\;\;\;\mbox{The sampling distribution on $y$, $z$} \\
\\
\mbox{VAE:}\;\;\;\Psi^*,\Phi^*  & = & \argmin_{\Psi,\Phi}\; \hat{I}_{\Psi,\Phi}(y,z) \;+\; \hat{H}_{\Psi,\Phi}(y|z) \\
\\
\mbox{$\beta$-VAE:}\;\;\; \Psi^*,\Phi^* & = & \argmin_{\Psi,\Phi}\; {\color{red} \beta}\; \hat{I}_{\Psi,\Phi}(y,z) \;+\; \hat{H}_{\Psi,\Phi}(y|z)
\end{eqnarray*}
}

The $\beta$-VAE introduces a rate-distortion tradeoff parameter into the VAE.  $\beta < 1$ may avoid posterior collapse.  $\beta > 1$ may improve interpretability.

\slide{The Universality Theorem for $\beta$-VAEs}

{\huge
\begin{eqnarray*}
\mbox{$\beta$-VAE:}\;\;\;\Psi^*,\Phi^*  & = & \argmin_{\Psi,\Phi}\; \beta\parens{E_{y,z} \;\;{\ln \frac{P_\Psi(z|y)}{\hat{P}_\Phi(z)}}}  + \parens{E_{y,z}\;-\ln {\hat{P}_\Phi(y|z)}}
\end{eqnarray*}


{\color{red}
$$I_\Psi(y,z) \leq \hat{I}_{\Psi,\Phi}(y,z) \;\;\;\;\;\; H_\Psi(y|z) \leq \hat{H}_{\Psi,\Phi}(y|z)$$

$$H(y) = I_\Psi(y,z) + H_\Psi(y|z)$$}
}

Assuming universaltiy, optimizing $\hat{P}_\Phi(z)$ while holding $P_\Psi(z|y)$ and $\hat{P}_\Phi(y|z)$ fixed drives the first inequality to equality.

\vfill
Optimizing $\hat{P}_\Phi(y|z)$ while holding $P_\Psi(z|y)$ and $\hat{P}_\Phi(z)$ fixed drives the second inequality to equality.

\slide{Encoder Autonomy}

Assuming universality, optimizing $\Phi$ for any fixed value of $\Psi$ yields the population distribution on $y$.

\vfill
This implies that we can add any loss on $\Psi$ alone and the universality theorem still holds.

{\huge
\begin{eqnarray*}
\mbox{VAE:}\;\;\;\Psi^*,\Phi^*  & = & \argmin_{\Psi,\Phi}\;\; {\color{red} \beta}\; \hat{I}_{\Psi,\Phi}(y,z) \;+\; \hat{H}_{\Psi,\Phi}(y|z) + {\cal L}(\Psi)
\end{eqnarray*}


{\color{red}
$$I_\Psi(y,z) \leq \hat{I}_{\Psi,\Phi}(y,z) \;\;\;\;\;\; H_\Psi(y|z) \leq \hat{H}_{\Psi,\Phi}(y|z)$$

$$H(y) = I_\Psi(y,z) + H_\Psi(y|z)$$}
}

\slide{END}

\end{document}
