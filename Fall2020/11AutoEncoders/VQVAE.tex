\input /Users/davidmcallester/ICloude/tex/SlidePreamble
\input /Users/davidmcallester/ICloude/tex/preamble


\begin{document}

{\Huge

  \centerline{\bf TTIC 31230, Fundamentals of Deep Learning}
  \bigskip
  \centerline{David McAllester, Autumn 2021}
  \vfill
  \vfil
  \centerline{Vector Quantized Variational Autoencoders (VQ-VAEs)}
  \vfill
  \vfill

\slide{Gaussian VAEs for Faces 2014}

We can sample faces from the VAE by sampling noise $z$ from $p_\Phi(z)$ and then sampling an image $y$ from $p_\Phi(y|z)$.

\vfill
\centerline{\includegraphics[width = 3in]{\images/VariationalFaces}}
\centerline{[Alec Radford]}

\slide{VQ-VAEs 2019}

\centerline{\includegraphics[width = 8in]{\images/VQ-VAE22}}

\vfill
VQ-VAE-2, Razavi et al. June, 2019

\slide{VQ-VAEs 2019}

\centerline{\includegraphics[width = 10in]{\images/VQ-VAE21}}

\vfill
VQ-VAE-2, Razavi et al. June, 2019



\slide{VQ-VAE-2 Model}


\centerline{\includegraphics[height =2in]{\images/VQSampler} \parbox[b]{1.0in}{\Large $z_1$ \\ \\ $z_2  $ \\ \\ $y$ \\ \\ ~}}

\vfill
The probability of an image $y$ is defined by the generator.

\vfill
The generator is top-down and is similar to that of a progressive GAN.


\slide{VQ-VAE-2 Model}


\centerline{\includegraphics[height =2in]{\images/VQSampler} \parbox[b]{1.0in}{\Large $z_1$ \\ \\ $z_2  $ \\ \\ $y$ \\ \\ ~}}

{\huge
$$P_{\Phi,\Theta}(y) = \sum_{z_1\ldots,z_n}\; P_\Phi(z_1,\ldots,z_N)P_\Theta(y|z_1,\ldots,z_N).$$

\vfill
Each $z_i$ is a ``symbolic image'': an assignment of an embedded symbol to each pixel.

\vfill
$P_{\Phi_{i+1}}(z_{i+1}\;|\; z_1,\ldots z_i)$ is auto-regressive, as in a language model.

}

\slide{VQ-VAE-2 Model}

We first describe the case of just one layer.

\vfill
We then describe a very general approach to multi-layer VAEs.

\slide{The Single-Layer VQ-VAE}

Let $s$ denote the image (we are using $y$ for an image coordinate).

\vfill
We have an encoder network, such as a CNN, which produces a layer with a vector at each pixel position.

\begin{eqnarray*}
L[X,Y,I] & = & \mathrm{Enc}_\Phi(s) \\
\end{eqnarray*}

Intuitively we cluster the vectors $L(x,y,I)$ using K-means clustering to produce cluster centers where $C[k,I]$
is the cluster center vector of cluster $k$.

\slide{VQ-VAE Model}

We then replace each vector $L[x,y,I]$ by the nearest cluster center to give $\hat{L}[X,Y,I]$.

{\huge
\begin{eqnarray*}
z[x,y] & = & \argmin_k \;||L[x,y,I] - C[k,I]|| \\
\\
\hat{L}[x,y,I] & = & C[z[x,y],I] \\
\end{eqnarray*}
}

The ``symbolic image'' $z[X,Y]$ is the latent variable.

\slide{VQ-VAE Model}

Finally we decode $\hat{L}[X,Y,I]$ to get $\hat{s}_\Theta(z)$.

\slide{Two-Phase Optimization}

{\huge
{\bf Phase 1:} Fix the prior $P_\Phi(z)$ at a simple (perhaps uniform) distribution and optimize the encoder $P_\Psi(z|s)$ and the decoder $P_\Theta(s|z)$.

\begin{eqnarray*}
\mbox{for}\;\;\;\Theta^*,\Psi^* &  = & \argmin_{\Theta,\Psi}\;E_{s\sim \pop,z\sim P_\Psi(z|s)}  \;\ln \frac{P_\Psi(z|s)}{P_\Phi(z)}  - \ln P_\Theta(s|z)
\\
\\
\mbox{we get}\;\;\Theta^* & = & \argmin_\Theta\;E_{s \sim \pop,z \sim P_{\Psi^*}(z|s)}\left[-\ln P_\Theta(s|z)\right]
\end{eqnarray*}

\vfill
{\bf Phase 2:} Train the prior $P_\Phi(z)$ holding the encoder and decoder fixed.

\begin{eqnarray*}
\Phi^* &  = & \argmin_{\Phi}\;E_{s\sim \pop,z\sim P_{\Psi^*}(z|s)}\left[-\ln P_\Phi(z)\right]
\end{eqnarray*}

\vfill
{\bf Joint Training of the prior $P_\Phi(z)$ with the encoder $P_\Psi(z|s)$ and the decoder $P_\Theta(s|z)$ is not required.}

}


\slide{One Layer VQ-VAE Training Phase 1}

We fix the prior on $z$ to be uniform.  $P_\Phi(z)$ can then be ignored in phase 1.

\vfill
The training objective in Phase 1 is then taken to be

\vfill
{\huge
\begin{eqnarray*}
\Psi^*,\Theta^* & = & \argmin_{\Psi,\Theta} \;E_{s\sim \pop,z\sim P_\Psi(z|s)}\left[ \frac{\beta}{2}\;||L[X,Y,I] - \hat{L}[X,Y,I]||^2 + (s-\hat{s}_\Theta(z))^2\right]
\end{eqnarray*}
}

\slide{Handling Discrete Latents}


{\huge
\begin{eqnarray*}
z[x,y] & = & \argmin_k \;||L[x,y,I] - C[k,I]|| \\
\\
\hat{L}[x,y,I] & = & C[z[x,y],I]
\end{eqnarray*}
}

Since $z[x,y]$ is discrete we have $z[x,y].\grad = 0$.
They use ``straight-through'' gradients and ``k-means'' gradients.

{\huge
\begin{eqnarray*}
L[x,y,I].\grad & = & \hat{L}[x,y,I].\grad + \beta(L[x,y,I] - C[z[x,y],I]) \\
\\
C[k,I].\grad & = & \sum_{z[x,y]=k} \gamma(C[k,I] - L[x,y,I])
\end{eqnarray*}
}

\slide{One Layer VQ-VAE Training Phase 2}

Finally we hold the encoder fixed and train the prior $P_\Phi(z)$ to be an auto-regressive model of the symbolic image $z[X,Y]$.

\slide{Progressive VAEs}

We now consider a general VAE with a sequence of latent variables $z_1,\dots,z_n$ where we want to model an observable variable $y$.

\vfill
In the image VQ-VAE each $z_i$ is a symbolic image.

\vfill
A progressive VAE has the following structure on the prior and decoder.
$$P_\Phi(z_1,\ldots,z_N) = P_\Phi(z_1)\prod_{i=2}^N\;P_\Phi(z_i\;|\;z_{i-1})$$
$$P_\Theta(y|z_1,\ldots,z_N) = P_\Theta(y|z_N)$$

\vfill
This copies the structure of a progressive GAN generator.

\slide{Progressive VAE Training Phase 1}

{\huge
We train the progressive VAE in two phases where the first phase trains a series of encoders and decoders.

$$P_\Psi(z_N|y),\;P_\Psi(z_{N-1}|z_N), \;\ldots, P_\Psi(z_1|z_2)$$
$$P_\Theta(y|z_n),\;P_\Theta(z_N|z_{N-1}),\;\ldots,P_\Theta(z_2|z_1)$$

\vfill
The pair of $P_\Psi(z_N|y)$ and $P_\Theta(y|z_N)$ is trained under a fixed simple prior on $z_N$.

Each pair of $P_\Psi(z_{i-1}|z_i)$ and $P_\Theta(z_i|z_{i-1})$ is trained under a fixed simple prior on $z_{i-1}$.

This defines an encoder $P_\Psi(z_1,\ldots,z_N|y)$ and an appropriately strong decoder $P_\Theta(y|z_N)$.
}

\slide{Progressive VAE Training Phase 2}

For each $y$ we how have a sequence of latent variables $z_1,\ldots,z_N$.

\vfill
This allows training of a strong prior $P_\Phi(z_1)$ and $P(z_{i+1}|z_i)$.

\slide{Quantitative Evaluation}

The VQ-VAE2 paper reports a classification accuracy score (CAS) for class-conditional image generation.

\vfill
We generate image-class pairs from the generative model trained on the ImageNet training data.

\vfill
We then train an image classifier from the generated pairs and measure its accuracy on the ImageNet test set.

\vfill
\centerline{\includegraphics[width=7in]{\images/VQ-VAE23}}

\slide{Image Compression}

\vfill
\centerline{\includegraphics[width = 10in]{\images/VQgirl}}

\slide{Rate-Distortion Evaluation.}

Rate-distortion metrics for image compression to discrete representations support unambiguous rate-distortion evaluation.

\vfill
Rate-distortion metrics also allow one to explore the rate-distortion trade-off.

\vfill
\centerline{\includegraphics[width = 10in]{\images/VQVAE2Scores}}

\slide{DALL$\cdot$E: A Text-Conditional Image dVAE}
\vfill
DALL$\cdot$E is a text-conditional VQ-VAE model of images.

\vfill
The Vector quantization is done independent of the text.  However, the model of the probability distribution of the symbolic image $z[x,y]$ is conditioned on text.

\vfill
\centerline{\huge Ramesh et al. 2021}

\slide{DALL$\cdot$E}

\centerline{\includegraphics[height= 5.5in]{\images/DALLE1}}


\slideplain{END}

\end{document}


\slide{VQ-VAE Encoder-Decoder}

\centerline{\includegraphics[width =5in]{\images/VQCodes}}

\vfill
In the one-layer case the latent variable (compressed image) is a ``symbolic image'' $z[X,Y]$ where $z[x,y]$ is a symbol (a cluster index).

\vfill
Naively $z[X,Y]$ can be represented with $XY \log_2 K$ bits.
