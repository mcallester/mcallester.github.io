\input ../../SlidePreamble
\input ../../preamble

\begin{document}

{\Huge

  \centerline{\bf TTIC 31230, Fundamentals of Deep Learning}
  \bigskip
  \centerline{David McAllester, Winter 2021}
  \vfill
  \vfill
  \centerline{\bf Variational Auto-Encoders (VAEs)}
  \vfill
  \vfill

\slidetwo{Meaningful Latent Variables:}
{Learning Phonemes and Words}

A child exposed to speech sounds learns to distinguish phonemes and then words.

\vfill
The phonemes and words are ``latent variables'' learned from listening to sounds.

\vfill
We will use $y$ for the raw input (sound waves) and $z$ for the latent variables (phonemes).

\slide{Other Examples}

$z$ might be a parse tree, or some other semantic representation, for an observable sentence (word string) $y$.

\vfill
$z$ might be a segmentation of an image $y$.

\vfill
$z$ might be a depth map (or 3D representation) of an image $y$.

\vfill
$z$ might be a class label for an image $y$.

\vfill
Here we are interested in the case where $z$ is {\bf latent} in the sense that we do not have training labels for $z$.

\vfill
{\bf We want reconstructions of $z$ from $y$ to emerge from observations of $y$ alone.}

\slide{Latent Variables}

Here we often think of $z$ as the causal source of $y$.

\vfill
$z$ might be a physical scene causing image $y$.

\vfill
$z$ might be a word sequence causing speech sound $y$.

\slide{Latent Variables}

{\color{red} $$P_{\Phi,\Theta}(y) = \sum_z\;P_\Phi(z)P_\Theta(y|z) = E_{z \sim P_\Phi(z)}\;P_\Theta(y|z)$$}

\vfill
$P_\Phi(z)$ is typically called the prior.

\vfill
$P_{\Phi,\Theta}(z|y)$ is the posterior where $y$ is the ``evidence''.

\slide{Assumptions}


We assume models $P_\Phi(z)$ and $P_\Theta(y|z)$ are both samplable and computable.

\vfill
In other words, we can sample from these distributions and for any given $z$ and $y$ we can compute $P_\Phi(z)$ and $P_\Theta(y|z)$.

\vfill
These assumptions hold for auto-regressive models (language) and for Gaussian densities.

\slide{Modeling $y$}

\vfill
We would like to use cross-entropy from the population to the model probability $P_{\Phi,\Theta}(y)$.

\begin{eqnarray*}
\Phi^* & = & \argmin_\Phi \;E_{y \sim \popd}\;-\ln P_{\Phi,\Theta}(y) \\
\\
P_{\Phi,\Theta}(y) & = & E_{z \sim P_\Phi(z)}\;P_\Theta(y|z)
\end{eqnarray*}

\vfill
But even when $P_\Phi(z)$ and $P_\Theta(y|z)$ are samplable and computable we cannot typically compute $P_{\Phi,\Theta}(y)$ or $P_{\Phi,\Theta}(z|y)$.

\slide{Modeling $y$}

\begin{eqnarray*}
\Phi^* & = & \argmin_\Phi \;E_{y \sim \popd}\;-\ln P_{\Phi,\Theta}(y) \\
\\
P_{\Phi,\Theta}(y) & = & E_{z \sim P_\Phi(z)}\;P_\Theta(y|z)
\end{eqnarray*}

\vfill
VAEs side-step the intractability problem by introducing another model component --- a model $P_\Psi(z|y)$ to approximate the intractible $P_{\Phi,\Theta}(z|y)$.

\slide{The Evidence Lower Bound (The ELBO)}

{\huge
\begin{eqnarray*}
 {\color{red} \ln P_{\Phi,\Theta}(y)} & = & E_{z \sim {\color{red} P_\Psi(z|y)}} \ln \frac{P_{\Phi,\Theta}(y)P_{\Phi,\Theta}(z|y)}{P_{\Phi,\Theta}(z|y)} \\
        \\
        \\
 & = & E_{z \sim {\color{red} P_\Psi(z|y)}} \left(\ln \frac{P_{\Phi,\Theta}(z,y)}{{\color{red} P_\Psi(z|y)}} + \ln \frac{{\color{red} P_\Psi(z|y)}}{P_{\Phi,\Theta}(z|y)}\right) \\
 \\
 \\
  & = & \left(E_{z \sim {\color{red} P_\Psi(z|y)}} \ln \frac{P_{\Phi,\Theta}(z,y)}{{\color{red} P_\Psi(z|y)}}\right) + KL({\color{red} P_\Psi(z|y)},P_{\Phi,\Theta}(z|y)) \\
  \\
  \\
  &  \geq & E_{z \sim {\color{red} P_\Psi(z|y)}} \ln \frac{P_{\Phi,\Theta}(z,y)}{{\color{red} P_\Psi(z|y)}} \;\;\;\;\;\mbox{The ELBO}
\end{eqnarray*}
}

\slide{The ELBO}

{\huge
\begin{eqnarray}
\ln P_{\Phi,\Theta}(y)   &  \geq & E_{z \sim {\color{red} P_\Psi(z|y)}} \ln \frac{P_{\Phi,\Theta}(z,y)}{{\color{red} P_\Psi(z|y)}} \label{ELBO1} \\
  \nonumber  \\
  \nonumber \\
  & = & E_{z \sim {\color{red} P_\Psi(z|y)}} \ln \frac{P_\Phi(z)P_\Theta(y|z)}{{\color{red} P_\Psi(z|y)}} \nonumber \\
  \nonumber \\
  \nonumber \\
  H(y) & \leq & E_{y \sim \pop,\;z \sim P_\Psi(z|y)}\;\ln\frac{P_\Psi(z|y)}{P_\Phi(z)} - \ln P_\Theta(y|z)
  \label{ELBO2}
\end{eqnarray}

\vfill
(\ref{ELBO1}) holds with equality when $P_\Psi(z|y)$ equals $P_{\Phi,\Theta}(z|y)$.

\vfill
(\ref{ELBO2}) holds with equality when we also have $P_{\Phi,\Theta}(y) = \pop$.
}

\slide{The Variational Auto-Encoder (VAE)}

$$\Phi^*,\Theta^*,\Psi^* = \argmin_{\Phi,\Theta,\Psi}\;E_{y \sim \pop,\;z \sim P_\Psi(z|y)}\;\ln\frac{P_\Psi(z|y)}{P_\Phi(z)} \;-\; \ln P_\Theta(y|z)$$

\vfill
Here $P_\Psi(z|y)$ is the encoder and $P_\Theta(y|z)$ is the decoder and the ``rate term'' $ E\left[\ln P_\Psi(z|y)/P_\Phi(z)\right]$ is a KL-divergence.

\slide{The Re-Parameterization Trick}

$$\Phi^*,\Theta^*,\Psi^* = \argmin_{\Phi,\Theta,\Psi}\;E_{y \sim \pop,\;z \sim P_\Psi(z|y)}\;\ln\frac{P_\Psi(z|y)}{P_\Phi(z)} \;-\; \ln P_\Theta(y|z)$$

\vfill
We cannot do gradient descent into $\Psi$ to handle the dependence of the loss on the sampling compute $z \sim P_\Psi(z|y)$.

\vfill
To handle this we sample noise $\epsilon$ from a fixed noise distribution and replace $P_\Psi(z|y)$ with $P_\Psi(z|y,\epsilon)$.

\vfill
The VAE training equation can then be written as

\vfill
\begin{eqnarray*}
\Phi^*,\Theta^*,\Psi^* & =  & \argmin_{\Phi,\Theta,\Psi}\;E_{y\sim\pop,\epsilon\sim\mathrm{noise}}  \;\ln \frac{P_\Psi(z|y,\epsilon)}{P_\Phi(z)}  - \ln P_\Theta(y|z)
\end{eqnarray*}


\slide{EM is Alternating Optimization of the VAE}

Expectation Maximimization (EM) applies in the (highly special) case where the exact posterior $P_{\Phi,\Theta}(z|y)$ is samplable and computable.
EM alternates exact optimization of $\Psi$ and the pair $(\Phi,\Theta)$ in:
$$\mbox{VAE:}\;\;\;\;\;\;\; {\color{red} \Phi^*,\Theta^*} = \argmin_{\color{red} \Phi,\Theta} \min_{\color{red} \Psi} E_{y,\;z \sim P_{\color{red} \Psi}(z|y)}\;\;- \ln \frac{P_{\color{red} \Phi}(z,y)}{P_{\color{red} \Psi}(z|y)}$$

\vfill
$$\mbox{EM:}\;\;\;\;\;\; {\color{red} \Phi^{t+1},\Theta^{t+1}} =  \argmin_{\color{red} \Phi,\Theta}\;\;\;\;E_{y,\;z \sim P_{\color{red} \Phi^t,\Theta^t}(z|y)}\; - \ln P_{\color{red} \Phi,\Theta}(z,y)$$

\vfill
\centerline{\hspace{1em} Inference \hspace{6em} Update \hspace{2.5em}~}
\centerline{(E Step) \hspace{6em} (M Step) ~}
\centerline{ $P_\Psi(z|y) = P_{\Phi^{\color{red} t},\Theta^{\color{red} t}}(z|y)$ \hspace{2.5em} Hold $P_\Psi(z|y)$ fixed \hspace{0em}~}

\slide{An Alternate Derivation of the VAE}

Consider the following quantities for the distribution defined by $y \sim \pop$ and $z \sim P_\Psi(z|y)$.

{\huge
\begin{eqnarray*}
H(z,y) & = & H(y) + H(z|y) \; = \;H(z) + H(y|z) \\
\\
H(y) & = & H(z,y) - H(z|y) \\
\\
& = & H(z) + H(y|z) - H(z|y) \\
\\
& \leq & H(z,P_\Phi) + H(y,P_\Theta\;|\;z) - H(z|y) \;\;\;\mbox{replacing entropies by cross-entropes} \\
\\
& = & E_{y \sim \pop, z\sim P_\Psi(z|y)}\;\ln\frac{P_\Psi(z|y)}{P_\Phi(z)} - \ln P_\Theta(y|z)
\end{eqnarray*}
}

\slide{Encoder Autonomy}

$$H(y)\;\; \leq \;\;  E_{y \sim \pop,\;z \sim P_\Psi(z|y)}\;\ln\frac{P_\Psi(z|y)}{P_\Phi(z)} - \ln P_\Theta(y|z)$$

\vfill
Replacing an entropy by a cross-entropy will yield an equality when the model equals the source.

\vfill
Thus {\color{red} for any encoder $\Psi$} we get equality when $P_\Phi(z) = P_{\pop,\Psi}(z)$ and $P_\Theta(y|z) = P_{\pop,\Psi}(y|z)$.


\slide{Encoder Autonomy and Posterior Collapse}

$$H(y)\;\; \leq \;\;  E_{y \sim \pop,\;z \sim P_\Psi(z|y)}\;\ln\frac{P_\Psi(z|y)}{P_\Phi(z)} - \ln P_\Theta(y|z)$$

\vfill
Equality can be achieved for any $\Psi$.

\vfill
For example $P_\Psi(z|y)$ can ``collapse'' to put all its weight on a single value.  This is called posterior collapse.

\slide{The $\beta$-VAE}

{\huge
\begin{eqnarray*}
\mbox{VAE:}\;\;\;\Phi^*,\Theta^*,\Psi^* &  = & \argmin_{\Phi,\Theta,\Psi}\;E_{y\sim \pop,z\sim P_\Psi(z|y)}  \;\ln \frac{P_\Psi(z|y)}{P_\Phi(z)}  - \ln P_\Theta(y|z) \\
\\
\\
\\
\mbox{$\beta$-VAE:}\;\;\;\Phi^*,\Theta^*,\Psi^* &  = & \argmin_{\Phi,\Theta,\Psi}\;E_{y\sim \pop,z\sim P_\Psi(z|y)} \;\; \beta\left(\;\ln \frac{P_\Psi(z|y)}{P_\Phi(z)}\right)  - \ln P_\Theta(y|z)
\end{eqnarray*}
}

\vfill
$\beta < 1$ may avoid posterior collapse.  $\beta > 1$ may improve interpretability.  This is widely used but the motivation seems unclear.

\slide{Autonomous Encoder VAE (AE-VAE)}

In the autonomous encoder VAE we add an arbitrary loss function on the encoder $\Psi$.

{\huge
\begin{eqnarray*}
\mbox{AE-VAE:}\;\;\;\Phi^*,\Theta^*,\Psi^* &  = & \argmin_{\Phi,\Theta,\Psi}\;E_{y\sim \pop,z\sim P_\Psi(z|y)} \;\; \;\ln \frac{P_\Psi(z|y)}{P_\Phi(z)}  - \ln P_\Theta(y|z) + {\cal L}(\Psi)
\end{eqnarray*}
}

For any regularization on $\Psi$ the first two terms will converge to $H(y)$ if $P_\Phi(z)$ and $P_\Theta(y|z)$ are sufficiently expressive and optimizable.


\slide{END}

\end{document}
