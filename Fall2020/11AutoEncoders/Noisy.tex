\input /Users/davidmcallester/ICloude/tex/SlidePreamble
\input /Users/davidmcallester/ICloude/tex/preamble

\begin{document}

{\Huge

  \centerline{\bf TTIC 31230, Fundamentals of Deep Learning}
  \bigskip
  \centerline{David McAllester, Autumn 2021}
  \vfill
  \centerline{Noisy Channel RDAs}
  \vfill
  \vfill


\slide{The KL term as Channel Capacity}

\begin{eqnarray*}
\Phi^* & = & \argmin_\Phi\; E_{y,z} \;\; \ln \frac{p_\psi(z\; |\; y)}{p_\Phi(z)}\; - \ln p_\Phi(y|z) \\
\\
\\
 & = & \argmin_\Phi \;{\color{red} I_{\Psi,\Phi}(y,z)} + E_{y,z}\; - \ln p_\Phi(y|z)
\end{eqnarray*}

\vfill
The mutual information {\color{red} $I_{\Psi,\Phi}(y,z)$} is the channel capacity giving the {\bf rate} of information transfer from $y$ to $z$.

\slide{$L_2$ Distortion}

$${\cal L}(\Phi) = E_{y \sim \mathrm{Pop}}\;-\ln P_\Phi(\tilde{z}_\Phi(y)) + \lambda\mathrm{Dist}(y,y_\Phi(\tilde{z}_\Phi(y)))$$

\vfill
It is common to take

\begin{eqnarray*}
\mathrm{Dist}(y,\hat{y}) & = & ||y-\hat{y}||^2 \;\;\;\;\;(L_2) \\
\\
& = & -\frac{1}{\lambda}\ln p(y|\hat{y}) + C \;\;\;\;\;\mbox{for}\;p(y|\hat{y}) \propto \exp(-\lambda||y-\hat{y}||^2)
\end{eqnarray*}

\vfill
We will ignore the log density interpretation and just call this $L_2$ distortion.

\slide{$L_1$ Distortion}

$${\cal L}(\Phi) = E_{y \sim \mathrm{Pop}}\;-\ln P_\Phi(\tilde{z}_\Phi(y)) + \lambda \mathrm{Dist}(y,y_\Phi(\tilde{z}_\Phi(y)))$$

Alternatively we have

\begin{eqnarray*}
\mathrm{Dist}(y,\hat{y}) & = & ||y-\hat{y}||_1 \hspace{4em}(L_1) \\
\\
& = & -\frac{1}{\lambda}\ln p(y|\hat{y}) + C \;\;\mbox{for}\;p(y|\hat{y}) \propto \exp(-\lambda ||y-\hat{y}||_1)
\end{eqnarray*}

\vfill
Again, we will ignore the log density interpretation and just call this $L_1$ distortion.

\slide{A Variational Bound on Mutual Information}

\begin{eqnarray*}
{\color{red} I(y,z)}  & = & E_{y,\epsilon}\;\ln \frac{p_\Phi(z|y)}{p_{\Phi}(z)} \\
\\
& = & E_{y,\epsilon}\;\ln \frac{p_\Phi(z|y)}{\hat{p}_\Phi(z)} + E_{y,\epsilon}\;\ln\frac{\hat{p}_\Phi(z)}{p_{\Phi}(z)} \\
\\
& = & E_{y,\epsilon}\;\ln \frac{p_\Phi(z|y)}{\hat{p}_\Phi(z)} - KL(p_{\Phi}(z),\hat{p}_\Phi(z)) \\
\\
& {\color{red} \leq} & {\color{red} E_{y,\epsilon}\;\ln \frac{p_\Phi(z|y)}{\hat{p}_\Phi(z)}}
\end{eqnarray*}

\slide{The Noisy Channel RDA}

{\huge
\begin{eqnarray*}
\Phi^* & = & \argmin_\Phi E_{y,\epsilon}\;\ln \frac{p_\Phi(z_\Phi(y,\epsilon)|y)}{\hat{p}_\Phi(z_\Phi(y,\epsilon))}
+ \lambda \mathrm{Dist}(y,y_\Phi(z_\Phi(y,\epsilon)))
\end{eqnarray*}

\vfill
\centerline{$y,\epsilon$\includegraphics[width=4in]{\images/deconvleft} $\;\;z\;\;$ \includegraphics[width=4in]{\images/deconvright}$\hat{y}$}


\slide{VAE = RDA}
{\huge
\begin{eqnarray*}
\mbox{\color{red} VAE:}\;\;\;\Phi^*  & = & \argmin_\Phi E_{y \sim \pop,\;z \sim {\color{red} \hat{P}_\Phi(z|y)}}\;\; \ln \frac{{\color{red} \hat{P}_\Phi(z|y)}}{P_\Phi(z)}  - \ln P_\Phi(y|z)
\end{eqnarray*}
}

\vfill
$P_\Phi(z)$, $P_\Phi(y|z)$ and $\hat{P}_\Phi(z|y)$ are model components and we can switch the notation to $\hat{P}_\Phi(z)$ $\hat{P}_\Phi(y|z)$ and $P_\Phi(z|y)$
with no change in the model.

\vfill
{\huge
\begin{eqnarray*}
\mbox{\color{red} RDA:}\;\;\;\Phi^*  & = & \argmin_\Phi\; E_{y \sim \pop,\;z \sim P_\Phi(z|y)} \;\;\ln \frac{P_\Phi(z|y)}{\color{red} \hat{P}_\Phi(z)}  - \ln {\color{red} \hat{P}_\Phi(y|z)}
\end{eqnarray*}
}

\vfill
In an RDA we take $P_\Phi(y,z)$ to be $\pop(y)P_\Phi(z|y)$ so that the rate term is an upper bound on $I_\Phi(y,z)$.

\slide{Sampling}

We can require $\hat{p}_\Phi(z)$ be Gaussian.  In that case we can sample $z$ from $\hat{p}_\Phi(z)$ and generate images (as in a GAN).

\vfill
\centerline{\includegraphics[width = 3in]{\images/VariationalFaces}}
\centerline{[Alec Radford]}

\vfill
This is {\bf sampling} --- not compression.  We are decompressing noise.


\anaslide{A General Autoencoder}

\bigskip
\centerline{$y$\includegraphics[width=4in]{\images/deconvleft} $\;\;z\;\;$ \includegraphics[width=4in]{\images/deconvright}$\hat{y}$}

\bigskip
\bigskip
We show below that for $p_\Phi(z|y)$ and $\hat{p}_\Phi(z)$ both required to be Gaussian we can assume without loss
of generality that
\bigskip
$$\hat{p}_\Phi(z) = {\cal N}(0,I)$$

\slide{Gaussian Noisy-Channel RDA}

We now show that a reparameterization can always convert $\hat{p}_\Phi(z)$ to a zero-mean identity-covariance Gaussian.

\vfill
$$\Phi^* = \argmin_\Phi E_{y,\epsilon}\;\ln \frac{p_\Phi(z_\Phi(y,\epsilon)|y)}{\hat{p}_\Phi(z_\Phi(y,\epsilon))} + \lambda \mathrm{Dist}(y,y_\Phi(z_\Phi(y,\epsilon)))$$

{\color{red}
\begin{eqnarray*}
z_\Phi(y,\epsilon) & = & \mu_\Phi(y) + \sigma_\Phi(y) \odot \epsilon\;\;\;\epsilon \sim {\cal N}(0,I) \\
\\
p_\Phi(z[i]|y) & = & {\cal N}(\mu_\Phi(y)[i],\sigma_\Phi(y)[i])) \\
\\
\hat{p}_\Phi(z[i]) & = & {\cal N}(\hat{\mu}_z[i],\hat{\sigma}_z[i]) \\
\\
\mathrm{Dist}(y,\hat{y}) & = & ||y-\hat{y}||^2
\end{eqnarray*}
}

\slide{Gaussian Noisy-Channel RDA}

$$\Phi^* = \argmin_\Phi E_{y,\epsilon}\;\ln \frac{p_\Phi(z_\Phi(y,\epsilon)|y)}{\hat{p}_\Phi(z_\Phi(y,\epsilon))} + \lambda \mathrm{Dist}(y,y_\Phi(z_\Phi(y,\epsilon)))$$

\vfill
We will show that we can fix $\hat{p}_\Phi(z)$ to ${\cal N}(0,I)$.

{\color{red}
\begin{eqnarray*}
p_\Phi(z[i]|y) & = & {\cal N}(\mu_\Phi(y)[i],\sigma_\Phi(y)[i]) \\
\\
\hat{p}_\Phi(z[i]) & = & {\cal N}(0,1) \\
\\
\mathrm{Dist}(y,\hat{y}) & = & ||y-\hat{y}||^2
\end{eqnarray*}
}


\slide{Gaussian Noisy-Channel RDA}

\begin{eqnarray*}
\Phi^* &  = & \argmin_\Phi E_{y,\epsilon}\;\ln \frac{p_\Phi(z_\Phi(y,\epsilon)|y)}{\hat{p}_\Phi(z_\Phi(y,\epsilon))} + \lambda \mathrm{Dist}(y,y_\Phi(z_\Phi(y,\epsilon))) \\
\\
\\
& = & \argmin_{\Phi}\;E_{y\sim \pop} \left(\begin{array}{l}\;\;\;\;KL(p_\Phi(z|y),\hat{p}_\Phi(z)) \\
\\
+ \lambda \; E_\epsilon\;\mathrm{Dist}(y,\;y_\Phi(z_\Phi(y,\epsilon)))\end{array}\right)
\end{eqnarray*}

\slide{Closed Form KL-Divergence}

\begin{eqnarray*}
& & KL(p_\Phi(z|y),\hat{p}_\Phi(z)) \\
\\
\\
& = & \sum_i \;\frac{\sigma_\Phi(y)[i]^2 + (\mu_\Phi(y)[i]-\mu_z[i])^2}{2 \sigma_z[i]^2}
+ \ln\frac{\sigma_z[i]}{\sigma_\Phi(y)[i]} - \frac{1}{2}
\end{eqnarray*}


\slide{Standardizing $\hat{p}_\Phi(z)$}

\begin{eqnarray*}
 &  & KL(p_\Phi(z|y),p_\Phi(z)) \\
 \\
 & = & \sum_i \;\frac{ \sigma_\Phi(y)[i]^2 +(\mu_\Phi(y)[i] - \mu_z[i])^2}{2\sigma_z[i]^2}
+ \ln\frac{\sigma_z[i]}{\sigma_\Phi(y)[i]}
- \frac{1}{2}
\\
\\
\\
\\
 &  & KL(p_{\Phi'}(z|y),{\cal N}(0,I)) \\
 \\
 & = & \sum_i \;\frac{\sigma_{\Phi'}(y)[i]^2 +\mu_{\Phi'}(y)[i]^2}{2} + \ln\frac{1}{\sigma_{\Phi'}(y)[i]} - \frac{1}{2}
\end{eqnarray*}

\slide{Standardizing $\hat{p}_\Phi(z)$}

\begin{eqnarray*}
KL_\Phi & = & \sum_i \;\frac{ \sigma_\Phi(y)[i]^2 +(\mu_\Phi(y)[i] - \mu_z[i])^2}{2\sigma_z[i]^2}
+ \ln\frac{\sigma_z[i]}{\sigma_\Phi(y)[i]} - \frac{1}{2}
\\
KL_{\Phi'} & = & \sum_i \;\frac{\sigma_{\Phi'}(y)[i]^2 +\mu_{\Phi'}(y)[i]^2}{2} + \ln\frac{1}{\sigma_{\Phi'}(y)[i]} - \frac{1}{2}
\end{eqnarray*}

Setting $\Phi'$ so that
\begin{eqnarray*}
\mu_{\Phi'}(y)[i] & = & (\mu_\Phi(y)[i] - \mu_z[i])/\sigma_z[i] \\
\sigma_{\Phi'}(y)[i] & = & \sigma_\Phi(y)[i]/\sigma_z[i]
\end{eqnarray*}

\vfill
gives {\color{red} $KL(p_{\Phi}(z|y),\hat{p}_\Phi(z)) = KL(p_{\Phi'}(z|y),{\cal N}(0,I))$}.

\slide{END}

}
\end{document}
