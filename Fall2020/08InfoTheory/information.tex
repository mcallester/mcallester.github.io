\input ../../SlidePreamble
\input ../../preamble

\begin{document}

{\Huge

  \centerline{\bf TTIC 31230, Fundamentals of Deep Learning}
  \bigskip
  \centerline{David McAllester, Winter 2021}
  \vfill
  \centerline{\bf Gaussian Models}
  \vfill
  \centerline{\bf and The Parels of Differential Entropy}
  \vfill
\vfill
\vfill

\slide{Gaussian VAEs}

\vfill
\begin{eqnarray*}
\mbox{VAE:}\;\;\;\Phi^*,\Psi^* &  = & \argmin_{\Phi,\Psi}\;E_{y,z}  \;\ln \frac{\hat{p}_\Psi(z|y)}{p_\Phi(z)}  - \ln p_\Phi(y|z) \\
\end{eqnarray*}

\vfill
All models are Gaussian densities.

\vfill
{\huge 
\begin{eqnarray*}
p_\Phi(z[i]) & \propto & \exp((z[i]-\mu_\Phi[i])^2/2\sigma^2_\Phi[i])\\
\\
p_\Psi(z[i]|y) & \propto & \exp((z[i]-\hat{z}_\Psi(y)[i])^2/2\sigma^2_\Psi(y)[i])\\
\\
p_\Phi(y[i]|z) & \propto & \exp((y[i]-\hat{y}_\Phi(z)[i])^2/2\sigma_\Phi^2(z)[i])\\
\end{eqnarray*}
}

\slide{Differential Entropy}

In the case of a continuous density (as opposed to a discrete probability) we have the notion of differential entropy.

\vfill
For a density $p(x)$ on a real value $x$ we have

\vfill
\begin{eqnarray*}
H(p) & = & E_{x \sim p}\;\left[-\ln p(x)\right] \\
\\
& = &\int_{-\infty}^\infty p(x) \;(-\ln p(x)) dx
\end{eqnarray*}

\slide{Differential Entropy can Diverge to $-\infty$}

For a uniform distribution over an interval on the real line of width $\Delta$ we have

\begin{eqnarray*}
H & = & E_{x \sim p} \left[-\ln p(x)\right] \\
\\
& = & E_{x \sim p} \left[-\ln \frac{1}{\Delta}\right] \\
\\
& = & \ln \Delta
\end{eqnarray*}

\vfill
\centerline{As $\Delta \rightarrow 0$ we have $H \rightarrow -\infty$}

\slide{Differential Entropy can Diverge to $-\infty$}

\hfill

{\huge
\begin{eqnarray*}
H({\cal N}(0,\sigma^2)) & = & E_{x \sim {\cal N}(0,\sigma^2)} \left[ -\ln \left(\frac{1}{\sigma\sqrt{2\pi}} \exp{\frac{-x^2}{2\sigma^2}}\right)\right] \\
\\
& =& E_{x \sim {\cal N}(0,\sigma^2)} \left[ \ln (\sigma\sqrt{2\pi}) + \frac{-x^2}{2\sigma^2}\right] \\
\\
& = & (\ln\sigma) + \ln(\sqrt{2\pi}) + E_x \left[\frac{x^2}{2\sigma^2}\right] \\
\\
& = & (\ln\sigma) + \ln(\sqrt{2\pi}) + \frac{1}{2} \\
\end{eqnarray*}
}

\vfill
\centerline{As $\sigma \rightarrow 0$ we have $H \rightarrow -\infty$}

\slide{Sensitivity to the Choice of Units}

$$H(N(0,\sigma)) = C + \ln \sigma$$

\vfill
Differential entropy depends on the choice of units --- a distributions on lengths will have a different entropy
when measuring in inches than when measuring in feet.

\slide{Differential Cross-Entropy can Diverge to $-\infty$}

Consider the unsupervised training objective.

$$\Phi^* = \argmin_\Phi E_{y \sim \mathrm{train}}\;-\ln p_\Phi(y)$$

\vfill
The training set is finite (discrete).

\vfill
For each $y$ the density $p_\Phi(y)$ can go to infinity.

\vfill
This will drive the cross-entropy training loss to $-\infty$.


\slide{Differential Entropy Can Be Considered Infinite}

An actual real number carries an infinite number of bits.

\vfill
Consider quantizing the real numbers into bins.

\vfill
A continuous probability densisty $p$ assigns a probability $p(B)$ to each bin.

\vfill
As the bin size decreases toward zero the entropy of the bin distribution increases toward $\infty$.

\vfill
A meaningful convention is that $H(p) = +\infty$ for any continuous density $p$.

\slide{Differential KL-divergence is Meaningful}

$$KL(p,q) = \int \left( \ln \frac{p(x)}{q(x)}\right) p(x) dx$$

\vfill
Unlike differential entropy, differential KL divergence is always non-negative (but can be infinite).

\vfill
Note that $KL(p,p) = 0$ independent of $H(p)$.

\slide{Mutual Information}

$I(x,y)$ is the reduction in the number of bits we need to name $y$ as a result of observing $x$ (on average).

{\huge
\begin{eqnarray*}
I(x,y) & = & E \;\ln \frac{P(x,y)}{P(x)P(y)} \\
\\
\\
& = & E \;\ln \frac{P(x,y)}{P(x)} - \ln P(y) \\
\\
\\
& = & H(y) - H(y|x) \\
\end{eqnarray*}
}
Intuitively, how much does $x$ know about $y$?


\slide{Differential Mutual Information}

\begin{eqnarray*}
  I(x,y) & = & KL(p(x,y),p(x)p(y)) \\
  \\
  & = & E_{x,y} \ln \frac{p(x,y)}{p(x)p(y)}
\end{eqnarray*}

\vfill
Mutual information is a KL divergence and hence differential mutual information is always non-negative.

\slide{The Data Processing Inequality}

For continuous $y$ and $z$ with $z = f(y)$ we get that $H(z)$ can be either larger or smaller than $H(y)$ (consider $z = ay$ for $a >1$ vs. $a<1$).

\vfill
However, mutual information is a KL divergence and is more meaningful than entropy and for $z = f(y)$ we do have

$$I(x,z) \leq I(x,y)$$

\slide{Continuous Cross-Entropy as Distortion}

For a Gaussian VAE on images we typically have

{\huge
\begin{eqnarray*}
\Phi^*, \Psi^* & = & \argmin_{\Phi,\Psi}\; E_{y\sim\popd,z \sim \hat{p}_\Psi(z|y)}\;\; \ln \frac{ \hat{p}_\Psi(z|y)}{p_\Phi(z)}  - \ln p_\Phi(y|z)\\
\\
p_\Phi(y|z) & \propto & \frac{1}{Z}\;\exp(||y-\hat{y}_\Phi(z)||^2/2\sigma^2) \\
\\
\Phi^*, \Psi^* & = & \argmin_{\Phi,\Psi}\; E_{y\sim\popd,z \sim \hat{p}_\Psi(z|y)}\;\; \ln \frac{ \hat{p}_\Psi(z|y)}{p_\Phi(z)}  \;{\color{red} +\; \frac{1}{2\sigma^2}||y-\hat{y}_\Phi(z)||^2 + d\ln \sigma}
\end{eqnarray*}
}

The KL divergence term is not problematic. The problematic differential cross-entropy term can just be thought of as a weighted $L_2$ distortion.

\slide{Continuous Cross-Entropy as Distortion}


{\huge
\begin{eqnarray*}
\Phi^*, \Psi^* & = & \argmin_{\Phi,\Psi}\; E_{y\sim\popd,z \sim \hat{p}_\Psi(z|y)}\;\; \ln \frac{ \hat{p}_\Psi(z|y)}{p_\Phi(z)}  \;{\color{red} +\; \lambda\;\mathrm{Dist}(y,\hat{y}_\Phi(z))}
\end{eqnarray*}
}

\vfill
Various choices for distortion are possible including $L_2$ and $L_1$ distortion measures.

\begin{eqnarray*}
\mathrm{Dist}(y,\hat{y}) & = & ||y-\hat{y}||^2 \hspace{4em}(L_2) \\
\\
\mathrm{or}\;\;\mathrm{Dist}(y,\hat{y}) & = & ||y-\hat{y}||_1 = \sum_i \;|y[i] - \hat{y}[i]| \hspace{4em}(L_1)
\end{eqnarray*}


\slide{END}

}
\end{document}

\slide{Source Coding Theorem}

Consider a probability distribution $\mathrm{Pop}$ on a finite set $S$.

\vfill
Consider a code $C$ assigning a bit string code word $C(y_1,\ldots,y_B)$ to each possible batch of $B$ elements with $y_i \sim \mathrm{Pop}$.

\vfill
Source coding theorem: As $B \rightarrow \infty$ the optimal coding uses exactly $H(\mathrm{Pop})$
bits per batch element.

\slide{Prefix Free Codes}

Let $S$ be a finite set.

\vfill
Let $C$ be assignment of a bit string $C(y)$ to each $y \in S$.

\vfill
$C$ is called {\em prefix-free} if for $x \not = y$ we have that $C(x)$ is not a prefix of $C(y)$.

\vfill
A concatenation of sequence of prefix-free code words can be uniquely segmented (parsed) back into a sequence of code words.

\slide{Prefix-Free Codes as Trees and as Probabilities}

A prefix-free code defines a binary branching tree --- branch on the first code bit, then the second, and so on.

\vfill
The leaves of this tree are labeled with the elements of $S$.

\vfill
The code defines a probability distribution on $S$ by randomly selecting branches.

\vfill
We have $P_C(y) = 2^{-|C(y)|}$.

\slide{The Source Coding Theorem}

(1) There exists a prefix-free code $C$ such that
$$|C(y)| <= (- \log_2 \mathrm{Pop}(y)) + 1$$
and hence
$$E_{y\sim \mathrm{Pop}} |C(y)| \leq H(\mathrm{Pop}) +1$$

\vfill
(2) For any prefix-free code $C$

$$E_{y \sim \mathrm{Pop}}\;|C(y)| = E_{y \sim \pop} -\ln \;P_C(y) = H(\pop,P_C) \geq H(\mathrm{Pop})$$

\slide{Code Construction}

\vfill
We construct a code by iterating over $y \in S$ in order of decreasing probability (most likely first).

\vfill
For each $y$ select a code word $C(y)$ (a tree leaf) with length (depth)

\vfill
$$|C(y)| = \lceil - \log_2 \mathrm{Pop}(y)\rceil$$

\vfill
and where $C(y)$ is not an extension of (under) any previously selected code word.

\slide{Code Existence Proof}

At any point before coding all elements of $S$ we have
$$\sum_{y \in \mathrm{Defined}} 2^{-|C(y)|} \leq \sum_{y \in \mathrm{Defined}} \mathrm{Pop}(y) < 1$$

\vfill
Therefore there exists an infinite descent into the tree that misses all previous code words.

\vfill
Hence there exists a code word $C(x)$ not under any previous code word with
$|C(x)| = \lceil - \log_2 \mathrm{Pop}(y)\rceil$.

\vfill
Furthermore $C(x)$ is at least as long as all previous code words and hence $C(x)$ is not a prefix of any previously selected code word.

\slidetwo{Huffman Coding Produces an Optimal Code}
{For Finite Distributions}

Maintain a list of trees $T_1,\;\dots,\;T_N$ where each leaf of each tree is labeled with a population element $y$.

\vfill
We will write $y \in T_i$ to mean that some leaf of $T_i$ is labeled with $y$ and for $y \in T_i$ write $d(y,T_i)$ for the depth
of the leaf labeled with $y$ in the tree $T_i$.

\vfill
Initially each tree is just one root node labeled with a single $y$ value and every $y$ value labels some tree.

\slide{Tree Weight}

\vfill
We define the weight of a free $T_i$ to be the total probability mass of the the items labeling the leaves.

$$W(T_i) = \sum_{y \in T_i}\;\pop(y)$$

\vfill
The Huffman coding algorithm repeatedly merges the two trees of lowest weight into a single tree until all trees are merged.

\vfill
When all trees are merged the weight of the final tree is the expected code length.

\slide{Optimality of Huffman Coding}

{\bf Theorem}: The Huffman code $T$ for $\mathrm{Pop}$ gives an optimal code $C$ --- for any other tree $T'$ defining code $C'$ we have

$$E_{y \sim \pop}\;|C(y)| \leq E_{y \sim \pop} |C'(y)|$$

\vfill
{\bf Invariant:} The merge operation preserves the invariant that there exists an optimal tree including
all the subtrees on the list.

\slide{The Merge Operation Preserves the Invariant}

Assume there exists an optimal tree containing the given subtrees.

\vfill
Consider the two subtrees $T_i$ and $T_j$ of minimal weight.  Without loss of generality we can assume that $T_i$ is at least as deep as $T_j$.

\vfill
Swapping the sibling of $T_i$ for $T_j$ brings $T_i$ and $T_j$ together.  This can only improve the average depth (next slide).

\slide{Why The Swap Can Only Improve the Tree}

We can swap a heavier deeper tree for a shallower lighter tree.

\vfill
For a subtree $T_i$ of an optimal tree $T$ let $d(T_i)$ be the depth of $T_i$ in $T$.

Swapping $T_1$ and $T_2$ replaces a cost of $d(T_i)W(T_i) + d(T_2)W(T_2)$ with $d(T_1)W(T_2) + d(T_2)W(T_1)$.

\vfill
For $d(T_1) \geq d(T_2)$ and $W(T_1) \geq W(T_2)$ we have

\vfill
$$d(T_1)W(T_1) + d(T_2)W(T_2) \geq d(T_1)W(T_2) + d(T_2)W(T_1)$$
