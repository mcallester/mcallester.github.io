\input ../../SlidePreamble
\input ../../preamble

\begin{document}

{\Huge

  \centerline{\bf TTIC 31230, Fundamentals of Deep Learning}
  \bigskip
  \centerline{David McAllester, Autumn 2020}
  \vfill
  \vfill
  \centerline{\bf Monte-Carlo Markov Chain (MCMC) Sampling}
\vfill
\vfill
\vfill

\slide{Notation}

$x$ is an input (e.g. an image).

\vfill
$\hat{\cal Y} [N]$ is a structured label for $x$ --- a vector $\hat{\cal Y}[0],\ldots,\hat{\cal Y}[N-1]$. (e.g., $n$ ranges over pixels where $\hat{\cal Y}[n]$ is a semantic label of pixel $n$.)

\vfill
$\hat{\cal Y}/n$ is the set of labels assigned by $\hat{\cal Y}$ at indeces (pixels) other than $n$.

\vfill
$\hat{\cal Y}[n=\ell]$ is the structured label identical to $\hat{\cal Y}$ except that it assigns label $\ell$ to index (pixel) $n$.

\slide{Sampling From the Model}

For back-propagation of $- \ln P_s(\hat{\cal Y})$ through the exponential softmax defined by $P_s(\hat{\cal Y}) = \frac{1}{Z} e^{s(\hat{\cal Y})}$ we have

\vfill
\begin{eqnarray*}
    s^N.\mathrm{grad}[n,\ell] & = &   P_{\hat{\cal Y}' \sim P_s}(\;\;{\color{red} \hat{\cal Y}'}[n] = \ell\;\;) \\
    & & - \bbone[\;\;{\color{red} \hat{\cal Y}}[n] = y\;\;] \\
    \\
    s^E.\mathrm{grad}[\tuple{n,m},\ell,\ell'] & = &  P_{\hat{\cal Y}' \sim P_s}(\;\;{\color{red} \hat{\cal Y}'}[n] = \ell \; \wedge \; {\color{red} \hat{\cal Y}'}[m] = \ell'\;\;) \\
    & & - \bbone[\;\;{\color{red} \hat{\cal Y}}[n] = \ell\; \wedge \; {\color{red} \hat{\cal Y}}[m] = \ell'\;\;]
\end{eqnarray*}

\slide{MCMC Sampling}
The model marginals, such as the node marginals
 ${\color{red} P_s(\hat{\cal Y}[n]=\ell)}$, can be estimated by sampling $\hat{\cal Y}$ from $P_s(\hat{\cal Y})$.

\vfill
There are various ways to design a Markov process whose states are the structured labels $\hat{\cal Y}$ and whose stationary distribution is $P_s$.

\vfill
Given such a process we can sample $\hat{\cal Y}$ from $P_s$ by running the process past its mixing time.

\vfill
We will consider Metropolis MCMC and the Gibbs MCMC.  But there are more (like Hamiltonian MCMC).

\slide{Metroplis MCMC}

We assume a neighor relation on the structured labels $\hat{\cal Y}$ and let $N(\hat{\cal Y})$ be the set of neighbors of structured label $\hat{\cal Y}$.

\vfill
For example, $N(\hat{\cal Y})$ can be taken to be the set of assignments $\hat{\cal Y}'$ that differ form $\hat{\cal Y}$ on exactly one index (pixel) $n$.

\vfill
For the correctness of Metropolis MCMC we need that all structured labels have the same number of neighbors and that the neighbor relation is symmetric ---
$\hat{\cal Y}' \in N(\hat{\cal Y})$ if and only if $\hat{\cal Y} \in N(\hat{\cal Y}')$.

\slide{Metropolis MCMC}

Pick an initial state $\hat{\cal Y}_0$ and for $t \geq 0$ do

\vfill
\begin{quotation}

    \noindent \begin{enumerate}
    \item Pick a neighbor $\hat{\cal Y}' \in N(\hat{\cal Y}_t)$ uniformly at random.

    \vfill      
    \item If $s(\hat{\cal Y}') > s(\hat{\cal Y}_t)$ then {\color{red} $\hat{\cal Y}_{t+1} = \hat{\cal Y}'$}

    \vfill      
    \item If $s(\hat{\cal Y}') \leq s(\hat{\cal Y})$ then with probability $e^{-\Delta s} = e^{-(s(\hat{\cal Y}) - s(\hat{\cal Y}'))}$
   do  {\color{red} $\hat{\cal Y}_{t+1} = \hat{\cal Y}'$} and otherwise {\color{red} $\hat{\cal Y}_{t+1} = \hat{\cal Y}_t$} 
  \end{enumerate}  
\end{quotation}

\slide{The Metropolis Markov Chain}
We need to show that $P_s(\hat{\cal Y}) = \frac{1}{Z}e^{s(\hat{\cal Y})}$ is a stationary distribution of this process.

\vfill
Let $Q(\hat{\cal Y})$ be the distribution on states defined by drawing a state from $P_s$ and applying one stochastic transition of the Metropolis process.

\vfill
We must show that $Q(\hat{\cal Y}) = P_s(\hat{\cal Y})$.

\slide{The Stationary Distribution}
Let $P_{\mathrm{Trans}}(\hat{\cal Y} \rightarrow \hat{\cal Y}')$ denote the probability of transitioning from $\hat{\cal Y}$ to $\hat{\cal Y}'$, or more formally,

\vfill
$$P_{\mathrm{Trans}}(\hat{\cal Y} \rightarrow \hat{\cal Y}') = P(\hat{\cal Y}_{t+1} = \hat{\cal Y}'\;|\; \hat{\cal Y}_y = \hat{\cal Y})$$

\vfill
We can then write $Q(\hat{\cal Y}')$ as
\vfill
$$Q(\hat{\cal Y}') =  \sum_{\hat{\cal Y}}\;P_s(\hat{\cal Y})P_\mathrm{Trans}(\hat{\cal Y} \rightarrow \hat{\cal Y}')$$

\slide{The Stationary Distribution}

{\huge
\begin{eqnarray*}
Q(\hat{\cal Y}')& = &  \sum_{\hat{\cal Y}}\;P_s(\hat{\cal Y})P_\mathrm{Trans}(\hat{\cal Y} \rightarrow \hat{\cal Y}') \\
\\
\\
& = & P_s({\cal Y'})P_\mathrm{Trans}(\hat{\cal Y}' \rightarrow \hat{\cal Y}') + \sum_{\hat{\cal Y} \in N(\hat{\cal Y}')} P_s(\hat{\cal Y}){P_{\mathrm{Trans}}(\hat{\cal Y} \rightarrow \hat{\cal Y}')} \\
\\
\\
& = & \left\{\begin{array}{l}P_s(\hat{\cal Y}')\left(1-\sum_{\hat{\cal Y} \in N({\cal Y'})} P_\mathrm{Trans}(\hat{\cal Y}'\rightarrow \hat{\cal Y})\right) \\
\\
  + \; \sum_{\hat{\cal Y} \in N(\hat{\cal Y}')} P_s(\hat{\cal Y}){P_{\mathrm{Trans}}(\hat{\cal Y} \rightarrow \hat{\cal Y}')}\end{array}\right.
\end{eqnarray*}
}

\slide{The Stationary Distribution}

{\huge
\begin{eqnarray*}
Q(\hat{\cal Y}')& = &  \left\{\begin{array}{l}P_s(\hat{\cal Y}')\left(1-\sum_{\hat{\cal Y} \in N({\cal Y'})} P_\mathrm{Trans}(\hat{\cal Y}'\rightarrow \hat{\cal Y})\right) \\
\\
  + \; \sum_{\hat{\cal Y} \in N(\hat{\cal Y}')} P_s(\hat{\cal Y}){P_{\mathrm{Trans}}(\hat{\cal Y} \rightarrow \hat{\cal Y}')}\end{array}\right. \\
  \\
  \\
  & = &  \left\{\begin{array}{l}P_s(\hat{\cal Y}') \\
  \\
  -\sum_{\hat{\cal Y} \in N({\cal Y'})} P_s(\hat{\cal Y}')P_\mathrm{Trans}(\hat{\cal Y}'\rightarrow \hat{\cal Y}) \\
\\
  + \; \sum_{\hat{\cal Y} \in N(\hat{\cal Y}')} P_s(\hat{\cal Y}){P_{\mathrm{Trans}}(\hat{\cal Y} \rightarrow \hat{\cal Y}')}\end{array}\right. \\
  \\
  \\
  &= & P_s(\hat{\cal Y}')\; - \;\mbox{flow out}\; +\;\mbox{flow in}
\end{eqnarray*}
}

\slide{Detailed Balance}

Detailed balance means that for each pair of neighboring assignments $\hat{\cal Y}$, $\hat{\cal Y}'$ we have equal flows in both directions.

\vfill
$$P_s(\hat{\cal Y}')P_{\mathrm{Trans}}(\hat{\cal Y}' \rightarrow \hat{\cal Y}) = P_s(\hat{\cal Y})P_{\mathrm{Trans}}(\hat{\cal Y} \rightarrow \hat{\cal Y}')$$

\vfill
If we can show detailed balance we have that the flow out equals the flow in and we get $Q({\cal Y'}) = P_s(\hat{\cal Y}')$ and hence $P_s$ is the stationary distribution.

\slide{Detailed Balance}

To show detailed balance we can assume without loss generality that $s(\hat{\cal Y}') \geq s(\hat{\cal Y})$.

\vfill
We then have

\begin{eqnarray*}
P_s(\hat{\cal Y}')P_{\mathrm{Trans}}(\hat{\cal Y}' \rightarrow \hat{\cal Y}) & = & \frac{1}{Z}e^{s(\hat{\cal Y}')}\;\;\left(\frac{1}{N}\;e^{-\Delta s}\right) \\
\\
& = & \frac{1}{Z}e^{s(\hat{\cal Y})}\;\;\frac{1}{N} \\
\\
 &= &P_s(\hat{\cal Y})P_{\mathrm{Trans}}(\hat{\cal Y} \rightarrow \hat{\cal Y}')
\end{eqnarray*}

\slide{Gibbs Sampling}

The Metropolis algorithm wastes time by rejecting proposed moves.

\vfill
Gibbs sampling avoids this move rejection.

\vfill
In Gibbs sampling we select a node $n$ at random and change that node by drawing a new node value conditioned on the current values of the other nodes.

\vfill
We let {\color{red} $\hat{\cal Y} \backslash n$} be the assignment of labels given by $\hat{\cal Y}$ except that no label is assigned to node $n$.

\vfill
We let {\color{red} $\hat{\cal Y}[N(n)]$} be the assignment that $\hat{\cal Y}$ gives to the nodes (pixels) that are the neighbors of node $n$ (connected to $n$ by an edge.)

\slide{Gibbs Sampling}

Markov Blanket Property:
{\color{red} $$P_s(\hat{\cal Y}[n] \;|\;\hat{\cal Y} \backslash n) = P_s(\hat{\cal Y}[n] \;|\; \hat{\cal Y}[N(n)])$$}
\vfill
Gibbs Sampling, Repeat:

\begin{itemize}
\item   Select $n$ at random

\item {\color{red} draw $y$ from $P_s(\hat{\cal Y}[n]\;|\;\hat{\cal Y} \backslash n) = P_s(\hat{\cal Y}[n] \;|\;\hat{\cal Y}[N(n)])$}

\item $\hat{\cal Y}[n] = y$
\end{itemize}

\vfill
This algorithm does not require knowledge of $Z$.

\vfill
The stationary distribution is $P_s$.

\slide{END}

}

\end{document}
