\input /users/davidmcallester/ICloude/tex/SlidePreamble
\input /users/davidmcallester/ICloude/tex/preamble

\begin{document}

{\Huge

  \centerline{\bf TTIC 31230, Fundamentals of Deep Learning}
  \bigskip
  \centerline{David McAllester, Autumn 2020}
  \vfill
  \vfill
  \centerline{\bf Deep Graphical Models}
  \vfill
  \centerline{\bf aka, Energy Based Models}
\vfill
\vfill
\vfill

\slide{Distributions on Exponentially Large Sets}

\vfill
{\color{red}
$$\Phi^* = \argmin_\Phi E_{(x,y) \sim \mathrm{Pop}}\;-\ln \;P(y|x)$$

\vfill
$$\Phi^* = \argmin_\Phi E_{y \sim \mathrm{Pop}}\;-\ln \;P(y)$$
}

{\color{red} The structured case:} $y \in {\cal Y}$ where ${\cal Y}$ is discrete but {\color{red} iteration over $\hat{y} \in {\cal Y}$ is infeasible}.

\slide{Parsing and CKY}

Consider the case where $x$ is a sentence and $y$ is a parse tree for $x$.  There are exponentially many possible parse trees for a given sentence.

\vfill
The Cocke–Kasami–Younger (CKY) algorithm is a dynamic programming algorithm for finding the most probably parse under a certain family of energy based models.

\vfill
CKY is still the most accurate way to parse sentences where the energy based model is computed by a deep network.

\slide{Speech Recognition and CTC}

Consider the case where $x$ is the sound wave from a microphone and $y$ is the transcription into written language.  There are clearly exponentially many
possible transcriptions.

\vfill
Connectionist Temporal classification (CTC) is a dynamic programming algorithm for finding the most likely output under a certain family of energy based models.

\vfill
CTC is still the most accurate way to do speech recognition where the energy based model is computed by a deep network.


\slide{Semantic Segmentation}
\centerline{\includegraphics[height = 2.5in]{\images/SemSeg}}

\vfill
We want to assign each pixel to one of $Y$ semantic classes.

\vfill
For example ``person'', ``car'', ``building'', ``sky'' or ``other''.

\slide{Semantic Segmentation}

\centerline{\includegraphics[height = 2.5in]{\images/SemSeg}}

\vfill
Although semantic segmentation is not currently done with energy based models, perhaps it should be.

\vfill
Semantic segmentation is simpler than parsing or speech recognition and will be used as a simple example of energy based models.

\slide{Constructing a Graph}

We construct a graph whose nodes are the pixels and where there is an edges between each pixel and its four nearest neighboring pixels.

\vfill
%\centerline{\includegraphics[height= 4in]{\images/grid}}

\slide{Labeling the Nodes of a Graph}
\centerline{\includegraphics[height= 1.5in]{\images/Graph}}     
\medskip

$\hat{y} $ assigns a semantic class $\hat{y}[n]$ to each node (pixel) $n$.

\vfill
We assign a score to $\hat{y}$ by assigning a score to each node and each edge of the graph.

{\color{red} $$s(\hat{y}) = \sum_{n \in \mathrm{Nodes}}\; s^N[n,\hat{y}[n]]\; + \sum_{\tuple{n,m} \in \mathrm{Edges}}\;s^E[\tuple{n,m},\hat{y}[n],\hat{y}[m]]$$}
\centerline{Node Scores \hspace{6em}Edge Scores \hspace{3em}~}

\slide{Using Deep Networks}

For input $x$ we use a network to compute the score tensors.

\vfill
\begin{eqnarray*}
s^N[N,Y] & = & f^N_\Phi(x) \\
\\
\\
s^E[E,Y,Y] & = & f^E_\Phi(x)
\end{eqnarray*}

\slide{Exponential Softmax}

{\huge
$$\begin{array}{lrcl}
\mbox{for}\;\hat{y} & {\color{red} s(\hat{y})} & = & \sum_n\;s^N[n,\hat{y}[n]] + \sum_{\tuple{n,m} \in \mathrm{Edges}}\;s^E[\tuple{n,m},\hat{y}[n],\;\hat{y}[m]] \\
\\
\\
\mbox{for}\;\hat{y} & {\color{red} P_s(\hat{y})} & = & \softmax_{\hat{y}}\;s(\hat{y}) \;\;\;\mbox{\color{red} all possible $\hat{y}$} \\
\\
 & {\cal L} & = & - \ln P_s(y) \;\;\;{\color{red} \;\;\;\;\;\;\mbox{gold label (training label) $y$}}
\end{array}$$
}

\slide{Exponential Softmax is Typically Intractable}
\centerline{\includegraphics[height= 1.5in]{\images/Graph}}
\medskip
$\hat{y} $ assigns a label $\hat{y}[n]$ to each node $n$.

\vfill
$s(\hat{y})$ is defined by a sum over node and edge tensor scores.

\vfill
$P_s(\hat{y})$ is defined by an exponential softmax over $s(\hat{y})$.

\vfill
Computing $Z$ in general is \#P hard (there is an easy direct reduction from SAT).

\slidetwo{Compactly Representing Scores}{on Exponentially Many Labels}

The tensor {\color{red} $s^N[N,Y]$} holds $NY$ scores.

\vfill
The tensor {\color{red} $s^E[E,Y,Y]$} holds $EY^2$ scores where $e$ ranges over edges $\tuple{n,m} \in \mathrm{Edges}$.

\slide{Back-Propagation Through Exponential Softmax}

\begin{eqnarray*}
s^N[I,Y] & = & f^N_\Phi(x) \\
s^E[E,Y,Y] & = & f^E_\Phi(x)
\end{eqnarray*}

\vfill
\begin{eqnarray*}
{\color{red} s(\hat{y})} & = & \sum_n\;s^N[n,\hat{y}[n]] + \sum_{\tuple{n,m} \in \mathrm{Edges}}\;s^E[\tuple{n,m},\hat{y}[n],\;\hat{y}[m]] \\
\\
{\color{red} P_s(\hat{y})} & = & \softmax_{\hat{y}}\;s(\hat{y}) \;\;\mbox{\color{red} all possible $\hat{y}$} \\
\\
{\cal L} & = & {\color{red} - \ln P_s(y) \;\;\;\mbox{gold label $y$}}
\end{eqnarray*}

\vfill
We want the gradients {\color{red} $s^N.\grad[N,Y]$} and {\color{red} $s^E.\grad[E,Y,Y]$}.


\slide{END}

}

\end{document}
