\input ../../SlidePreamble
\input ../../preamble

\begin{document}

{\Huge

  \centerline{\bf TTIC 31230, Fundamentals of Deep Learning}
  \bigskip
  \centerline{David McAllester, Autumn 2020}
  \vfill
  \vfill
  \centerline{\bf Pseudo-Likelihood and Contrastive Divergence}
\vfill
\vfill
\vfill

\slide{Psuedo-Likelihood}

For any distribution $P(\hat{\cal Y})$ on colorings $\hat{\cal Y}$,
we define the {\color{red} pseudo-likelihood}  $\tilde{P}(\hat{\cal Y})$ as follows

{\color{red} $$\tilde{P}(\hat{\cal Y}) = \prod_n\;P(\hat{\cal Y}[n]\;|\; \hat{\cal Y}/n) = \prod_n\;P(\hat{\cal Y}[n]\;|\; \hat{\cal Y}[N(n)])$$}

\vfill
While computing $P_{\Phi,x}({\cal Y})$ is intractable, computing $\tilde{P}_{\Phi,x}({\cal Y})$ is tractable.  We then use

{\color{red} $$\Phi^* = \argmin_\Phi E_{\tuple{x,{\cal Y}} \sim \pop}\;\;-\ln \tilde{P}_{\Phi,x}({\cal Y})$$}

\slide{Pseudolikelihood Theorem}

{\color{red} $$\argmin_Q \; E_{{\cal Y} \sim \pop} \;-\ln \tilde{Q}({\cal Y}) = \pop$$}

\vfill
It suffices to show that for any $Q$ we have

\vfill
$$ E_{{\cal Y} \sim \pop}\;-\ln \widetilde{\pop}({\cal Y}) \leq \;E_{{\cal Y} \sim \pop}\;-\ln \tilde{Q}({\cal Y})$$

\slide{Proof II}

{\huge
\begin{eqnarray*}
  & & \min_Q \;E_{Y\sim \pop} -\ln \tilde{Q}(Y) \\
  \\
  & = & \min_Q \;E_{{\cal Y}\sim \pop} \;\sum_n\;-\ln Q({\cal Y}[n]|{\cal Y}[N(n)]) \\
  \\
  & \geq & \min_{P_1,\ldots,P_N} \;E_{{\cal Y}\sim \pop} \;\sum_n\;-\ln P_n({\cal Y}[n]|{\cal Y}[N(n)]) \\
  \\
  & = & \min_{P_1,\ldots,P_N} \;\sum_n \;E_{{\cal Y} \sim \pop}\;\; -\ln P_n({\cal Y}[n]|{\cal Y}[N(n)]) \\
  \\          
  & = & \sum_n \;\min_{P_n}\;E_{{\cal Y} \sim \pop}\;\; -\ln P_n({\cal Y}[n]|{\cal Y}[N(n)]) \\
  \\
    & = & \sum_n E_{{\cal Y} \sim \pop}\;\; -\ln \pop({\cal Y}[n]|{\cal Y}[N(n)]) =  E_{{\cal Y} \sim \pop}{-\ln \widetilde{\pop}({\cal Y})}
\end{eqnarray*}
}

\slideplain{Contrastive Divergence (CDk)}

In contrastive divergence we first construct an MCMC process whose stationary distribution is ${\color{red} P_s}$.  This could be
Metropolis or Gibbs or something else.

\vfill
{\bf Algorithm CDk}: Given a gold segmentation ${\cal Y}$, start the MCMC process from initial state ${\cal Y}$ and run the process for $k$ steps
to get ${\color{red} \hat{\cal Y}'}$.  Then take the loss to be

\vfill
{\color{red} $${\cal L}_{\mathrm{CD}}  = s(\hat{\cal Y}') - s({\cal Y})$$}

If $P_s = \pop$ then the the distribution on $\hat{\cal Y}'$ is the same as the distribution on ${\cal Y}$ and the
expected loss gradient is zero.

\slideplain{Gibbs CD1}

CD1 for the Gibbs MCMC process is a particularly interesting special case.

\vfill
{\bf Algorithm (Gibbs CD1)}: Given ${\cal Y}$, select a node $n$ at random and draw {\color{red} $y \sim P({\cal Y}[n]\;| \;{\cal Y}[N(n)])$}. Define {\color{red} ${\cal Y}[n=y]$}
to be the assignment (segmentation) which is the same as ${\cal Y}$ except that node $n$ is assigned label $y$.  Take the loss to be

\vfill
{\color{red} $${\cal L}_{\mathrm{CD}}  = s({\cal Y}[n=y]) - s({\cal Y})$$}

\slide{Gibbs CD1 Theorem}

Gibbs CD1 is equivalent in expectation to pseudolikelihood.

{\huge
\begin{eqnarray*}
{\cal L}_{\mathrm{PL}} & = & E_{{\cal Y} \sim \pop}\;\sum_n \; - \ln P_s({\cal Y}[n]=y\;|\;{\cal Y}[N(n)]) \\
\\
 & = & E_{{\cal Y} \sim \pop}\;\sum_n -\ln \frac{e^{s({\cal Y})}}{Z_n}\;\;\;\;\;{Z_n = \sum_{y'} e^{s({\cal Y}[n=y'])}} \\
\\
& = & E_{{\cal Y} \sim \pop}\;\sum_n\; \left(\ln Z_n - s({\cal Y})\right) \\
\\
\nabla_\Phi {\cal L}_{\mathrm{PL}} & = & E_{{\cal Y} \sim \pop}\;\sum_n \left(\frac{1}{Z_n} \sum_{y'} e^{s({\cal Y}[n=y'])} \; \nabla_\Phi\;s({\cal Y}[n]=y')\right) - \nabla_\Phi s({\cal Y}) \\
\\
& = & E_{{\cal Y} \sim \pop}\;\sum_n \left(\sum_{y'} P({\cal Y}[n=y'\;|\;{\cal Y}[N(n)]]) \; \nabla_\Phi\;s({\cal Y}[n=y'])\right) - \nabla_\Phi s({\cal Y})
\end{eqnarray*}
}

\slideplain{Gibbs CD1 Theorem}

{\huge
\begin{eqnarray*}
\nabla_\Phi\;{\cal L}_{\mathrm{PL}} & = & E_{{\cal Y} \sim \pop}\;\sum_n \left(\sum_{y'} P({\cal Y}[n=y'\;|\;{\cal Y}[N(n)]]) \nabla_\Phi\;s({\cal Y}[n]=y')\right) - \nabla_\Phi s({\cal Y}) \\
\\
& = & E_{{\cal Y} \sim \pop}\;\sum_n \left(E_{y' \sim P({\cal Y}[n=y'\;|\;{\cal Y}[N(n)]])} \nabla_\Phi\;s({\cal Y}[n]=y')\right) - \nabla_\Phi s({\cal Y}) \\
\\
& \propto & E_{{\cal Y} \sim \pop}\;E_n\; E_{y' \sim P({\cal Y}[n=y'\;|\;{\cal Y}[N(n)]])}\;\; (\nabla_\Phi\;s({\cal Y}[n]=y') - \nabla_\Phi s({\cal Y})) \\
\\
& = & E_{{\cal Y} \sim \pop}\;E_n\; E_{y' \sim P({\cal Y}[n=y'\;|\;{\cal Y}[N(n)]])}\;\; \nabla_\Phi\;{\cal L}_{\mbox{Gibbs CD(1)}}
\end{eqnarray*}
}

\slide{END}
}
\end{document}
