\input /Users/davidmcallester/Icloude/tex/SlidePreamble
\input /Users/davidmcallester/Icloude/tex/preamble

\begin{document}

{\Huge

  \centerline{\bf TTIC 31230, Fundamentals of Deep Learning}
  \bigskip
  \centerline{David McAllester, Winter 2020}
  \vfill
  \centerline{\bf Machine Translation and Attention}

\slide{Review of Auto-regressive RNN Language Modeling}

\centerline{\includegraphics[width=3.5in]{\images/RNN}}
\centerline{{\large [Christopher Olah]}}

\vfill
A typical RNN neural language model has the form

$$P_\Phi(w_t\;|\;w_0,\cdots,w_{t-1}) = \softmax_{w_t} e[w_t,J]\vec{h}[t-1,J]$$

\slide{Training an Auto-regressive RNN Language Model}

At train time the full sentence is given and the loss function is given by

\begin{eqnarray*}
-\ln P(w_0,\ldots,w_T) & = & \sum_t -\ln P(w_t\;|\;w_0,\ldots,w_{t-1}) \\
\\
P_\Phi(w_t\;|\;w_0,\cdots,w_{t-1}) & = & \softmax_{w_t} e[w_t,J]h[t-1,J]
\end{eqnarray*}

\slide{Sampling from an Auto-regressive RNN Language Model}

Draw $w_0$ from $P_\Phi(w_0)$,

$t = 0$

While $w_t \not = \mathrm{<EOS>}$

\begin{quotation}
Draw word $w_{t+1}$ from $$~\;\;\;\;\;P_\Phi(w_{t+1}\;|\;w_0,\cdots,w_t) = \softmax_{w_t} e[w_t,J]\vec{h}[t-1,J]$$

Compute $\vec{h}[t,J]$ from $\vec{h}[t-1,J]$ and $e[w_t,J]$.

\vspace{1ex}
increment $t$
\end{quotation}

\slide{Machine Translation}

$$w_0,\ldots,w_{T_{\mathrm{in}}} \Rightarrow \tilde{w}_0,\ldots,\tilde{w}_{T_{\mathrm{out}}}$$

\vfill
Translation is a {\bf sequence to sequence} (seq2seq) task.

\vfill
{\bf Sequence to Sequence Learning with Neural Networks}, Sutskever, Vinyals and Le, NeurIPS 2014, arXiv Sept 10, 2014.

\slide{Machine Translation}

\vfill
We define a model

\vfill
$$P_\Phi\left(\tilde{w}_0,\ldots,\tilde{w}_{T_{\mathrm{out}}}\;|\; w_0,\ldots,w_{T_{\mathrm{in}}}\right)$$

\vfill
\begin{eqnarray*}
\Phi^*  & = & \argmin_\Phi\; E_{\mathrm{Pop}} \;\left[-\ln\;P_\Phi\left(\tilde{w}_0,\ldots,\tilde{w}_{T_{\mathrm{out}}}\;|\; w_0,\ldots,w_{T_{\mathrm{in}}}\right)\right] \\
\\
& = & \argmin_\Phi \; E_{\tuple{x,y} \sim \mathrm{Pop}} \; \left[-\ln P_\Phi(y|x)\right]
\end{eqnarray*}

\slide{A Simple RNN Translation Model}

\vfill
The final state of a {\bf right-to-left (backward)} RNN, $\cev{h}_{\mathrm{in}}[0,J]$, is viewed as a {\bf ``thought vector''} representation of the input sentence.

\vfill
We use the input thought vector $\cev{h}_{\mathrm{in}}[0,J]$ as the initial hidden state of a {\bf left-to-right (forward)} RNN language model
generating the output sentence.

\vfill
Computing the input thought vector backward provides a good start to the forward generation of the output.

\slide{Machine Translation Decoding}

We can sample a translation

$$w_t \sim P(w_t\;|\;\cev{h}_{\mathrm{in}}[0,J],\;w_0,\ldots,w_{t-1})$$

\vfill
But typically we do a greedy decoding

$$w_t = \argmax_{w_t}\; P(w_t\;|\;\cev{h}_{\mathrm{in}}[0,J],\;w_0,\ldots,w_{t-1})$$

\slide{Machine Translation with Attention}

{\bf Neural Machine Translation by Jointly Learning to {\color{red} Align} and Translate}
Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio, ICLR 2015 (arXiv Sept. 1, 2014)

\vfill
We describe a slight simplification of the paper.

\slide{Representing Sentences by Vector Sequences}
We first run a bidirectional RNN on the
input sentence to get a {\color{red} sequence} $\dvec{h}_\mathrm{in}[T_{\mathrm{in}},J]$ of hidden vectors $\dvec{h}_\mathrm{in}[t_\mathrm{in},J]$ for $1 \leq t_\mathrm{in} \leq T_{\mathrm{in}}$.
We then replace

$$w_t \sim P(w_t\;|\;{\color{red} \cev{h}_{\mathrm{in}}[0,J]},\;w_0,\ldots,w_{t-1})$$

\vfill
by

$$w_t \sim P(w_t\;|\;{\color{red} \dvec{h}_\mathrm{in}[T_{\mathrm{in}},J]},\;\;w_0,\ldots,w_{t-1})$$

\vfill
The autoregressive probability model of the output is now conditioned on a sequence of vectors rather than a single thought vector.

\slide{Machine Translation with Attention}
{\huge
$$\begin{array}{rrcl}
        \mathrm{Autoregression:}    &    & &    P(w_{t_\mathrm{out}}\;|\;\dvec{h}_\mathrm{in}[T_{\mathrm{in}},J],\;w_0,\cdots,w_{{t_\mathrm{out}}-1}) \\
        \\
        \mathrm{Autoregression:} &   & = & \softmax_{w_{t_\mathrm{out}}}\;e[w_{t_\mathrm{out}},J]\;\vec{h}_\mathrm{out}[t_\mathrm{out}-1,J] \\
   \\
   \mathrm{RNN:} &\vec{h}_\mathrm{out}[0,J] & = & \cev{h}_\mathrm{in}[0,J/2];\vec{h}_\mathrm{in}[T_\mathrm{in},J/2] \\
   \\
  \mathrm{RNN:} & \vec{h}_\mathrm{out}[t_\mathrm{out},J] & = & \mathrm{CELL}(\vec{h}_\mathrm{out}[t_{\mathrm{out}}-1,J],\;e[w_{t_\mathrm{out}},I],\;{\color{red} \tilde{h}_\mathrm{in}[t_\mathrm{out},J]}) \\
  \\
  \mathrm{Attention:} & {\color{red} \tilde{h}_\mathrm{in}[t_\mathrm{out},J]} & = & {\color{red} \alpha[t_\mathrm{out},T_{\mathrm{in}}]}\;\dvec{h}_\mathrm{in}[T_{\mathrm{in}},J] \\
  \\
  \mathrm{Attention:} & {\color{red} \alpha[t_\mathrm{out},t_{\mathrm{in}}]} & =& \softmax_{t_\mathrm{in}} \;{\color{red} e[w_{t_\mathrm{out}},J]}\;\dvec{h}_\mathrm{in}[t_{\mathrm{in}},J]
  \end{array}$$
}

\slide{Attention}
\begin{eqnarray*}
  \alpha[t_\mathrm{out},t_{\mathrm{in}}]& =& \softmax_{t_\mathrm{in}} \;e[w_{t_\mathrm{out}},J]\;\dvec{h}_\mathrm{in}[t_{\mathrm{in}},J] \\
\\
\tilde{h}_\mathrm{in}[t_\mathrm{out},J]& = & \alpha[t_\mathrm{out},T_{\mathrm{in}}]\;\dvec{h}_\mathrm{in}[T_{\mathrm{in}},J]
\end{eqnarray*}

\vfill
$\tilde{h}_\mathrm{in}[t_\mathrm{out},J]$ is a convex combination of vectors $\dvec{h}_{\mathrm{in}}[t_{\mathrm{in}},J]$.

\vfill
More generally, attention computes a convex combination of vectors where the combination weights are computed by a softmax of an inner product with a ``query'' vector (such as $e[w_{t_{\mathrm{out}}},J]$ above).

\slide{Attention in Image Captioning}
We can treat image captioning as translating an image into a caption.

\vfill
In translation with attention involves an attention over the input aligning output words with positions in the input.

\vfill
For each output word we get an attention over the image positions.

\slide{Attention in Image Captioning}

\centerline{\includegraphics[height = 4.8in]{\images/AttentionInCaptioning1}}
\centerline{\Large Xu et al. ICML 2015}


\slide{Further Comments on Decoding}

We can sample a translation

$$w_t \sim P(w_t\;|\;\cev{h}_{\mathrm{in}}[0,J],\;w_0,\ldots,w_{t-1})$$

\vfill
Typically we do a greedy decoding

$$w_t = \argmax_{w_t}\; P(w_t\;|\;\cev{h}_{\mathrm{in}}[0,J],\;w_0,\ldots,w_{t-1})$$

\vfill
or we might try maximize total probability.

\begin{eqnarray*}
w_0,\ldots,w_{T_{\mathrm{out}}}
& = & \argmax_{w_0,\ldots,w_{T_{\mathrm{out}}}} \;P_\Phi\left(w_0,\ldots,w_{T_{\mathrm{out}}} \;|\; \cev{h}_{\mathrm{in}}[0,J]\right)
\end{eqnarray*}

\slideplain{Greedy Decoding vs. Beam Search}

We would like

\vfill
$$W_{\mathrm{out}}[T_{\mathrm{out}}]^* = \argmax_{W_{\mathrm{out}}[T_{\mathrm{out}}]}
P_\Phi(W_{\mathrm{out}}[T_{\mathrm{out}}] \;|\;W_{\mathrm{in}}[T_{\mathrm{in}}])$$

\vfill
But a greedy algorithm may do well

\vfill
$$w_t = \argmax_{w_t}\; P_\Phi(w_t\;|\;W_{\mathrm{in}}[T_{\mathrm{in}}],\;w_0,\ldots,w_{t-1})$$

\vfill
But these are not the same.

\slide{Example}

``Those apples are good'' vs. ``Apples are good''

\vfill
$$P_\Phi(\mbox{Apples are Good {\tt <eos>}}) > P_\Phi(\mbox{Those apples are good {\tt <eos>}})$$

\vfill
$$P_\Phi(\mbox{Those}|\varepsilon) > P_\Phi(\mbox{Apples}|\varepsilon)$$
    
\slide{Beam Search}

At each time step we maintain a list the $K$ best words and their associated hidden vectors.

\vfill
This can be used to produce a list of $k$ ``best'' decodings which can then be compared to select
the most likely one.

\slide{END}
}
\end{document}
