\input /users/davidmcallester/ICloude/tex/SlidePreamble
\input /users/davidmcallester/ICloude/tex/preamble

\newcommand{\solution}[1]{\bigskip {\bf Solution}: #1}

\begin{document}

{\Huge
  \centerline{\bf TTIC 31230, Fundamentals of Deep Learning}
  \bigskip
  \centerline{David McAllester, Autumn 2020}
  \vfill
  \centerline{\bf SGD with Momentum}
  \vfill
  \vfill

\slideplain{Momentum}

The standard (PyTorch) momentum SGD equations are

\begin{eqnarray*}
  {\color{red} v_t} & {\color{red} =} & {\color{red} \mu v_{t-1} + \eta * \hat{g}_t} \;\;\;\mbox{$\mu$ is typically .9 or .99}\\
  \\
  {\color{red} \Phi_{t+1}} & {\color{red} =} & {\color{red} \Phi_t -  v_t} \\
\end{eqnarray*}

\vfill
Here $v$ is velocity, $0 \leq \mu < 1$ represents friction drag and $\eta \hat{g}$ is the acceleration generated by the gradient force.

\slide{Momentum}

The theory of momentum is generally given in terms of second order structure and total gradients (GD rather than SGD).

\vfill
But second order analyses are controversial for SDG in very large dimension.

\vfill
Still, momentum is widely used in practice.

\slide{Momentum and Temperature}

\begin{eqnarray*}
  {\color{red} v_t} & {\color{red} =} & {\color{red} \mu v_{t-1} + \eta * \hat{g}_t} \;\;\;\mbox{$\mu$ is typically .9 or .99}\\
  \\
  {\color{red} \Phi_{t+1}} & {\color{red} =} & {\color{red} \Phi_t -  v_t} \\
\end{eqnarray*}

\vfill
We will use a first order analysis to argue that by setting

{\color{red} $$\eta = (1-\mu)B\eta_0$$}

\vfill
the temperature will be essentially determined by $\eta_0$ independent of the choice of the momentum parameter $\mu$ or the batch size $B$.

\slide{Momentum and Temperature}

{\color{red} $$\eta = (1-\mu)B\eta_0$$}

\vfill
Emprical evidence for this setting of $\eta$ is given in

\vfill
{\bf Don't Decay the Learning Rate, Increase the Batch Size}, Smith et al., 2018

\slide{Momentum as an Exponential Moving Average (EMA)}
Consider a sequence $x_1$, $x_2$, $x_3$, $\ldots$.
\vfill
For $t \geq N$, consider the average of the $N$ most recent values.
$$\overline{x}_t = \frac{1}{N} \;\; \sum_{k = 0}^{N-1}\; x_{t-k}$$

\vfill
This can be approximated efficiently with
\begin{eqnarray*}
\tilde{x}_0 & = & 0 \\
\\
\tilde{x}_t & = & \left(1-\frac{1}{N}\right)\tilde{x}_{t-1} + \left(\frac{1}{N}\right)x_t
\end{eqnarray*}

\slide{Deep Learning Convention for EMAs}

In deep learning an exponential moving average

$$\tilde{x}_t = \left(1-\frac{1}{N}\right)\tilde{x}_{t-1} + \left(\frac{1}{N}\right)x_t$$

\vfill
is written as

$$\tilde{x}_t = \beta\tilde{x}_{t-1} + (1-\beta)x_t$$

\vfill
where
$$\beta = 1 - 1/N$$

\vfill
Typical values for $\beta$ are .9, .99 or .999 corresponding to $N$ being 10, 100 or 1000.

\vfill
It will be convenient here to use $N$ rather than $\beta$.

\slide{Momentum as an EMA}

\begin{eqnarray*}
v_t & = & \mu v_{t-1} + {\color{red} \eta \hat{g}_t} \\
\\
& = & \left(1-\frac{1}{N}\right) v_{t-1} + {\color{red} \frac{1}{N}(N\eta \hat{g}_t)}
\end{eqnarray*}

\vfill
We see that $v_t$ is an EMA of $N \eta \hat{g}$.

\slide{Momentum as an EMA}

\centerline{\color{red} $v_t$ is an EMA of $N \eta \hat{g}$.}

\vfill
Alternatively, we can consider an EMA of the gradient.
{\color{red} $$\tilde{g}_t = \left(1-\frac{1}{N}\right)\tilde{g}_{t-1} + \left(\frac{1}{N}\right) \hat{g}_t$$}

\vfill
The moving average of $N\eta\hat{g}$ is the same as $N\eta$ times the moving average of $\hat{g}$.  Hence

\vfill
{\color{red} $$v_t = N \eta \tilde{g}_t$$}

\slide{Momentum as an EMA}

We have now shown that the standard formulation of momentum can be written as

\vfill
\begin{eqnarray*}
\tilde{g}_t & = & \left(1-\frac{1}{N}\right)\tilde{g}_{t-1} + \left(\frac{1}{N}\right) \hat{g}_t \\
\\
\\
\\
\Phi_{t+1} & = &  \Phi_t - N\eta\tilde{g}_t
\end{eqnarray*}

\slide{Total Effect Rule}

We will adopt the rule of thumb that the temperature is determined by the total effect of a single training gradient $g_{t,b}$.


\vfill
Also that ``temperature'' corresponds to the converged loss at fixed learning rate.

\slide{Total Effect Rule}

The effect of $g_{t,b}$ on the batch average $\hat{g}_t$ is $\left(\frac{1}{B}\right)g_{t,b}$.

\vfill
$$\mathrm{Using}\;\;\;\;\; \sum_{i = 0}^\infty \;\frac{1}{N}\left(1 - \frac{1}{N}\right)^i = 1$$

\vfill
we get that the effect of $\hat{g}_t$ on $\sum_{t=0}^\infty \tilde{g}_t$ equals $\hat{g}_t$.

\vfill
So for $\Phi_{t+1} =  \Phi_t - N\eta\tilde{g}_t$ the total effect of $g_{t,b}$ is $(N/B)\eta\; g_{t,b}$.


\slide{Total Effect Rule}

For $\Phi_{t+1} =  \Phi_t - N\eta\tilde{g}_t$ the total effect of $g_{t,b}$ is $(N/B)\eta\; g_{t,b}$.

\vfill
By taking $\eta = \frac{B}{N} \eta_0$ we get that the total effect, and hence the temperature, is determined by $\eta_0$ independent of the choice of $N$ and $B$.

\vfill
For the standard momentum paramenter $\mu = (1 - 1/N)$ this becomes

\vfill
{\color{red} $$\eta = (1-\mu)B \eta_0$$}

\vfill
where $\eta_0$ determines temperature independent of $\mu$ and $B$.

\slide{END}

} \end{document}

