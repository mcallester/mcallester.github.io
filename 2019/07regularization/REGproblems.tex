\documentclass{article}
\input ../preamble
\parindent = 0em

%\newcommand{\solution}[1]{}
\newcommand{\solution}[1]{\bigskip {\color{red} {\bf Solution}: #1}}

\begin{document}

\centerline{\bf TTIC 31230 Fundamentals of Deep Learning}
\bigskip
\centerline{\bf Regularization and  Generalization Problems}

\bigskip
\bigskip

{\bf Problem 1. The Stationary Points for $L_2$ Regularization.} Consider the regularized objective
$$\Phi^* = \argmin_\Phi \;E_{(x,y) \sim \mathrm{Train}}\;\left({\cal L}(\Phi,x,y) + \frac{1}{2N_{\mathrm{train}}\sigma^2}||\Phi||^2\right)$$
By setting the gradient of the objective to zero, solve for $\Phi$ as a function of the average gradient $g$ defined by
$$g = E_{\tuple{x,y}\sim \mathrm{Train}} \nabla \Phi {\cal L}(\Phi,x,y).$$

\solution{
  \begin{eqnarray*}
    & & \nabla_\Phi E_{(x,y) \sim \mathrm{Train}}\;{\cal L}(\Phi,x,y) + \frac{1}{2N_{\mathrm{train}}\sigma^2}||\Phi||^2 \\
    \\
    & = & \left(E_{(x,y) \sim \mathrm{Train}}\;\nabla_\Phi {\cal L}(\Phi,x,y)\right) + \frac{1}{N_{\mathrm{train}}\sigma^2}\Phi \\
    \\
    & = & g + \frac{1}{N_{\mathrm{train}}\sigma^2}\Phi \;\;= 0 \\
    \\
    \Phi & = & N_{\mathrm{train}}\sigma^2g
  \end{eqnarray*}

  \medskip
  Note that a larger sample size justifies having a larger norm for the parameter vector.
}

\medskip
{\bf PAC-Bayes Background for the problems 2 through 5.} Consider any probability distribution $P(h)$ over a discrete class ${\cal H}$.
Assume $0 \leq {\cal L}(h,x,y) \leq \lmax$. Define
\begin{eqnarray*}
{\cal L}(h)  & = &  E_{(x,y)\sim \mathrm{Pop}}\;{\cal L}(h,x,y) \\
\\
\hat{{\cal L}}(h) & = & E_{(x,y)\sim \mathrm{Train}}\;{\cal L}(h,x,y)
\end{eqnarray*}
We now have the theorem that with probability
at least $1-\delta$ over the draw of training data the following holds simultaneously for all $h$.
$${\cal L}(h) \leq \frac{10}{9}\parens{\hat{\cal L}(h) + \frac{5 L_\mathrm{max}}{N}\parens{\ln \frac{1}{P(h)} + \ln\frac{1}{\delta}}} \;\;\;(1)$$

This motivates
$$h^* = \argmin_h \;\hat{{\cal L}}(h) + \frac{5\lmax}{N_{\mathrm{train}}} \ln\frac{1}{P(h)}\;\;\;\;(2)$$
The Bayesian maximum a-posteriori (MAP) rule is
$$h^* = \argmax_h\;P(h)\prod_{(x,y) \in \mathrm{Train}}\;P(y|x,h)\;\;\;\;(3)$$

\medskip
{\bf Problem 2. The Meaning of a PAC-Bayes Prior.}  Consider an optimal hypothysis for the population distribution.
$$h^* = \argmin_h\;E_{\tuple{x,y} \sim \pop}\;{\cal L}(h,x,y)$$
Equation (1) holds for any prior $P$.  Consider two priors $P_{\mathrm{lucky}}$ and $P_{\mathrm{unlucky}}$ where we have
$$P_{\mathrm{lucky}}(h^*) >> P_{\mathrm{unlucky}}(h^*)$$
Explain how equation (1) can hold for both of these priors.

\solution{
  The prior $P$ should be interpreted as saying which hyopthesis will be measured accurately first as $N_{\mathrm{train}}$ increases.
  We can interpret $P$ as a ``guess'' as to where we think the good hypotheses are.
  The prior $P$ is not stating any actual propability of where the optimal hypothesis is.
  We get accurate measurements first
  for the hypotheses $h$ for which $P(h)$ is large.
  For $P_{\mathrm{unlucky}}$ we get an accurate measureent of ${\cal L}(h^*)$ only much later than we do under $P_{\mathrm{lucky}}$.
}

\medskip
{\bf Problem 3. Code Length as Probability.}
Assume that a model $h$ is represented by a (compressed) file $|h|$ bits long.  Files have a specific length and no file is a proper prefix of any other file.
We say that the set of file bit strings is {\bf prefix free}.

\medskip
(a) Show that for any prefix-free representation of files as bit strings we have
the following Kraft inequality where the sum is over all possible files (of unbounded size).
$$\sum_h\;2^{-|h|} \leq 1$$

\solution{Consider a probabilistic process which flips an unbiased coin to determine a next bit until it generates a legal file bit string at which point
  it outputs that file.  This process generates file $h$ with probability $2^{-|h|}$ and, by the prefix-free property, all files can be generated by this process.
  However, it is possible that this process never terminates 
  The Kraft inequality then follows from $\sum_h P(h) \leq 1$ where we also have $P(\mathrm{divergence}) + \sum_h\;P(h) = 1$.
}

\medskip
(b) rewrite (1) in terms of $|h|$ where we take $P(h) = 2^{-|h|}$.

\solution{
  $${\cal L}(h) \leq \frac{10}{9}\parens{\hat{\cal L}(h) + \frac{5 L_\mathrm{max}}{N_{\mathrm{train}}}\parens{(\ln 2)|h| + \ln\frac{1}{\delta}}} \;\;\;(1)$$
}

\medskip
{\bf Problem 4. Comparing Bayesian MAP to PAC-Bayes}
For ${\cal L}(h,x,y) = - \ln P(y|x,h)$ (cross entropy loss) rewrite (2) so as to be as similar to (3) as possible.
Note that (1) holds independent of any ``truth'' of the ``prior'' $P$.

\solution{
  \begin{eqnarray*}
    & & \argmin_h \;\left(\frac{1}{N} \sum_{(x,y)\sim \mathrm{Train}}\;-\ln P(y|x,h)\right) + \frac{5\lmax}{N} \ln\frac{1}{P(h)} \\
    \\
    & =  & \argmax_h \;\left(\frac{1}{N} \sum_{(x,y)\sim \mathrm{Train}}\;\ln P(y|x,h)\right) + \frac{5\lmax}{N} \ln P(h) \\
    \\
    & =  & \argmax_h \left(\sum_{(x,y)\sim \mathrm{Train}}\;\ln P(y|x,h)\right) + 5\lmax \ln P(h) \\
    \\
    & =  & \argmax_h \ln \left(P(h)^{5\lmax}\prod_{(x,y)\sim \mathrm{Train}}\; P(y|x,h)\right) \\
    \\
    & = & \argmax_h\;  P(h)^{5\lmax}\prod_{(x,y)\sim \mathrm{Train}}\; P(y|x,h)
  \end{eqnarray*}
}

\bigskip
{\bf Problem 5. Finite Precision Parameters.}

\medskip
(a) Consider a model where the parameter vector $\Phi$ has $d$ parameters each of which is represented by a 16 bit floating point number.
Express the bound (1) in terms of the dimension $d$ assuming all parameter vectors are equally likely.

\solution{
  $${\cal L}(h) \leq \frac{10}{9}\parens{\hat{\cal L}(h) + \frac{5 L_\mathrm{max}}{N}\parens{16d\ln 2 + \ln\frac{1}{\delta}}}$$
}

\medskip
(b) Assume a variable precision representation of numbers where $\Phi[i]$ is given with $|\Phi[i]|$ bits.  Express the bound (1) as a function of $\Phi$ assuming that $P(\Phi)$
is defined so that each parameter is selected independently and that
$$P(\Phi[i]) = 2^{-|\Phi[i]|}$$.

\solution{
  \begin{eqnarray*}
    {\cal L}(h) & \leq & \frac{10}{9}\parens{\hat{\cal L}(h) + \frac{5 L_\mathrm{max}}{N_{\mathrm{train}}}\parens{\ln 2|\Phi| + \ln\frac{1}{\delta}}} \\
    \\
    |\Phi| & = & \sum_i |\Phi[i]|
  \end{eqnarray*}
}

\medskip
(c) Repeat part (a) but for a model with $d$ parameters represented by
$\Phi_i = z[J[i]]$ where $J[i]$ is an integer index with $0 \leq J[i] < k$ and where $z[j]$ is a $b$ bit floating point number and where all parameter vectors are equally likely.

\solution{
  $${\cal L}(h) \leq \frac{10}{9}\parens{\hat{\cal L}(h) + \frac{5 L_\mathrm{max}}{N}\parens{kb\ln 2 + d\ln k + \ln\frac{1}{\delta}}}$$
  Since $d$ is large this is typically much tighter bound than using floating point or even integer representationfs of parameters.  It is a much more compact representaiton
  of the parameters.
}

\medskip
~{\bf Problem 6. Implicit Bias for SGD on Least Squares Regression.}  Consider a hypothesis space ${\cal H}$ and a learning algorithm ${\cal A}$
that maps trainging data to a hyothesis in ${\cal H}$.  Write ${\cal A}(\mathrm{Train})$ for the result of running algorithm ${\cal A}$ on training data $\mathrm{Train}$.
Also consider a given population distribution $\pop$ where $\mathrm{Train}$ consists of $N_{\mathrm{train}}$ samples drawn independently from $\pop$.
Let $P_{{\cal A},\pop}(h)$ be the probability that ${\cal A}(\mathrm{Train}) = h$
when $\mathrm{Train}$ is drawn at random from $\pop$.  The propbability distribution $P_{{\cal A},\pop}$ is independent of any particular training sample
and can be used as a PAC-Bayes prior on ${\cal H}$. A PAC-Bayes prior represents a learning bias.
The distribution $P_{{\cal A},\pop}$ is the {\bf implicit bias} of algorithm ${\cal A}$ run on population $\pop$.

\medskip
In this problem we consider the implicit bias of the SGD algorithm applied to least squares regression in the case where there are many more parameters than data points.
Least squares regression is defined by
$$\Phi[J]^* = \argmin_\Phi \; E_{\tuple{x,y} \sim \mathrm{Train}}\;(\Phi[J]x[J] - y)^2$$
To solve this optimization problem we consider using SGD where $\Phi$ is initialized to the zero vector and we then apply the update
\begin{eqnarray*}
  \Phi_{t+1} & = & \Phi_t - \eta \nabla_\Phi\;(\Phi^\top x_y - y)^2 \\
  \\
  & = & \Phi_t - 2 \eta (\Phi^\top x_t - y)x_t
\end{eqnarray*}

\medskip
(a) In the case where $N_{\mathrm{train}} < d$, where $d$ is the dimension of $\Phi$ and $x$, define a linear proper subspace of $R^d$ such that we are guaranteed
that $\Phi_t$ is in that space for al $t$.

\solution{
  Since every update is in the direction of some input vector $x_t$ in the training data, SGD maintains the invariant that $\Phi_t$ is some linear combination
  of the training vectors $x_1,\ldots x_{N_{\mathrm{train}}}$. Since $N_{\mathrm{train}} < d$ the span of the training vectors must be a proper subspace of $R^d$.
}

(b) Assume that the training vectors $x_1,\ldots,x_{N_{\mathrm{train}}}$ are linearly independent.  In this case it can be shown that there exists a unique solution $\Phi^*$ in the space
spanned by these vectors for which the square loss of the training data is zero (if these were not independent then we would have more training points than degrees of freedom
in the space spanned by the input vectors). Let $b_1,\ldots,b_{N_{\mathrm{train}}}$ be an orthonormal basis
for the space spanned by the input vectors. For any $\Phi \in R^d$ define the projection of $\Phi$ into the subspace by
\begin{eqnarray*}
  \Phi_\pi & = & \sum_i\;(\Phi^\top b_i)b_i \\
  \\
  \Phi_\bot & = & \Phi - \Phi_\pi
\end{eqnarray*}

The convergence theorem for SGD now gives that SGD on least squares regression will converge in the limit to $\Phi^*$.  Show that SGD applied to least
squared regression has a form of implicit bias similar to $L_2$ regression in that the result $\Phi^*$ is the least norm point in $R^d$ for which
the square loss of the training data is zero.

\solution{
  Consider any $\Phi \in R^d$ for which the training loss is zero.  The projection $\Phi_\pi$ must also have zero training loss because
  each training vector can be written as a linear combination of basis vectors and $\Phi$ and $\Phi_\pi$ have the same inner product with each basis vector.
  Therefore $\Phi_\pi = \Phi^*$.  Futhermore $\Phi = \Phi_\pi + \Phi_\bot$ and
  $$||\Phi||^2 = ||\Phi_\pi||^2 + ||\Phi_\bot||^2 = ||\Phi^*||^2 + ||\Phi_\bot||^2$$
  which gives that $||\Phi|| \geq ||\Phi^*||$ as desired.
}


\bigskip
{\bf Problem 7. Generalization Bounds for the realizable case. (25 points)}

Consider a finite hyothesis class ${\cal H}$ and a population distribution $\pop$ on pairs $\tuple{x,y}$ such that for $\tuple{x,y}$ drawn from the
population and $h \in {\cal H}$ we have that $h$ makes a prediction for $y$ which we will write as $h(x)$.  The error rate of hypothesis $h$ on the population is defined by
$$\mathrm{Err}_{\pop}(h) = P_{\tuple{x,y} \sim \pop}(h(x) \not = y)$$
We draw a training sample $\mathrm{Train}$ consisting of $N_{\mathrm{Train}}$ pairs $\tuple{x,y}$ drawn IID from the population.
$$\mathrm{Err}_{\mathrm{train}}(h) = \frac{1}{N_{\mathrm{train}}} \sum_{\tuple{x,y} \in \mathrm{Train}}\;\bbone(h(x) \not = y)$$

\medskip
(a) For a given hypothesis $h$ with error rate $\epsilon$ what is the probability that $\mathrm{Err}_{\mathrm{train}}(h) = 0$.

\solution{
  $(1-\epsilon)^{N_{\mathrm{train}}}$
}

\medskip
(b) We now consider a fixed threshold $\epsilon$ and consider the hypotheses $h$ satisfying $\mathrm{Err}_{\pop} \geq \epsilon$.
We will call these the ``bad'' hypotheses. 

\medskip
The simple form of the union bound is
$$P(A \cup B) \leq P(A) + P(B)$$
This can be generalized to
$$P(\exists z\; Q(z)) \leq \sum_z P(Q(z))$$
where $Q(z)$ is any statement about $z$.  

\medskip
Use your answer to (a) and the union bound to give an upper bound on the probability that 
there exists a bad hypothesis $h$ with $\mathrm{Err}_{\mathrm{train}}(h) = 0$.
You solution should be stated in terms of $\epsilon$, the number of elements $|{\cal H}|$ of ${\cal H}$,
and the number of training pairs $N_{\mathrm{train}}$.
Simplify your solution using the inequality $1-\epsilon \leq e^{-\epsilon}$.

\solution{
  \begin{eqnarray*}
    & & P(\exists h\; \;\mathrm{Err}_{\pop} \geq \epsilon,\; \mathrm{Err}_{\mathrm{train}}(h) = 0) \\
    \\
    & \leq & \sum_{h:\;\mathrm{Err}_{\pop}(h) \geq \epsilon}\;P(\mathrm{Err}_{\mathrm{train}}(h) = 0) \\
    \\
    & \leq & |{\cal H}| (1-\epsilon)^{N_{\mathrm{train}}} \\
    \\
    & \leq & |{\cal H}| e^{-N_{\mathrm{train}}\epsilon} \\
  \end{eqnarray*}
}

\medskip
(c) Now consider a small positive number $\delta$ and solve for $\epsilon$ such that the probability that a bad hypothesis has zero training error is less than $\delta$.
Your solution gives a value of $\epsilon$  such that with probability $1-\delta$ over the draw of the training error all hypothesis with zero training error
have population error no lareger than $\epsilon$.

\solution{
  \begin{eqnarray*}
    \delta & = & |{\cal H}| e^{-N_{\mathrm{train}}\epsilon} \\
    \\
    \epsilon & = & \frac{\ln |{\cal H}| + \ln \frac{1}{\delta}}{N_{\mathrm{train}}}
  \end{eqnarray*}
  }


\end{document}
