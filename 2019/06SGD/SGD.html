<header>TTIC 31230: Fundamentals of Deep Learning Winter 2020</header>

<p> Stochastic Gradient Descent (SGD) </p>

<p><a href = SGD.pdf> Slides on Basic Algorithms and Hyperparameter Decoupling</a></p>

<p><a href = SGDproblems.pdf> Problems</a></p>

<p>Additional Material:</p>

<p><a href = flow.pdf> Slides on Gradient Flow and Langevin Dynamics</a></p>

<p><a href = SGDproblems.pdf> Langevin Dynamics Probles</a></p>

<p><a href = safe.pdf> Slides on a Quenching Algorithm</a></p>

<p><a href = http://ruder.io/optimizing-gradient-descent/> Blog post on SGD variants</a></p>

<p><a href = https://arxiv.org/abs/1706.02677> Training Resnt-50 on Imagenet in one hour</a></p>

<p><a href = https://openreview.net/pdf?id=B1Yy1BxCZ>Paper on batch size scaling of the learning rate and momentum parameter</a><p>

<p><a href = https://arxiv.org/abs/1511.06807> Adding Gradient Noise</a></p>

<p><a href = https://arxiv.org/abs/1704.00109> Temperature Cycling in SGD </a></p>

<p><a href = https://arxiv.org/abs/1206.1901> MCMC with momentum</a></p>






  
