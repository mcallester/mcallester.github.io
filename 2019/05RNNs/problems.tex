\documentclass{article}
\input ../preamble
\parindent = 0em

%\newcommand{\solution}[1]{}
\newcommand{\solution}[1]{\bigskip {\color{red} {\bf Solution}: #1}}

\begin{document}


\centerline{\bf TTIC 31230 Fundamentals of Deep Learning, 2020}
\medskip
\centerline{\bf Problems For Language Modeling, Translation and Attention.}

\bigskip

In these problems, as in the lecture notes, capital letter indeces are used to indicate subtensors (slices) so that, for example,  $M[I,J]$ denotes a matrix
while $M[i,j]$ denotes one element of the matrix, $M[i,J]$ denotes the $i$th row, and $M[I,j]$ denotes the $j$th collumn.

\medskip
Throughout these problems we assume a word embedding matrix $e[W,I]$ where $e[w,I]$ is the word vector for word $w$. We then have that $e[w,I]^\top h[t,I]$
is the inner product of the word vector $w[w,I]$ and the hidden state vector $h[t,I]$.

\medskip
We will adopt the convention, similar to true Einstein notation, that repeated capital indeces in a product of tensors are implicitly summed.  We can then write
the inner product $e[w,I]^\top h[t,I]$ simply as $e[w,I]h[t,I]$ without the need for the (meaningless) transpose operation.

\medskip
~{\bf Problem 1. RNN parallel run time.}  Consider an autoregressive RNN neural language model with $P_\Phi(w_{t+1}|w_1,\ldots,w_t)$ defined by
$$P_\Phi(w_t| w_1,\ldots,w_{t-1}) = \softmax_{w_{t+1}}\;\;e[w_t,I]h[t-1,I]$$
Here $e[w,I]$ is the word vector for word $w$, $h[t,I]$ is the hidden state vector at time $t$ of a left-to-right RNN, and as described above $e[w,I]h[t,I]$
is the inner prodcut of these two vectors where we have assumed that they have the same dimension.
For the first word $w_1$ we have an externally provided initial hidden state $h[0,I]$ and $w_1,\ldots,w_0$ denotes the empty string.
We train the model on the full loss
\begin{eqnarray*}
  \Phi^* &  = & \argmin_\Phi E_{w_1,\ldots,w_T \sim \mathrm{Train}}\;-\ln P_\Phi(w_1,\ldots,w_T) \\
  \\
  & = & \argmin_\Phi E_{w_1,\ldots,w_T \sim \mathrm{Train}}\;\sum_{t=1}^T\;-\ln P_\Phi(w_t|w_1,\ldots,w_{t-1})
\end{eqnarray*}

\medskip
What is the order of run time as a function of sentence length $T$ for the backpropagation for this model
run on a sentence $w_1,\ldots,w_T$?  Explain your answer.

\solution{
  The backprogation takes $O(T)$ time (not $O(T^2)$). The model consists of $O(T)$ objects each of which performs a single forward operation and a single backward operation.
  As the backpropagation procedes more of the loss terms in the sum over $t$ get incorporated.
}

\bigskip
~{\bf Problem 2. Translating diagrams into equations.} A UGRNN cell for computing $h[b,t,J]$ from $h[b,t-1,J]$ and $x[b,t,J]$ can be written as

\begin{eqnarray*}
G[b,t,j] & = & \sigma\left(W^{h,G}[j,I] h[b,t\!-\!1,I] +  W^{x,G}[j,K] x[b,t,K] - B^G[j]\right) \\
\\
R[b,t,j] & = & \mathrm{tanh}\left(W^{h,R}[j,I] h[b,t\!-\!1,I] + W^{x,R}[j,K] x[b,t,K] - B^R[j]\right) \\
\\
h[b,t,j] & = & G[b,t,j]h[b,t\!-\!1,j] + (1-G[b,t,j])R[b,t,j]
\end{eqnarray*}


Modify the above equations so that they correspond to the following diagram for a Gated Recurent Unit (GRU).

\centerline{\includegraphics[width=2.0in]{../images/GRU}}
\centerline{{\small [Christopher Olah]}}

\solution{

  \begin{eqnarray*}
    G_1[b,t,j] & = & \sigma\left(W^{h,G_1}[j,I] h[b,t\!-\!1,I] +  W^{x,G_1}[j,K] x[b,t,K] - B^{G_1}[j]\right) \\
    \\
    h'[b,t,j] & = & G_1[b,t,j]h[b,t\!-\!1,j] \\
    \\
    G_2[b,t,j] & = & \sigma\left(W^{h,G_2}[j,I] h[b,t\!-\!1,I] +  W^{x,G_2}[j,K] x[b,t,K] - B^{G_2}[j]\right) \\
    \\
    R[b,t,j] & = & \mathrm{tanh}\left(W^{h,R}[j,I] h'[b,t\!-\!1,I] + W^{x,R}[j,K] x[b,t,K] - B^R[j]\right) \\
    \\
    h[b,t,j] & = & (1-G_2[b,t,j])h[b,t\!-\!1,j] + G_2[b,t,j]R[b,t,j]
  \end{eqnarray*}
}

\bigskip
\bigskip
~{\bf Problem 3. Blank Language modeling.} This problem considers ``blank language modeling'' which is used in BERT.
For blank language modelng we draw a sentence $w_1,\ldots,w_T$ from a corpose and blank out a word at random
and ask the system to predict the blanked word.  The cross-entropy loss for blank language modeling can be written as
$$\Phi^* = \argmin_\Phi\;E_{w_1,\ldots,w_T \sim \mathrm{Train}, t \sim \{1,\ldots,T\}}\;\;- \ln P_\Phi(w_t| w_1,\ldots,w_{t-1},w_{t+1},\ldots,w_T)$$
    
Consider a bidirectional RNN run on a sequence of words $w_1,\ldots,w_T$ such that for each time $t$ we have a forward
hidden state $\vec{h}[t,J]$ computed from $w_1,\ldots,w_t$ and a backward hidden state $\cev{h}[t,J]$ computed from $w_T,\;w_{T-1},\ldots w_t$.
Also assume that each word $w$ has an associated word vector $e[w,J]$.
Give a definition of $P(w_t\;|\;w_1,\ldots w_{t-1},\;w_{t+1},\ldots,w_T)$ as a function of
the vectors $\vec{h}[t-1,J]$ and $\cev{h}[t+1,J]$ and the word vectors $e[W,I]$.
You can assume that $\vec{h}[T,J]$ and $\cev[h][T,J]$ have the same shape (same dimensions) but
do not make any assumptions about the dimension of the word vectors $e[W,I]$.
You can assume whatever tensor parameters you want.

\solution{
  There are various acceptable solutions.  A simple one is to assume the parameters include matrices  $\vec{W}[I,J]$ and $\cev{W}[I,J]$.
  Using this convention and the standard convention for matrix-vector products we can then write a solution as
  \begin{eqnarray*}
    & & P_\Phi(w_t| w_1,\ldots,w_{t-1},w_{t+1},\ldots,w_T) \\
    & = & \softmax_{w_t} \;\;e[w_t,I]\vec{W}[I,J]\vec{h}[t-1,J]\; + \; e[w_t,I]\cev{W}[I,J]\cev{h}[t+1,J]
  \end{eqnarray*}
}

~ {\bf Problem 4. Image Captioning as Translation with Attention.} In machine translation with attention the translation is generated from an autoregressive language model for the target langauge translation given the source language sentence.
The source sentence is converted to an initial hidden vector $h[0,J]$ for the decoding (usually the final hidden vector of a right-to-left RNN run on the input), plus the sequence
$M[T_{\mathrm{in}},J]$ of hidden vectors computed by the RNN on the source sentence where $T$ is the length of the source sentence.  We then define an autoregressive conditional language model
$$P_\Phi(w_1,\ldots,w_{T_\mathrm{out}}\;|\; h[0,J],\;M[T_{\mathrm{in}},J])$$
An autoregressive conditional language model with attention can be defined by
\begin{eqnarray*}
P(w_t\;|\;w_0,\cdots,w_{t-1}) & = & \softmax_{w_t}\;e[w_t,I]W^{\mathrm{auto}}[I,J]h[t-1,J] \\
\\
\alpha[t_{\mathrm{in}}]& =& \softmax_{t_\mathrm{in}} \;h[t\!-\!1,J_1]W^{\mathrm{key}}[J_1,J_2]M[t_{\mathrm{in}},J_2]\\
\\
V[J]& = & \sum_{t_{\mathrm{in}}}\;\alpha[t_{\mathrm{in}}]M[t_{\mathrm{in}},J] \\
\\
h[t,J] & = & \mathrm{CELL}_\Phi (h[t-1,J],\;V[J],\;e[w_t,I])
\end{eqnarray*}
Here $\mathrm{CELL}$ is some function taking (objects for) two vectors of dimensions J and one vector of dimension I and returning (an object for) a vector of dimension $J$.

\medskip
Rewrite these equations for image captioning where instead of $M[t_{\mathrm{in}},J]$ we are given an image feature tensor $M[x,y,K]$

\solution{
\begin{eqnarray*}
P(w_t\;|\;w_0,\cdots,w_{t-1}) & = & \softmax_{w_t}\;e[w_t,I]W^{\mathrm{auto}}[I,J]h[t-1,J] \\
\\
\alpha[x,y]& =& \softmax_{x,y} \;h[t\!-\!1,J]W^{\mathrm{key}}[J,K]M[x,y,K]\\
\\
V[K]& = & \sum_{x,y}\;\alpha[x,y]M[x,y,K] \\
\\
h[t,J] & = & \mathrm{CELL}_\Phi (h[t-1,J],\;V[K],\;e[w_t,I])
\end{eqnarray*}
}

\medskip
{\bf Problem 5. Gated CNNs}

A UGRNN is defined by the following equations.

\begin{eqnarray*}
\tilde{R}_t[b,j] & = & \left(\sum_i\;W^{h,R}[j,i]{\color{red} h_{t-1}[b,i]}\right) + \left(\sum_k W^{x,R}[j,k]{\color{red} x_t[b,k]}\right) - B^R[j] \\
\\
R_t[b,j] & = & \mathrm{\tanh}(\tilde{R}_t[b,j]) \\
\\
\tilde{G}_t[b,j] & = & \left(\sum_i\;W^{h,G}[j,i]{\color{red} h_{t-1}[b,i]}\right) + \left(\sum_k W^{x,G}[j,k]{\color{red} x_t[b,k]}\right) - B^G[j] \\
\\
G_t[b,j] & = & \mathrm{\sigma}(\tilde{G}_t[b,j]) \\
\\
{\color{red} h_t[b,j]} & = & G_t[b,j]{\color{red} h_{t-1}[b,j]} + (1-G_t[b,j])R_t[b,j]
\end{eqnarray*}

Modify these to form a data-dependent data-flow CNN for vision --- an Update-Gate CNN (UGCNN).
More specifically, give equations analogous to those for UGRNN for computing a CNN ``box'' $L_{\ell+1}[b,x,y,j]$ from $L_\ell[b,x,y,i]$ (stride 1) using a computed ``gate box'' $G_{\ell+1}[b,x,y,j]$ and an ``update box'' $R_{\ell+1}[b,x,y,j]$.
$$\Phi = (W^{L,R}_{\ell+1}[\Delta x, \Delta y,j,j'], B^R_{\ell+1}[j], W^{L,G}_{\ell+1}[\Delta x, \Delta y,j,j'],\;B^G_{\ell+1}[j])$$

\solution{
  \begin{eqnarray*}
    R_{\ell+1}[b,x,y,j] & = & \mathrm{tanh}\left(\left(\sum_{\Delta x,\Delta y,j'}\;W^{L,R}_{\ell+1}[\Delta x, \Delta y, j',j]
    L_{\ell}[b,x+\Delta x,\;y+\Delta y,\;j']\right) - B^R_{\ell+1}[j]\right) \\
    \\
    G_{\ell+1}[b,x,y,j] & = & \sigma\left(\left(\sum_{\Delta x,\Delta y,i}\;W^{L,G}_{\ell+1}[\Delta x, \Delta y, i,j]
    L_{\ell}[b,x+\Delta x,\;y+\Delta y,\;i]\right) - B^G_{\ell+1}[j]\right) \\
    \\
    L_{\ell+1}[b,x,y,j] & = & G_{\ell+1}[b,x,y,j]L_\ell[b,x,y,j] + (1-G_t[b,x,y,j])R_t[b,x,y,j]
  \end{eqnarray*}
}

{\bf Problem 6. Language CNNs.}
This problem is on CNNs for sentences.  We consider a model with parameters
$$\Phi = (e[w,i], W_1[\Delta t,i,i'], B_1[i],\ldots,W_L[\Delta t,i,i'],B_L[i])$$
The matrix $e$ is the word embedding matrix where $e[w,I]$ is the vector embedding of word $w$.

\medskip
(a) Give an equation for the convolution layer $L_0[b,t,i]$ as a function of the word embeddings and the input sentence $w_1,\ldots,w_T$.

\solution{
  $$L_0[b,t,i] = e[w[b,t],i]$$}

\medskip
(b) Give an equation for $L_{\ell+1}[b,t,i]$ as a function of $L_\ell[b,t,i]$ and the parameters $W_{\ell+1}[\Delta t,i',i]$ and $B_{\ell+1}[i]$
and where $L_{\ell+1}$ is computed stride 2.

\solution{
$$L_{\ell+1}[b,t,i] = \sigma\left(\left(\sum_{\Delta t,i'}\;W_{\ell+1}[\Delta t,i',i]L_\ell[2t + \Delta t,i']\right) - B_{\ell+1}[i]\right)$$
}
  
\medskip
(c) Assuming all computations can be done in parallel as soon the inputs have been computed, what is the {\bf parallel} order
of run time for this convolutional model as a function of the input length $T$ and the number of layers $L$ (assume all parameter tensors
of size $O(1)$). Compare this with the parallel run time of an RNN.

\solution{The CNN has $O(L)$ parallel run time while the RNN is $O(T)$ or $O(T+L)$ with $L$ layers of RNN.}

\bigskip
\bigskip
~ {\bf Problem 7.} A self-attention layer in the transformer takes a sequence of vectors $h_\mathrm{in}[T,J]$ and computes
a sequence of vectors $h_\mathrm{out}[T,J]$ using the following equations where $k$ ranges over ``heads''. Heads are intended to allow for
different relationship between words such as ``coreference'' or ``subject of'' for a verb. But the actual meaning emerges during training
and is typically difficult or impossible to interpret.  In the following equations we typically hve $U < J$ and we require $I = J/K$
so that the concatenation of $K$ vectors of dimension $I$ is a vector of dimension $J$.

\begin{eqnarray*}
\mathrm{Query}[k,t,U] & = & W^Q[k,U,J]h_\mathrm{in}[t,J] \\
\\
\mathrm{Key}[k,t,U] & = &  W^K[k,U,J]h_\mathrm{in}[t,J] \\
\\
\alpha[k,t_1,t_2] & = & \softmax_{t_2}\; \mathrm{Query}[k,t_1,U]\mathrm{Key}[k,t_2,U]\\
\\
\mathrm{Value}[k,t,I] & = & W^V[k,I,J]h_\mathrm{in}[t,J] \\
\\
\mathrm{Out}[k,t,I] & = & \sum_{t'}\alpha[k,t,t']\mathrm{Value}[k,t',I] \\
\\
h_\mathrm{out}[t,J] & = & \mathrm{Out}[1,t,I];\cdots;\mathrm{Out}[K,t,I]
\end{eqnarray*}

A summation over $N$ terms can be done in parallel in $O(\log N)$ time.

\medskip
(a) For a given head $k$ and position $t_1$ what is the parallel running time of the above softmax operation, as a function of $T$ and $U$
where we first compute the scores to be used in the softmax and then compute the normalizing constant $Z$.

\solution{
  The scores can be computed in parallel in $\ln U$ time and then $Z$ can be computed in $\ln T$ time.  We then get $O(\ln T + \ln U)$.
  In practice the inner product used in computing the scores would be done in $O(U)$ time giving $O(U + \ln T)$.
}

\medskip
(b) What is the order of running time of the self-attention layer as a function of $T$, $J$ and $K$ (we have $I$ and $U$ are both less than $J$.)

\solution{$O(\ln T + \ln J)$.  In practice the inner products would be done serially which would give $O(J + \ln T)$.}


\bigskip

{\bf Problem 8.}
Just as CNNs can be done in two dimensions for vision and in one dimension for language, the Transformer can be done in two dimensions for vision --- the so-called spatial transformer.

\medskip
(a) Rewrite the equations from problem 1 so that the time index $t$ is replaced by spatial dimensions $x$ and $y$.

\solution{

\begin{eqnarray*}
\mathrm{Query}[k,x,y,U] & = & W^Q[k,U,J]h_\mathrm{in}[x,y,J] \\
\\
\mathrm{Key}[k,x,y,U] & = &  W^K[k,U,J]h_\mathrm{in}[x,y,J] \\
\\
\alpha[k,x_1,y_1,x_2,y_2] & = & \softmax_{x_2,y_2}\; \mathrm{Query}[k,x_1,y_1,U]\mathrm{Key}[k,x_2,y_2,U]\\
\\
\mathrm{Value}[k,x,y,I] & = & W^V[k,I,J]h_\mathrm{in}[x,y,J] \\
\\
\mathrm{Out}[k,x,y,I] & = & \sum_{x',y'}\alpha[k,x,y,x',y']\mathrm{Value}[k,x',y',I] \\
\\
h_\mathrm{out}[x,y,J] & = & \mathrm{Out}[1,x,y,I];\cdots;\mathrm{Out}[K,x,y,I]
\end{eqnarray*}
}

\medskip
(b) Assuming that summations take logarithmic parallel time, give the parallel order of run time for the spatial self-attention layer as a function
of $X$, $Y$, $J$ and $K$ (we have that $I$ and $U$ are both less than $J$).

\solution{$O(\ln XY + \ln J)$}

\end{document}
