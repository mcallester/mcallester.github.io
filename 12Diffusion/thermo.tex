\input ../SlidePreamble
\input ../preamble


\begin{document}

{\Huge

  \centerline{\bf TTIC 31230, Fundamentals of Deep Learning}
  \bigskip
  \centerline{David McAllester, Autumn 2022}
  \vfill
  \vfil
  \centerline{The Thermodynamic Interpretation of Diffusion Models}
  \vfill
  \centerline{Why are they called ``diffusion'' models?}
  \vfill
  \vfill
  \centerline{\bf Generative Modeling by Estimating Gradients ...}
\centerline{\bf Song and Erman, July 2019}

 
\slide{Langevin Dynamics}

Consider a model density defined by a continuous softmax on a model score.

\vfill
\begin{eqnarray*}
  p_\score(y) & = & \softmax_y\; \score(y) \\
  \\
  & = & \frac{1}{Z} \;e^{\score(y)} \\
  \\
  Z & = & \int e^{\score(y)}\;dy
\end{eqnarray*}

\vfill
Here $\score(y)$ is a parameterized model computing a score and defining a probability density on $R^d$.

\slide{Langevin Dynamics}

If $y$ is discrete, but from an exponentially large space (such as sentences or a semantic image segmentation) we can use MCMC sampling
(the Metropolis algorithm or Gibbs sampling).

\vfill
In the continuous case we can use Langevin dynamics.

\slide{Langevin Dynamics}

Noisy gradient ascent on score.
\begin{eqnarray*}
  y(t+ \Delta t) & = & y(t) + \eta g \Delta t +  \sigma \epsilon \sqrt{\Delta t} \\
\\
g & = & \nabla_y\;\score(y) \\
\\
\epsilon & \sim & {\cal N}(0,I)
\end{eqnarray*}

\vfill
This give a well-defined distribution on functions of time in the limit as $\Delta t \rightarrow 0$.

{\color{red} $$dy =  \eta g dt + \sigma \epsilon \sqrt{dt}\;\;\;\;\;\; \epsilon \sim {\cal N}(0,I)$$}

\slide{Langevin Dynamics}

$$dy =  \eta g dt + \sigma \epsilon \sqrt{dt}\;\;\;\;\;\; \epsilon \sim {\cal N}(0,I)$$

\vfill
This has stationary (equilibrium) density.

\vfill
The derivation is mathematically identical to the derivation of the stationary distribution of SGD at a learning rate $\eta$ and noise covariance $\Sigma$.

\vfill
However, here we have isotropic noise rather than arbitrary gradient noise.

\vfill
Isotropic noise always yields a Gibbs distribution.

\vfill
Imposing isotropic noise is called Langevin dynamics.


\slide{The Stationary Density}

To derive the stationary density we consider a gradient flow and a {\bf diffusion flow} as a function of density $p(y)$.

\vfill
The gradient flow is $\eta p(y) \nabla_y \score(y)$ and the diffusion flow is $\frac{1}{2}\eta\sigma^2 \nabla_y p(y)$

\vfill
Setting them to be opposite and solving the resulting differential equation gives

\vfill
$$p(y)  = \frac{1}{Z}\;e^{\frac{2\score(y)}{\eta\sigma^2}}$$


\slide{The Stationary Density}

$$p(y)  = \frac{1}{Z}\;e^{\frac{2\score(y)}{\eta\sigma^2}}$$

\vfill
Setting $\eta = 1$ and $\sigma^2 = 2$ gives

\vfill
$$p(y)  = \frac{1}{Z}\;e^{\score(y)} \;\;= \;\;\softmax_y\;\score(y)$$

\vfill
Running Langevin dynamics long enough will yield a sample from the softmax distribution.

\slide{Score Matching}

In score matching we train $g(y)$ rather than $\score(y)$ so as to make $g(y) \approx \nabla_y \;\score(y)$

\vfill
The training objective for the decoder of a diffusion model can be viewed as training an update direction $g$
to approximate $\nabla_y \ln p(y)$.

\vfill
{\bf The score matching interpretation identifies the diffusion model decoding vector $\epsilon(z)$ with $- \nabla_z\;\ln p(z)$}

\vfill
{\bf Warning:} The term ``score'' in score matching technically refers to the gradient vector $\nabla_y\; \score(y)$ rather than to the scalar ``score'' used in the softmax.

\slide{Simulated Annealing}

In simulated annealing one tries to avoid local optima by first running at a high temperature and then then gradually reducing the temperature.

\vfill
In the diffusion model $\sigma_\ell$ increases with increasing $\ell$ which is claimed to be an analogy with simulated annealing.

\vfill
However, simulated annealing corresponds to adding noise {\bf in sampling} rather than adding noise to a population sample.

\slide{Score Matching vs. VAE}

The VAE interpretation of diffusion models does not rely on Langevin dynamics, score matching or simulated annealing.

\vfill
However, the score matching interpretation, which identifies $\epsilon(z_\ell,\ell)$ with $- \nabla_z\; p(z)$,
plays a role in ``classifier conditioned guidance'' used in DALLE-2.

\slide{The DDPM Stochastic Differential Equation (SDE)}

Consider a DDPM (denoising diffision probabilistic model) for modeling $P(y)$ with $y \in R^d$ where the noise model is defined by

\begin{eqnarray*}
  z_0 & = & y \\
  \\
  z_\ell & = & \alpha z_{\ell-1} + \sqrt{1-\alpha^2}\; \epsilon\;\;\;\epsilon\sim {\cal N}(0,I)
\end{eqnarray*}

\vfill
For technical simplicity we take $\alpha$ to be constant for all $\ell$ and allow $\ell \geq 1$ to be arbitrarily large.

\slide{The DDPM SDE}

For sampling $z_\ell$ given $z_0$ the unit variance constraint gives

\vfill
$$z_\ell = \alpha^\ell z_0 + \sqrt{1 - \alpha^{2\ell}}\;\epsilon \;\;\;\epsilon \sim {\cal N}(0,I)$$

\vfill
For sampling $z_{(\ell+k)}$ given $z_\ell$ we have

\vfill
$$z_{(\ell + k)} = \alpha^k z_\ell + \sqrt{1 - \alpha^{2k}}\;\epsilon \;\;\;\epsilon \sim {\cal N}(0,I)$$

\slide{The DDPM SDE}

\medskip
Setting $\alpha = e^{\frac{-1}{N}}$ we have


\begin{eqnarray*}
  z_\ell & = & e^{\frac{-\ell}{N}} z_0 + \sqrt{1 - e^{\frac{-2\ell}{N}}}\;\epsilon \;\;\;\epsilon \sim {\cal N}(0,I) \\
  \\
  z_{(\ell + k)|\ell} & = & e^{\frac{-k}{N}} z_\ell + \sqrt{1 - e^{\frac{-2k}{N}}}\;\epsilon \;\;\;\epsilon \sim {\cal N}(0,I)
\end{eqnarray*}

\slide{The DDPM SDE}

\medskip
Taking $t = \frac{\ell}{N}$. We have $\ell = Nt$ and the previous slide can be written as

\begin{eqnarray*}
  z(t) & = & e^{-t} z(0) + \sqrt{1 - e^{-2t}}\;\epsilon \;\;\;\epsilon \sim {\cal N}(0,I) \\
  \\
  z(t+\Delta t) & = & e^{- \Delta t} z(t) + \sqrt{1 - e^{-2\Delta t}}\;\epsilon \;\;\;\epsilon \sim {\cal N}(0,I)
\end{eqnarray*}

\slide{The DDPM SDE}
For small $\epsilon$ we have $e^{-\epsilon} \approx 1-\epsilon$ and for small $\Delta t$ the previous slide can be written as

\vfill
  \begin{eqnarray*}
    z((t + \Delta t)|t) & \approx & z(t) - z(t)\Delta t + \sqrt{2\Delta t} \;\epsilon \\
    \\
    \Delta z & \approx & -z \Delta t + \sqrt{\Delta t} \;\delta \;\;\;\delta \sim {\cal N}(0,2I)
    \end{eqnarray*}    

\vfill
This can be interpreted as the stochastic differential equation for the forward process (the encoder) for diffusion models.

\slide{END}
}
\end{document}
