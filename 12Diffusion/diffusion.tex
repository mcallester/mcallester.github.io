\input ../SlidePreamble
\input ../preamble


\begin{document}

{\Huge

  \centerline{\bf TTIC 31230, Fundamentals of Deep Learning}
  \bigskip
  \centerline{David McAllester, Autumn 2023}
  \vfill
  \vfil
  \centerline{The Mathematics of Diffusion Models}
  \vfill
  \centerline{McAllester, arXiv 2023}
    \vfill
  \vfill

\slidetwo{Denoising Diffusion Probabilistic Models (DDPM)}{Ho, Jain and Abbeel, June 2020}

\centerline{\includegraphics[width = 4in]{\images/DiffCeleb}}

\slide{The Diffusion SDE}
\centerline{\includegraphics[width = 2in]{\images/DiffSequence}}

Consider a discrete time process $z(0),z(\Delta t),z(2\Delta t),z(3\Delta t),\ldots$
{\huge
\begin{eqnarray*}
  z(0) & = & y,\;\;\;y \sim \pop(y) \\
  \\
  z(t+\Delta t) & = & z(t) + \sqrt{\Delta t}\; \epsilon,\;\;\;\epsilon \sim {\cal N}(0,I)
\end{eqnarray*}
}
A sum of two Gaussians is a Gaussian whose {\bf variance} is the sum of the two variances.
$$z(t+n\Delta t) = z(t) + \sqrt{n\Delta t}\; \epsilon,\;\;\;\epsilon \sim {\cal N}(0,I)$$
Here $\sqrt{n \Delta t}$ is the {\bf standard deviation} of the added noise.


\slide{The Diffusion SDE}
The stochastic differential equation is the limit as the discrete step size $\Delta t$ goes to zero.

{\huge
\begin{eqnarray}
  z(0) & = & y,\;\;\;y \sim \pop(y) \nonumber \\
  \nonumber \\
  z(t+\Delta t) & = & z(t) + \sqrt{\Delta t}\; \epsilon,\;\;\;\epsilon \sim {\cal N}(0,I) \label{forward}
\end{eqnarray}
}

In this limit equation (\ref{forward}) holds for all $t$ and $\Delta t$.

\slide{The Markovian ELBO}

For a Markovian VAE we have
{\Large
\begin{eqnarray}
  - \ln \pop(y) & = & - \ln\frac{P_\enc(z_N)P_\enc(z_{N-1}|z_N)\cdots P_\enc(z_1|z_2)P_\enc(y|z_1)}{P_\enc(z_N|y)P_\enc(z_{N-1}|z_N,y)\cdots P_\enc(z_1|z_2,y)} \nonumber\\
  \nonumber \\
  & = & \left\{\begin{array}{l} KL(P_\enc(z_N|y),\;P_\enc(z_N)) \\ \\ + \sum_{i=2}^N  \; E_\enc\; KL(P_\enc(z_{i-1}|z_i,y),\;P_\enc(z_{i-1}|z_i)) \\ \\ - E_\enc\;[\ln P_\enc(y|z_1)] \end{array}\right. \label{Markov-eq} \\
  \nonumber\\
  H(y) & \leq & \left\{\begin{array}{l} E_\enc\;KL(P_\enc(z_N|y),\;p_\gen(z_N)) \\ \\ + \sum_{i=2}^N  \; E_\enc\; KL(P_\enc(z_{i-1}|z_i,y),\;p_\gen(z_{i-1}|z_i)) \\ \\ - \ln p_\gen(y|z_1) \end{array}\right. \;\;\;~\label{Markov-ELBO}
\end{eqnarray}
}Here we focus on (\ref{Markov-eq}) rather than (\ref{Markov-ELBO}).  We will first derive exact expressions rather than inequalities.


\slide{Exact Equalities}

{\huge
\begin{eqnarray*}
  - \ln \pop(y) & = & \left\{\begin{array}{l} KL(P_\enc(z_N|y),\;P_\enc(z_N)) \\ \\ + \sum_{i=2}^N  \; E_\enc\; KL(P_\enc(z_{i-1}|z_i,y),\;P_\enc(z_{i-1}|z_i)) \\ \\ - E_\enc\;[\ln P_\enc(y|z_1)] \end{array}\right.
  \end{eqnarray*}
}

\slide{Reverse-Time Probabilities}

In the limit of small $\Delta t$ it is possible to derive the following.

\vfill
\begin{eqnarray*}
  p(z(t - \Delta t)|z(t),y) & = & {\cal N}\left(\begin{array}{l}z(t) + \frac{\Delta t(y - z(t))}{t},\;\; \Delta t I\end{array}\right) \\
  \\
  \\
  p(z(t - \Delta t)|z(t)) & = & {\cal N}\left(\begin{array}{l} z(t) + \frac{\Delta t(E[y|t,z(t)] - z(t))}{t}, \;\; \Delta t I\end{array}\right)
\end{eqnarray*}


\slide{The Reverse-Diffusion SDE}

$$p(z(t - \Delta t)|z(t)) = {\cal N}\left(\begin{array}{l} z(t) + \frac{\Delta t(E[y|t,z(t)] - z(t))}{t}, \Delta t I\end{array}\right)$$

\vfill
This equation defines a reverse-diffusion SDE which we can write as

{\huge
$$z(t - \Delta t) = z(t) + \left(\frac{E[y|t,z(t)] - z(t)}{t}\right)\Delta t + \sqrt{\Delta t} \epsilon,\;\;\epsilon \sim {\cal N}(0,I)$$
}

\slide{KL-Divergence}
{\huge
\begin{eqnarray*}
  - \ln \pop(y) & = & \left\{\begin{array}{l} KL(P_\enc(z_N|y),\;P_\enc(z_N)) \\ \\ + \sum_{i=2}^N  \; E_\enc\; KL(P_\enc(z_{i-1}|z_i,y),\;P_\enc(z_{i-1}|z_i)) \\ \\ - E_\enc\;[\ln P_\enc(y|z_1)] \end{array}\right.
  \end{eqnarray*}

\vfill
For two Gaussian distributions with the same isotropic covariance we have

\vfill
$$KL\left(\begin{array}{l}{\cal N}(\mu_1,\sigma^2 I), \\ {\cal N}(\mu_2,\sigma^2 I) \end{array}\right) = \frac{||u_1 -\mu_2||^2}{2\sigma^2}$$
}

\slide{KL-Divergence}
\begin{eqnarray*}
  - \ln \pop(y) & = & \left\{\begin{array}{l} KL(P_\enc(z_N|y),\;P_\enc(z_N)) \\ \\ + \sum_{i=2}^N  \; E_\enc\; KL(P_\enc(z_{i-1}|z_i,y),\;P_\enc(z_{i-1}|z_i)) \\ \\ - E_\enc\;[\ln P_\enc(y|z_1)] \end{array}\right.
\end{eqnarray*}

\vfill
\begin{eqnarray*}
  p(z(t - \Delta t)|z(t),y) & = & {\cal N}\left(\begin{array}{l}z(t) + \frac{\Delta t(y - z(t))}{t},\;\; \Delta t I\end{array}\right) \\
  \\
  \\
  p(z(t - \Delta t)|z(t)) & = & {\cal N}\left(\begin{array}{l} z(t) + \frac{\Delta t(E[y|t,z(t)] - z(t))}{t}, \;\; \Delta t I\end{array}\right)
\end{eqnarray*}

\slide{KL-Divergences}
{\huge
\begin{eqnarray*}
  p(z(t - \Delta t)|z(t),y) & = & {\cal N}\left(\begin{array}{l}z(t) + \frac{\Delta t(y - z(t))}{t},\;\; \Delta t I\end{array}\right) \\
  \\
  \\
  p(z(t - \Delta t)|z(t)) & = & {\cal N}\left(\begin{array}{l} z(t) + \frac{\Delta t(E[y|t,z(t)] - z(t))}{t}, \;\; \Delta t I\end{array}\right)
\end{eqnarray*}

\vfill
\begin{eqnarray*}
  KL\left(\begin{array}{l}p(z(t-\Delta t)|z(t),y), \\p(z(t-\Delta  t)|z(t))\end{array}\right)
  & = & \left(\frac{||y-E[y|t,z(t)]||^2\Delta t^2}{2t^2\Delta t}\right) \\
  \\
  \\
  & =  & \left(\frac{||y-E[y|t,z(t)]||^2}{2t^2}\right) \Delta t
\end{eqnarray*}
}

\slide{KL-Divergences}
\begin{eqnarray*}
  - \ln \pop(y) & = & \left\{\begin{array}{l} KL(P_\enc(z_N|y),\;P_\enc(z_N)) \\ \\ + \sum_{i=2}^N  \; E_\enc\; KL(P_\enc(z_{i-1}|z_i,y),\;P_\enc(z_{i-1}|z_i)) \\ \\ - E_\enc\;[\ln P_\enc(y|z_1)] \end{array}\right. \\
  \\
  \\
  & = & \sum_{i = 2}^N  \left(\frac{||y-E[y|t,z(t)]||^2}{2t^2}\right) \Delta t + E_\enc\;[- \ln P_\enc(y|z_1)] \\
  & & \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;t = i\Delta t
\end{eqnarray*}

\slide{Passing to the Integral}

\begin{eqnarray*}
- \ln \popd(y) & = & \left\{\begin{array}{l}\int_{t_0}^\infty dt \;\; E_{z(t)|y}\;\left[\frac{||y - E[y|t,z(t)]||^2}{2t^2}\right] \\ \\ + \;E_{z(t_0)|y}[-\ln p(y|z(t_0))]\end{array}\right. \\
\\
\\
  H(y) & = & \left\{\begin{array}{l}\int_{t_0}^\infty dt \;\; E_{y,z(t_0)}\;\left[\frac{||y - E[y|t,z(t)]||^2}{2t^2}\right] \\ \\ +\;H(y|z(t_0))\end{array}\right.
\end{eqnarray*}

\slide{Mutual Information}

{\huge
\begin{eqnarray*}
  H(y) & = & \left\{\begin{array}{l}\int_{t_0}^\infty dt \;\; E_{y,z(t_0)}\;\left[\frac{||y - E[y|t,z(t)]||^2}{2t^2}\right] \\ \\ +\;H(y|z(t_0))\end{array}\right. \\
  \\
  \\
  H(y) - H(y|z(t_0)) & = & \int_{t_0}^\infty dt \;\; E_{y,z(t_0)}\;\left[\frac{||y - E[y|t,z(t)]||^2}{2t^2}\right] \\
  \\
  \\
  I(y,z(t_0)) & = & \int_{t_0}^\infty dt \;\; E_{y,z(t_0)}\;\left[\frac{||y - E[y|t,z(t)]||^2}{2t^2}\right] 
\end{eqnarray*}

\vfill
This is the information minimum mean squared error relation (I-MMSE) relation [Guo et al. 2005].
}

\slide{Estimating $E[y|t,z(t)]$}

$$z(t - \Delta t) = z(t) + \left(\frac{E[y|t,z(t)] - z(t)}{t}\right)\Delta t + \epsilon\sqrt{\Delta t},\;\;\epsilon \sim {\cal N}(0,I)$$

\vfill
We can train a denoising network $\hat{y}(t,z)$ to estimate $E[y|t,z(t)]$ using

\vfill
$$\hat{y}^* = \argmin_{\hat{y}} E_t \;E \;(\hat{y}(t,z) - y)^2$$

\vfill
Assuming universality

\vfill
$$\hat{y}^*(t,z) = E[y|t,z(t)]$$

\slide{Estimating $E[y|t,z(t)]$}
In practice it is better to train the denoising network on values of the same scale.

\vfill
If the population values are scaled so as to have scale 1, then the scale of $z(t)$ is $\sqrt{1+t}$.
$$\hat{y}^* = \argmin_{\hat{y}} E_{t,z(t)} \; (\hat{y}(t,z/\sqrt{1+t}) - y)^2$$

\vfill
We then have
$$E[y|t,z(t)] = \hat{y}^*(t,z/\sqrt{1+t}))$$

\slide{The Fokker-Plack Anaylysis (The Score Function)}

For $\epsilon \sim {\cal N}(0,I)$ a general SDE can be written as
\begin{eqnarray*}
z(t+\Delta t) & = & z(t) + \mu(z(t),t)\Delta t + \; \sigma(z(t),t)\epsilon\sqrt{\Delta t} \\
\\
dz & = & \mu(z(t),t)dt + \; \sigma(z(t),t)dB
\end{eqnarray*}

\vfill
The diffusion process is the special case of Brownian motion
\begin{eqnarray*}
z(t + \Delta t) & = & z(t) + \epsilon \sqrt{\Delta t} \\
dz & = & dB
\end{eqnarray*}

\slide{The Fokker-Planck Equation}

Let $P_t(z)$ be the probability that $z(t) = z$.

$$\frac{\partial P_t(z)}{\partial t} = - \nabla \cdot\left(\begin{array}{l}\mu(z(t),t)P_t(z) \\ \\ - \frac{1}{2}\sigma^2(z(t),t) \nabla_z P_t(z)\end{array}\right)$$

\vfill
For the special case of diffusion we have

$$\frac{\partial P_t(z)}{\partial t} = - \nabla \cdot\left(-\frac{1}{2}\nabla_z P_t(z)\right)$$

\slide{The Score Function}

$$\frac{\partial P_t(z)}{\partial t} = - \nabla \cdot\left(\begin{array}{l}\mu(z(t),t)P_t(z) \\ \\ - \frac{1}{2}\sigma^2(z(t),t) \nabla_z P_t(z)\end{array}\right)$$

\vfill
$$\frac{\partial P_t(z)}{\partial t} = - \nabla \cdot\left(-\frac{1}{2}\nabla_z P_t(z)\right)$$

\vfill
$$\frac{\partial P_t(z)}{\partial t} = - \nabla \cdot\left[\left(-\frac{1}{2}\nabla_z \ln P_t(z)\right)P_t(z)\right]$$

\slide{The Score Function}

$$\frac{\partial P_t(z)}{\partial t} = - \nabla \cdot\left[\left(-\frac{1}{2}\nabla_z \ln P_t(z)\right)P_t(z)\right]$$

\vfill
$\ln P_t(z)$ is the score function.

\vfill
The time evolution of $P_t(z)$ can be written as the result of {\bf deterministic} flow given by

$$\frac{dz}{dt} = - \frac{1}{2}\nabla_z \ln p_t(z)$$

\slide{Deterministic Denoising}

Following the deterministic flow backward in time samples from the population!

$$z(t-\Delta t) = z(t) + \frac{1}{2} \nabla_z \ln p_t(z) \Delta t$$

\vfill
No noise!

\slide{Solving for the Score Function}

{\Large
\begin{eqnarray*}
  P_t(z) & = & E_y \;P_t(z|y) \\
  \\
  & = & \;E_y\;\frac{1}{Z(t)} e^{-\frac{||z-y||^2}{2t}} \\
  \\
  \nabla_z P_t(z) & = & E_y \;P_t(z|y)\; (y-z)/t \\
  \\
 & \vdots & \\
  \\
  & = & P_t(z)\frac{E[y|t,z]-z}{t}
  \end{eqnarray*}
}

 {\color{red}
 $$\nabla_z \ln P_t(z)  = \frac{E[y|t,z]-z}{t}$$
 }

This is Tweedie's formula, Robbins 1956.

\slide{Stochastic vs. Deterministic Denoising}

\begin{eqnarray*}
z(t - \Delta t) & = & z(t) + \left(\frac{E[y|t,z(t)] - z(t)}{t}\right)\Delta t + \epsilon\sqrt{\Delta t} \\
\\
\\
z(t -\Delta t) & = & z(t) + \frac{1}{2}\left(\frac{E[y|t,z(t)] - z(t)}{t}\right)\Delta t
\end{eqnarray*}

\slide{Interpolating Stochastic and Deterministic}
One can show that for $\lambda \in [0,1]$ the following also samples from the population.

\begin{eqnarray*}
z(t -\Delta t) & = & z(t) + \frac{1+\lambda}{2}\left(\frac{E[y|t,z(t)] - z(t)}{t}\right)\Delta t + \lambda \epsilon\sqrt{\Delta t}
\end{eqnarray*}

\slide{END}
}
\end{document}

