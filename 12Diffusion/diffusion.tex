\input ../SlidePreamble
\input ../preamble


\begin{document}

{\Huge

  \centerline{\bf TTIC 31230, Fundamentals of Deep Learning}
  \bigskip
  \centerline{David McAllester, Autumn 2022}
  \vfill
  \vfil
  \centerline{Diffusion Models}
  \vfill
  \vfill

\slide{Progressive VAEs}

Diffusion models are a special case of progrssive VAEs.

\vfill
A progressive VAE for an observed valriable $y$ (such as an image) has layers of latent variables $z_1,\dots,z_{L}$.

\vfill
The encoder defines $P_\enc(z_1|y)$ and $P_\enc(z_{\ell+1}|z_\ell)$ (defining a Markov chain).

\vfill
In a diffusion model the encoder is held fixed (not trained) and defined in a way that guarantees that
the mutual ingformation $I(z_{\ell+1},y) < I(z_\ell,y)$ and $I(z_L,y) = 0$ with $P_\enc(z_L|y)$ being noise independent of $y$.

\slide{Progressive VAE Loss Function}

\vfill
The model has a prior $P_\pri(x_L)$.  In a diffusion model this prior analytically determined by the encoders and isnot trained.

decoder will $P_\dec(z_{\ell-1}|z_\ell)$ and $P_\dec(y|z_1)$.

\vfill
Following VQ-VAE, we will train the encoder and the decoder independent of any prior.

\vfill
We then train a prior on the top layer latent variable.  The top level prior and decoder allow us to sample $y$ from the model.

\slide{Phase One Training}

We train a encoders and decoders $\enc_1,\dec_1$, $\ldots$, $\enc_L,\dec_L$ where the distribution on $z_1,\ldots,Z_L$ is defined by $y$ and the encoder.

$$\enc_1^*,\dec_1^* =\argmin_{\enc_1,\dec_1}\;E_{y,z_1} \left[-\ln P_{\dec_1}(y|z_1)\right]$$

\vfill
$$\enc_{\ell+1}^*,\dec_{\ell+1}^* = \argmin_{\enc_{\ell+1},\dec_{\ell+1}}\;E_{z_\ell,z_{\ell+1}}\;\left[-\ln P_{\dec_{\ell+1}}(z_{\ell-1}|z_\ell)\right]$$

\vfill
If these encoders and decoders share parameters the shared parameters are influenced by all of the above training losses (this observation was added
after seeing DALLE-2's diffision model).

\slide{Phase Two Training}

$$\pri^*   =  \argmin_{\pri}\;E_{z_L}\left[-\ln P_\pri(z_L)\right]$$

\vfill
Because of the autonomy of the encoder, the universality assumption implies that we get a perfect model of the population distribution on $y$.

\vfill
Given the prior and the decoder we can sample images.

\slide{Modeling Densities on $R^d$}

Consider a model density $p_\Phi(y)$ on $y \in R^d$ (for example sound waves or images).

\vfill
Ideally we want to be able to compute $p_\Phi(y)$, the denisty for any given $y$, and to also sample $y$ from $p_\Phi(y)$.

\slide{Continuous Softmax}

We consider the case where a density is defined by a continuous softmax.

\vfill
\begin{eqnarray*}
  p_\score(y) & = & \softmax_y\; \score(y) \\
  \\
  & = & \frac{1}{Z} e^{\score(y)} \\
  \\
  Z & = & \int dy\;e^{\score(y)}
\end{eqnarray*}

\vfill
Here $\score(y)$ is a parameterized model computing a score and defining a probability density on $R^d$.

\slide{Langevin Dynamics --- MCMC for Continuous Densities} 

If $y$ is discrete, but from an exponentially large space (such as sentences or a semantic image segmentation) we can use MCMC sampling
(the Metropolis algorithm or Gibbs sampling).

\vfill
In the continuous case the analogue of MCMC sampling is Langevin dynamics.

\slide{Langevin Dynamics}

\begin{eqnarray*}
y(t + \Delta t) & = & y(t) + 2g\Delta t +  \epsilon \sqrt{\Delta t} \\
\\
g & = & \nabla_y\;\score(y) \\
\\
\epsilon & \sim & {\cal N}(0,I)
\end{eqnarray*}

\vfill
This give a well-defined distribution on functions of time in the limit as $\Delta t \rightarrow 0$.

{\color{red} $$dy =  2 g dt + \epsilon \sqrt{dt}\;\;\;\;\;\; \epsilon \sim {\cal N}(0,I)$$}

\slide{The Stationary Density}

The gradient flow is equal to $ 2 p(x) \;\nabla_y \;\score(y)$.

\vfill
The diffusion flow is  $- 2 \nabla _y\;p(y)$ (see the slides on SGD).

\vspace{-2ex}
\begin{eqnarray*}
\nabla_y\; p(y) & = & p(y)\; \nabla_y\; \score(y) \\
\\
\frac{dp}{p} & = & d\;\score \\
\\
\ln p & = & \score + C \\
\\
{\color{red} p(y)} & = & {\color{red} \frac{1}{Z}\;e^{\score(y)}} \;\;= \;\;{\color{red} \softmax_y\;\score(y)}
\end{eqnarray*}

\slide{The Stationary Density}

So in a limit where $\Delta t \rightarrow 0$ but $t \rightarrow \infty$ we have $p_t(y) = \softmax_y\;s(y)$.

\vfill
In theory this gives a sampling algorithm for $p(y) = \softmax_y \;\score(y)$.

\vfill
This is called ``score matching'' --- $Z$ remains unknown and we do not get any way of computing $Z$ or $p_\score(y)$.

\slide{END}
}
\end{document}
