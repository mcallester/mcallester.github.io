\input ../SlidePreamble
\input ../preamble

\begin{document}

{\Huge

  \centerline{\bf TTIC 31230, Fundamentals of Deep Learning}
  \bigskip
  \centerline{David McAllester, Autumn 2023}

    \vfill
  \centerline{\bf Eintein Notation}
  \vfill
  \centerline{\bf and Convolutional Neural Networks (CNNs)}
  \vfill

\slide{Einstein Notation}

For the representation of general relativity, Einstein introduced the convention of explicitly writing all indeces of tensors where repeated indeces in a product of tensors are implicitly summed.

\vfill
Writing indeces explicitly improves the clarity of the notation at the expense of not being in correspondence with framework notation.  Most frameworks hide indeces.

\vfill
This course will focus on conceptual understanding rather than framework implementations.  For conceptual understanding Einstein notation seems preferable.

\slide{Advantages of Einstein Notation}

The indeces of tensors generally have types such as ``batch index'', ``x coordinate'', ``y coordinate'' and  ``neuron index''.

\vfill
\centerline{\includegraphics[width = 5.0in]{\images/VGG}}

\vfill
A layer in a CNN has ``shape'' {\color{red} $L[b,x,y,n]$} where $b$ is a batch intex, $n$ is a neuron index, and $x$ and $y$ specify a spacial locotation.

\slide{Advantages of Einstein Notation}

A layer in a CNN has shape $L[b,x,y,n]$.

\vfill
Writing the indeces explicitily typically makes the meaning of indeces clear and avoids having to remember the somewhat arbitrary order of the indeces (the order matters for
efficiency of memory access --- discussed later).

\slide{Einstein Notation}

\vfill
We will use a modified form of Einstein notation where captial letters are used to denote slices of a tensor.  For example:

\vfill
\begin{itemize}

\item $L[b,x,y,n]$ denotes a single (scalar) number.
\vfill
\item $L[b,x,y,N]$ denotes a vector of neuron values.
\vfill
\item $L[b,X,Y,N]$ denotes the entire layer of the $b$th batch element.
\end{itemize}

\slide{Einstein Notation}

Frameworks (and NumPy) use C-order (row-major) in laying out a tensor in memory.

\vfill
The vector $L[b,x,y,N]$ denotes a contiguous block of memory.

\vfill
The $b$th batch layer $L[b,X,Y,N]$ also denotes a (larger) contiguous block of memory.

\vfill
Vector and matrix operations on contiguous memory better utilize the memory hierarchy (caching).

\slide{Einstein Notation}
Following Einstein I will use repeated capital letters in a product of tensors to denote summation over those letters.

\vfill
\begin{eqnarray*}
y = Wx &\;\;\;\equiv\;\;\; & y[i] = \sum_j \;W[i,j]x[j] \\
& \;\;\;\equiv \;\;\; & y[i] = W[i,J]x[J] \\
\\
\\
y = x^\top\;W & \;\;\;\equiv \;\;\; & y[j] = \sum_i \;W[i,j]x[i] \\
& \;\;\;\equiv \;\;\; & y[j] = W[I,j]x[I]
\end{eqnarray*}


\slide{Einstein Notation}
\vfill
For vectors $x$ and $y$ and matricies $A$, $B$, and $C$ we have

\begin{eqnarray*}
y = Ax &\;\;\;\equiv\;\;\; & y[i] = A[i,J]x[J] \\
\\
y = x^\top\;A & \;\;\;\equiv \;\;\; & y[j] = A[I,j]x[I] \\
\\
A = B^\top C & \;\;\; \equiv \;\;\; & A[i,j] = B[K,i]C[K,j]
\end{eqnarray*}

\vfill
In Einstein notation we never use transpose.

\slide{Convolution}
\vfill
\centerline{\includegraphics[width = 6.0in]{\images/VGG}}

\vfill
A layer is a tensor shape $L(b,x,y,n)$.

\vfill
I will refer to the index $n$ as a ``neuron''. The index $n$ is also called
a ``feature'' or ``channel''.

\slide{Convolution}
\vfill
\centerline{\includegraphics[width = 6.0in]{\images/VGG}}

\vfill
The input image (for each batch element) has three ``neurons'' (Red, Green, Blue).

\vfill
In VGG (above) the input image is $224\times224\times3$ (for each batch element).  The next two layers are $224\times224\times64$.

\slide{Convolution}
\vfill
\centerline{\includegraphics[width = 6.0in]{\images/VGG}}

A given neuron (index) $n$ occurs in every batch element and every position $x$, $y$ in the tensor $L(b,x,y,n)$.

\vfill
Each occurance of that neuron uses the same computation rule for compurting its response.

\slide{Convolution}
\vfill
\centerline{\includegraphics[width = 6.0in]{\images/VGG}}

A given neuron (index) $n$ occurs in every batch element and every position $x$, $y$ in the tensor $L(b,x,y,n)$.

\vfill
The ``receptive field'' of an occurance of neuron $n$ at position $x$, $y$ is a region in the previous layer centered at
$x$ and $y$.


\slide{A Convolution Layer}
\newcommand{\nin}{n_{\mathrm{in}}}
\newcommand{\nout}{n_{\mathrm{out}}}

\centerline{\includegraphics[width = 4.0in]{\images/Convolution}}
\centerline{$K_{\ell+1}[\nout,\Delta x,\Delta y,\nin]$\hspace{6ex}$L_{{\ell}}[b,x,y,\nin]$\hspace{6ex}$L_{{\ell+1}}[b,x,y,\nout]$}

\begin{eqnarray*}
 & &  L_{{\ell+1}}[b,x,y,\nout] \\
 \\
  & = &   \sigma\left(K_{\ell+1}[\nout,\Delta X, \Delta Y,N_{\mathrm{in}}]\; L_{{\ell}}[b,x + \Delta X, y + \Delta Y, N_{\mathrm{in}}] - B[\nout]\right)
\end{eqnarray*}

\slide{2D CNN in PyTorch}

conv2d({\bf input, weight, bias, stride, padding, dilation, groups})

\bigskip
{\bf input} – tensor (minibatch,in-channels,iH,iW)

\medskip
{\bf weight} – filters (out-channels, in-channels/groups,kH,kW)

\medskip
{\bf bias} – tensor (out-channels) . Default: None

\medskip
{\bf stride} – Single number or (sH, sW). Default: 1

\medskip
{\bf padding} – Single number or (padH, padW). Default: 0

\medskip
{\bf dilation} – Single number or (dH, dW). Default: 1

\medskip
{\bf groups} – split input into groups. Default: 1

\slide{Padding}

\centerline{\includegraphics[height = 2.0in]{\images/padding2}}
\centerline{\large Jonathan Hui}

\vfill
If we pad the input with zeros then the input and output can have the same spatial dimensions.

\slide{Zero Padding in NumPy}

In NumPy we can add a zero padding of width p to an image as follows:

\vfill
\begin{verbatim}
padded = np.zeros(W + 2*p,  H + 2*p)

padded[p:W+p, p:H+p] = x
\end{verbatim}

\slide{Padding}

$L'_{{\ell}} = \mathrm{Padd}(L_{{\ell}},\;p)$

\vfill
\begin{eqnarray*}
 & &  L_{{\ell+1}}[b,x,y,\nout] \\
 \\
  & = &   \sigma\left(K_{\ell+1}[\nout,\Delta X, \Delta Y,N_{\mathrm{in}}]\; L_{{\ell}}[b,x + \Delta X, y + \Delta Y, N_{\mathrm{in}}] - B[\nout]\right)
\end{eqnarray*}

\vfill
If the input is padded but the output is not padded then $\Delta x$ and $\Delta y$ are non-negative.

\slide{Reducing Spatial Dimention}

\centerline{\includegraphics[width = 8.0in]{\images/VGG}}

\slide{Reducing Spatial Dimensions: Strided Convolution}

We can move the filter by a ``stride'' $s$ for each spatial step.


\vfill
{\huge
\begin{eqnarray*}
 & &  L_{{\ell+1}}[b,{\color{red}x,y},\nout] \\
 \\
  & = &   \sigma\left(K_{\ell+1}[\nout,\Delta X, \Delta Y,N_{\mathrm{in}}]\; L_{{\ell}}[b,{\color{red} s*x} + \Delta X, {\color{red} s*y} + \Delta Y, N_{\mathrm{in}}] - B[\nout]\right)
\end{eqnarray*}
}

\vfill
For strides greater than 1 the spatial dimention is reduced.

\slide{Reducing Spatial Dimensions: Max Pooling}

\centerline{\includegraphics[width = 2.5in]{\images/Convolution}}

\vfill
$$L_{{\ell+1}}[b,{\color{red}x},{\color{red}y},i] = {\color{red} \max_{\Delta x, \Delta y}}\; L_{{\ell}}[b,{\color{red} s*x} + \Delta x,\; {\color{red} s*y} + \Delta y,\; i]$$

\vfill
This is typically done with a stride greater than one so that the image dimension is reduced.

\slide{Fully Connected (FC) Layers}

\centerline{\includegraphics[width = 8.0in]{\images/VGG}}


\slide{2D CNN in PyTorch}

conv2d({\bf input, weight, bias, stride, padding, dilation, groups})

\bigskip
{\bf input} – tensor (minibatch,in-channels,iH,iW)

\medskip
{\bf weight} – filters (out-channels, in-channels/groups,kH,kW)

\medskip
{\bf bias} – tensor (out-channels) . Default: None

\medskip
{\bf stride} – Single number or (sH, sW). Default: 1

\medskip
{\bf padding} – Single number or (padH, padW). Default: 0

\medskip
{\bf dilation} – Single number or (dH, dW). Default: 1

\medskip
{\bf groups} – split input into groups. Default: 1

\slide{Dilation and Grouping}

Dilation is used for ``hypercolumns'' where higher layers have the same spatial dimension as the input but each spatial location in a higher layer
is a whole-image representation of a region of the input image.

\vfill
Grouping reduces the computation by limiting the inputs to a neuron to be values in the same ``neuron group'' as the input.

\vfill
Dilation and grouping are rarely used today.

\slide{Modern Trends}

Modern Convolutions use 3X3 filters.  This is faster and has fewer parameters.  Expressive power is preserved by increasing depth with many stride 1 layers.

\vfill
Max pooling has disappeared.

\vfill
ResNet and resnet-like architectures are now dominant.

\slide{Alexnet, 2012}
{\huge
\centerline{Given Input$[227,227,3]$}

\begin{eqnarray*}
L_1[55 \times 55 \times 96] & = & \relu(\conv(\mathrm{Input},\Phi_1,\mathrm{width}\;11,\pad\;0,\stride\;4)) \\
L_2[27 \times 27 \times 96] & = & \mathrm{MaxPool}(L_1,\mathrm{width}\;3,\stride\;2))  \\
L_3[27 \times 27 \times 256] & = & \relu(\conv(L_2,\Phi_3,\mathrm{width}\;5,\pad\;2,\stride\;1))  \\
L_4[13 \times 13 \times 256] & = & \mathrm{MaxPool}(L_3,\mathrm{width}\;3,\stride\;2))  \\
L_5[13 \times 13 \times 384] & = & \relu(\conv(L_4,\Phi_5,\mathrm{width}\;3,\pad\;1,\stride\;1))  \\
L_6[13 \times 13 \times 384] & = & \relu(\conv(L_5,\Phi_6,\mathrm{width}\;3,\pad\;1,\stride\;1))  \\
L_7[13 \times 13 \times 256] & = & \relu(\conv(L_6,\Phi_7,\mathrm{width}\;3,\pad\;1,\stride\;1))  \\
L_8[6 \times 6 \times 256] & = & \mathrm{MaxPool}(L_7,\mathrm{width}\;3,\stride\;2)) \\
L_9[4096] & = & \relu(\mathrm{FC}(L_8,\Phi_9)) \\
L_{10}[4096] & = & \relu(\mathrm{FC}(L_9,\Phi_{10})) \\
s[1000] & = & \relu(\mathrm{FC}(L_{10},\Phi_s)) \;\;\mbox{class scores}
\end{eqnarray*}
}

\slideplain{VGG, 2014}
\centerline{\includegraphics[height=4.5in]{\images/VGGStack}}
\centerline{\large Stanford CS231}

\slide{Inception, Google, 2014}

\centerline{\includegraphics[width = 9.0in]{\images/inception1}}
\vfill
\centerline{\includegraphics[width = 3.5in]{\images/inception2}}
\slideplain{ResNet, 2015}

\centerline{\includegraphics[height= 5.2in]{\images/ResNetStack} {\large [Kaiming He]}}

\slide{CNN Imagenet Classification}

1000 kinds of objects.

\vfill
\centerline{\includegraphics[height=4.2in]{\images/IVLSRC}}
2016 was 3.0\%, 2017 was  2.25\%, 2020 was 1.3\%
 
\slideplain{END}

\slideplain{Image to Column (Im2C)}

Reduce convolution to matrix multiplication

\vfill
more space but faster.

\vfill
\begin{eqnarray*}
  & & \tilde{L}_{{\ell+1}}[b,x,y,n] \\
  \\
  & = & \left(\sum_{\Delta x, \Delta y, i} K_{\ell+1}[\Delta x, \Delta y, i, n] *L_{{\ell}}[b,x + \Delta x,\; y + \Delta y,\; i]\right) + B[n]
  \end{eqnarray*}

\vfill
We make a bigger tensor $\tilde{L}$ with two additional indeces.

$$\tilde{L}_{{\ell}}[b,x,y,\Delta x,\Delta y,i] = L_{{\ell}}[b,x+\Delta x,y+\Delta y,i]$$

\slideplain{Image to Column (Im2C)}

\begin{eqnarray*}
  & & \tilde{L}_{{\ell+1}}[b,x,y,n] \\
  \\
  & = & \left(\sum_{\Delta x, \Delta y, i} K_{\ell+1}[\Delta x, \Delta y, i, n] *L_{{\ell}}[b,x + \Delta x,\; y + \Delta y,\; i]\right) + B[n] \\
  \\
      & = & \left(\sum_{\Delta x, \Delta y, i} \begin{array}{l}
                                              \tilde{L}_{{\ell}}[b,x,y,\Delta x,\Delta y,i]
                                              * K_{\ell+1}[\Delta x, \Delta y, i, n] \\
  \end{array}\right) + B[n] \\
  \\
    & = & \left(\sum_{(\Delta x, \Delta y, i)} \begin{array}{l}
                                              \tilde{L}_{{\ell}}[(b,x,y),(\Delta x,\Delta y,i)]
                                              * K_{\ell+1}[(\Delta x, \Delta y, i), n] \\
                                           \end{array}\right) + B[n]
\end{eqnarray*}

}
\end{document}
