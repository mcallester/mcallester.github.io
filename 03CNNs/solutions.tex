\documentclass{article}
\input ../preamble
\parindent = 0em

%\newcommand{\solution}[1]{}
\newcommand{\solution}[1]{\bigskip {\color{red} {\bf Solution}: #1}}

\begin{document}



\centerline{\bf TTIC 31230 Fundamentals of Deep Learning, winter 2019}

\medskip
\centerline{\bf CNN Problems}

In these problems, as in the lecture notes, capital letter indeces are used to indicate subtensors (slices) so that, for example,  $M[I,J]$ denotes a matrix
while $M[i,j]$ denotes one element of the matrix, $M[i,J]$ denotes the $i$th row, and $M[I,j]$ denotes the $j$th collumn.

\medskip
We also adopt the convention, similar to true Einstein notation, that repeated capital indeces in a product of tensors are implicitly summed.  We can then write
the inner product $e[w,I]^\top h[t,I]$ as $e[w,I]h[t,I]$.  Using this implicit summation notation we can avoid ever using transpose.

\bigskip
{\bf Problem 1.}  Consider convolving a kernel $K[\nout,\Delta x, \Delta y,\nin]$  with thresholds $B[\nout]$ on a layer $L[b,x,y,\nin]$ where $B,\;X,\;Y,\Nout, \Nin,\Delta X,\;\Delta Y$
are the number of possible values for $b$, $x$, $y$, $\nout$, $\nin$, $\Delta x$ and $\Delta y$ respectively.
How many floating point multiplies are required
in computing the convolution on the batch (without any activation function)?

\solution{$$BXY\;\Delta X\;\Delta Y \;\Nout \;\Nin$$}

\bigskip
{\bf Problem 2:} Suppose that we want a video CNN producing layers of the form $L[b,x,y,t,n]$ which are the same as the layers of an image CNN but with an additional time index.
Write the equation for computing $L_{\ell+1}[b,x,y,t,j]$ from the tensor $L_\ell[B,X,Y,T,I]$.  Your filter should include an index $\Delta t$ and handle a stride $s$ applied
to both space and time. Use the repeated index notation for summation.

\solution{
  $$L_{\ell + 1}[b,x,y,t,\nout] = \sigma(K_{\ell+1}[\nout \Delta X, \Delta Y, \Delta T, \Nin] L_\ell[b, sx+ \Delta X, sy + \Delta Y, st + \Delta T, \Nin]- B[\nout])$$
  }

\ignore{
\bigskip
{\bf Problem 3:} Images have translation invariance --- a person detector must look for people at
various places in the image.  Translation invariance is the motivation for convolution --- all
places in the image are treated the same.

\medskip
Images also have some degree of scale invariance --- a person detector must look for people of different sizes
(near the camera or far from the camera).  We would like to design a deep architecture that treats all scales (sizes) the same just as CNNs
treat all places the same.

\medskip
Consider a batch of input images
$L_{0,d}[b,x,y,n]$ where $d$ is the spacial dimension of $x$ and $y$, which we assume to be a power of 2, and $n$ ranges over the three color values red, green, blue.
We start by constructing an ``image pyramid'' $L_{0,2^k}[b,x,y,n]$ for $2^k \leq d$
where $L_{0,d/{2^k}}[b,x,y,n]$ consists of the input images down-sampled by a factor 2 $k$ times.  For formally,
the image pyramid $L_{0,d/2}[b,x,y,i]$ is constructed from the input image $L_{0,d}[b,x,y,n]$ by down-sampling
using pixel averaging.
\begin{eqnarray*}
  L_{0,d/2}[b,x,y,n] & = & \frac{1}{4}\left(\begin{array}{l} L_{0,d}[b,2x,2y,n] + L_{0,d}[b,2x+1,2y,n] \\ + L_{0,d}[b,2x,2y+1,n] + L_{0,d}[b,2x+1,2y+1,n]\end{array}\right)
\end{eqnarray*}
We will compute a set of layers $L_{\ell,d}[b,x,y,n]$ where $d$ is a power of 2 and $\ell \leq \log_2 d$.
While we have a quadratic number of layers $L_{\ell,d}[b,x,y,n]$ we will use a linear number of kernels $K_{\ell+1}[\nout,\Delta x,\Delta Y,\nin]$.
Give an equation for computing $L_{\ell+1,d}[b,x,y,\nout]$ as the result of a linear threshold neuron taking inputs from both
$L_{\ell,d}[b,x,y,n]$ {\bf and} $L_{\ell,d/2}[b,x,y,n]$ using the same kernel  $K_{\ell+1}[\nout,\Delta x,\Delta Y,\nin]$.

\solution{
  $$L_{\ell+1,d/2}[b,x,y,n] = \sigma\left(\begin{array}{ll}
    & K_{\ell+1}[\nout,\Delta X,\Delta Y,\Nin]L_{\ell,d/2}[b,x+\Delta X,y+\Delta Y,\Nin] \\
    + & K_{\ell+1}[\nout,\Delta X,\Delta Y,\Nin]L_{\ell,d}[b,2x+\Delta X,2y+\Delta Y,\Nin] \\
    - & B_{\ell+1}[\nout] \end{array}\right)$$
}

}
\end{document}
