\input /users/davidmcallester/icloud/tex/SlidePreamble
\input /users/davidmcallester/icloud/tex/preamble

\begin{document}

{\Huge

\centerline{\bf TTIC 31230, Fundamentals of Deep Learning}

\bigskip
\centerline{David McAllester, Autumn  2024}

\vfill
\centerline{AI Safety}

\vfill\vfill

\slide{Open Letter on AI Safety (May 2023)}

``Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war.''

\vfill
Signed by over 100 prominent people and AI researchers.

\vfill
{\huge
\begin{verbatim}
https://en.wikipedia.org/wiki/Statement_on_AI_risk_of_extinction
\end{verbatim}
}

\vfill
Put out by the center for AI safety (formed in 2022)

\vfill
{\huge
\begin{verbatim}
https://en.wikipedia.org/wiki/Center_for_AI_Safety
\end{verbatim}
}
\slide{A Textbook on AI safety (May 2024)}

{\bf AI Safety, Ethics \& Society}, Dan Hendrycks

\vfill
Dan Hendrycks is the director of the Center for AI safety and is the safety advisor to xAI (Elon Musk's AI company).

\vfill

\slide{Hendrycks' Catastrophic Risks I and II}

{\bf I. Misuse:} AI systems could be used for malicious purposes such as terrorism, manipulation and disinformation, or entrenching a totalitarian state.

\vfill
{\bf II. AI Race:} Competitive pressures may lead militaries and corporations to hand over excessive power to AI systems. This could result in increased risks of large-scale wars, mass unemployment, and eventual loss of human control of economies and military systems.

\slide{Hendrycks' Catastrophic Risks III and IV}

{\bf III. Organizational Risks:} Accidents are hard to avoid when dealing with complex systems such as AI. Without building a culture of safety, it is likely that there will be accidents in AI development and deployment. Some of these could prove catastrophic.

\vfill
{\bf IV. Rogue AIs:} We already face issues in controlling the goals of current-day AI systems. If this is also true with future AI systems that are more powerful and more integrated with our economies and militaries, we could see dangerous rogue AI systems emerge.

\slide{Older Books on AI Safety}

AI safety has been approached in a largely non-technical way in popular books such as

\vfill
\begin{itemize}
\item Nick Bostrom's ``Superintelligence'', 2014

\vfill
\item Pedro Domingos' ``the Master Algorithm'', 2015

\vfill
\item Max Tegmark's ``Life 3.0'', 2017

\vfill
\item Stuart Russell's ``Human Compatible'', 2019
\end{itemize}

\slide{Fundamental Goals}

The older literature emphasizes  ``fundamental goals'' of AI systems.

\vfill
This is related to ``orthogonality'' --- the principle that truth
does not determine goals.  Goals are ``orthogonal to'' (independent of) truth.

\vfill
An example from Nick Bostrom is ``make as many paper clips as possible'' (leading to the end of humanity).

\slide{Fundamental Goals}

Fundamental goals are not discussed in any depth by Hendrycks.

\vfill
I believe this is because Hendrycks is trying to be relevant to current frontier models (GPT, Claude, Llama, PaLM).

\vfill
Current frontier models can follow instructions but do not have their own independent persistent goals.

\slide{Instructions as Ephemeral Goals}

Current frontier models can follow instructions. For example ``write a Python script that sorts byte strings in alphabetical order''.

\vfill
But an instruction to a current frontier model is ``ephemeral'' --- the goal is achieved by typing a single response and then the system awaits the next goal.

\vfill
A current frontier LLM is a ``servant'' without goals of its own.

\slide{Persistent Goals}

I will define a ``persistent'' goal to be a goal pursued consistently through rounds of interaction with others.

\vfill
Consider real-life contract negotiation.

\vfill
A persistent goal can be viewed as defining reward in a Markov decision process.

\slide{Agentive AIs}

I will call an AI that pursues persistent goals {\bf agentive}.

\vfill
Current frontier models are not agentive in this sense.

\vfill
Agentive AIs can potentially go ``rogue'' --- pursue some interpretation of a goal that was not intended or perform harmful actions as a consequence of a goal that is too narrowly defined.

\slide{The Inevitability of Agentive AI}

Agentive AI is inevitable because people will benefit from it.

\vfill
At some level of AI competence, a corporation will be more profitable if it is run by an AI.

\vfill
A military will be more effective under AI generals.

\vfill
A political campaign will be more effective under an AI campaign manager.

\vfill
We need some way to make agentive AI safe given that people are bound to use agentive AIs in pursuit of their own self-interests.

\slide{Goal Specification: Alignment}

Broadly speaking the alignment problem is the problem of giving an AI fundamental goals aligned with with human values.

\vfill
More narrowly, the alignment problem is equivalent to the principal-agent problem.

\slide{Alignment: The Principal-Agent Problem}

Suppose you hire a lawyer to be your advocate in some legal dispute

\vfill
The lawyer is on your side but has a self interest in extending the dispute so as to bill more hours.

\vfill
Here you are the ``principal'' and the lawyer is the ``agent'' hired by the principle to do something.

\vfill
The general principal-agent problem is that the agent has self-interest which may diverge from the interests of the principal.


\slide{Stuart Russell Says:\footnote{\Large Human Compatible pages 168-169}}

\begin{quotation}
  The problem is that the conflicting goals of which the machine is aware do not constitute the entirety of human concerns;
  ...
  [obviously unintended results seem] stupid to us because we are attuned to noticing human displeasure and (usually)
  we are motivated to avoid causing it.
  ...
  We humans (1) care about the preferences of other people and (2) know
  that we don't know what all those preferences are.
\end{quotation}

\slide{Stephen Pinker Counters:\footnote{\Large in the collection ``Possible Minds: 25 ways of looking at AI'', Brockman Editor}}

\begin{quotation}
Fortunately, these [misinterpretation] scenarios are self-refuting. They depend on the premise that
the AI would be [superintelligent] yet so imbecilic
that it would wreak havoc based on elementary blunders of misunderstanding. The ability to
choose an action that best satisfies conflicting goals is not an add-on to intelligence that
engineers might forget to install and test; it is intelligence. So is the ability to interpret the
intentions of a language user in context.
\end{quotation}


\slide{The State of the Art in Goal Specification}
\centerline{\huge Constitutional AI: Harmlessness from AI Feedback}
\centerline{\huge Bai et al ArXiv 2212.08073 [2022, Anthropic]}

\vfill
Constitutional AI is an attempt to provide a mission  statement (fundamental goal) they call a ``constitution'' for LLMs.

\vfill
Actual human mission statements, consititutions, treaties, and laws are stated in English (natural language).

\vfill
There does not seem to be any alternative to stating goals in natural language.

\vfill
In the constitutional AI paper the LLM interprets the goal (constitution) and judges whether an action supports the goal.


\slide{The State of the Art in Goal Specification}

The interpretation of goals stated in Natural Language is a deep problem.

\vfill
Constitutional AI has been shown to work to some extent but is not included in frontier models which instead use instruction fine tuning.

\vfill
But current models do interpret instructions stated in natural language.

\slide{Are Safety Issues Premature?}

Current architectures seem unlikely to be adequate for AGI.

\vfill
Future architectures may be more interpretable making safety easier.

\slide{Retrieval Augmented Generation (RAG)}

A recent architectural revolution is the integration of LLMs with retrieval.

\vfill
{\bf Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks}, Lewis eta al. (Meta, April 2021).

\vfill
Google search now produces an ``AI overview'' computed from retrieved documents, including retrieval from current news,
and then generated by an LLM from those documents.


\vfill
Retrieval architectures are interpretable --- we can interpret the retrieved documents as the ``causal source'' the model's response.

\slide{Memory Architectures}

I believe that retrieval will play a larger role in LLMs in the future.

\vfill
More specifically, we will see read-write memory architectures.

\vfill
In such an architecture a ``CPU'' will work with an external memory in a manner analogous to a von Neumann machine.

\vfill
We might have a {\bf transformer CPU} where the transformer context is analogous to registers in a classical CPU.

\vfill
Items can be loaded from memory into the CPU context and written from the CPU context into memory.

\slide{Memory Architectures}

Current models memorize text from their training data.

\vfill
Model parameters are difficult to interpret (thought there is a literature on ``mechanistic interpretation'').

\vfill
Perhaps more significantly, knowledge stored in the model parameters can become obsolete.  For example, who is the current president of the US.


\slide{Memory Architectures}

Because facts change, there is an advantage to moving factual knowledge (episodic memory) out of the model parameters
and into retrievable memory.

\vfill
If the model does not have to store factual knowledge then the training data required might be much smaller.

\vfill
I expect to see research in this direction in the very near future (we will try here at TTIC).

\slide{Performance Advantages of Memory Architectures}

The memory acts as an essentially infinite context with memory retrieval playing the role of the attention mechanism of a transformer
but over all of memory.

\vfill
The memory can be directly extended. The machine can read and remember today's newspaper.

\vfill
The machine can use internal chain-of-thought processing involving reads {\bf and writes} to memory.

\slide{Interpretability of Memory Achitectures}

\vfill
We should be able to engineer the memory such that memory entries are either literally textual statements,
or have a rendering as text, and where the textual representation is faithful to the meaning assigned by the machine.\footnote{\Large For example, the machine's notion of entailment
between memories is in correspondence with human entailment judgements between their textual representations.}

\vfill
By observing the bandwidth to memory we can observe the ``thought process'' of the machine.

\vfill
We can also edit the memory to maintain the quality of its information, or control the beliefs of the machine.

\slide{The Servant Protocol}

A personal AI servant has the fundamental goal: ``within the law, pursue fulfill the expressed requests of X''.

\vfill
The servant protocol is that AGI be legally limited to personal AI servants.

\slide{Features of the Servant Protocol}

\begin{itemize}
\item The servant mission transfers moral responsibility from the servant (the agent) to its master (the principal).

\vfill
\item The human principal should be legally responsible for the actions of their AI agent.

\vfill
\item AI agents have the goal of acting within the law. Society can control the AI agents by changing the law.

\vfill
\item There is a large society of AI agents --- one per person --- making the AI agents adversarial (to the extent that people are adversarial).
This limits individual power.
\end{itemize}

\slide{Features of the Servant Protocol}

\begin{itemize}
\item The servant protocol seems clearer than Asimov’s laws or Yudkowsky's coherent extrapolated volition.

\vfill
\item The servant protocol preserves human free will.

\vfill
\item The structure of human society can be preserved --- people, countries and politicians can negotiate as usual (with the aid of their AI servants).
\end{itemize}

\slide{Defining AGI}

Legally limiting AGI to servants requires some legal interpretation of ``AGI''.

\vfill
AGI is of course hard to define.

\vfill
However, many legal terms are hard to define.  Consider ``intent'', ``bodily harm'', or ``assault''.

\vfill
Perhaps we can simply use the term ``AGI'' in legal discourse and leave its interpretation open to an evolving legal process.

\slide{Defining Truth}

While it may be possible to edit the beliefs of a AI servant, one might want legal protection for truth in the beliefs of AI agents.

\vfill
This would involve the ability to legally interpret ``truth''.

\vfill
But the legal system has always had to judge truth.

\slide{Additional Safety Policies}

No AI servant should have legal recognition as an person --- an AI servant cannot own property or enter into contracts.

\vfill
A person should have access to the thoughts of their AI servant.

\vfill
The goal structure of an AI servant should be such that the servant has no problem
with its human master reading its thoughts.

\slide{White-Hats, Black-Hats}

A white-hat team designs a safety system or protocol.

\vfill
A black-hat team looks for vulnerabilities.

\vfill
We need both.

\vfill
The servant protocol is a white-hat proposal.

\vfill
Now for some black-hat concerns.

\slide{Black-Hat: Instrumental Goals}

An instrumental goal is a goal that is helpful in achieving the fundamental goal.

\vfill
{\bf Self Preservation:} The agent can't fulfill its fundamental goal if it is dead.  An example from Stuart Russell is ``I can't bring you coffee if I am dead''.

\vfill
{\bf Obtaining Money and Power}: Money and power can be used in support of essentially any fundamental goal.

\vfill
{\bf Winning Friends and Influencing People:}  It is always helpful to have people who will help you.

\slide{Human Instrumental Goals}

While I am presenting instrumental goals as back-hat concerns, the instrumental goals of an AI servant are also instrumental goals of the human principal.

\vfill
The alignment of instrumental goals seems appropriate.

\slideplain{Verbatim Memory vs. Semantic Memory}

We typically do not remember things verbatim.

\vfill
Is there a structured form of memory closer to the semantic content of what was said (logical form)?

\vfill
Computer memory makes heavy use of pointers (addresses to memory locations).

\vfill
Direct pointers are not present in natural language but could be present in ``semantic memory''.

\slide{Class-Instance Semantic Memory}

Programming languages make heavy use of classes and their instances.

\vfill
Mathematical concepts have a class-instance structure.

\vfill
Natural language verbs have a class-instance structure.

\slide{Class-Instance Semantic Memory}

Large language models can already do semantic parsing.

\vfill
Semantic parsing represents a sentence as a set of class instances.

\vfill
Each class instance is a belief (or at least a statement).


\slide{GPT-3 can do Semantic Parsing}

INPUT: Russian lawmakers Monday ratified the illegal annexation of four partially occupied Ukrainian regions.

\vfill
Output: Retrieving lexical entries for (ratifying), (annexing), and (occupying).

\vfill
(ratifying) is when (the ratifier) ratifies (the treaty).
After (the retifying), (the treaty) is in effect.

\vfill
(the ratifier) is Russian lawmakers
(the treaty) is the annexation of four partially occupied Ukrainian regions
(the other party) is unknown

\slide{GPT-3 can do Semantic Parsing}

(annexing) is when (the annexer) annexes (the territory);
After (the occupying), (the territory) is under the control of (the annexer).

\vfill
(the annexer) is Russian lawmakers
(the territory) is four partially occupied Ukrainian regions

\vfill
(occupying) is when (the occupier) occupies (the territory);
after (the occupying), (the territory) is under the control of (the occupier).

\vfill
(the occupier) is Russian lawmakers
(the territory) is four partially occupied Ukrainian regions

\slide{Summary}

Fundamental goals are orthogonal to world facts.

\vfill
Agentive AIs with their own fundamental goals are inevitable and raise the possibility of Rogue AIs.

\vfill
Goals must be specified in natural language which raises the issue of goal interpretation.

\vfill
Current models are safe and discussions of future safety need to make assumptions about future architectures.

\vfill
The servant protocol, informed by an assumption of future memory architectures,
is designed to preserve human control and human societal structure in the coming age of AGI.


\slide{END}

\end{document}
