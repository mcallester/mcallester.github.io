\input /users/davidmcallester/icloud/tex/SlidePreamble
\input /users/davidmcallester/icloud/tex/preamble

\begin{document}

{\Huge

  \centerline{\bf TTIC 31230, Fundamentals of Deep Learning}
  \bigskip
  \centerline{David McAllester, Winter 2022}
  \vfill
  \vfill
  \centerline{\bf Variational Auto-Encoders (VAEs)}
  \vfill
  \vfill

\slidetwo{Meaningful Latent Variables:}
{Learning Phonemes and Words}

A child exposed to speech sounds learns to distinguish phonemes and then words.

\vfill
The phonemes and words are ``latent variables'' learned from listening to sounds.

\vfill
We will use $y$ for the raw input (sound waves) and $z$ for the latent variables (phonemes).

\slide{Other Examples}

$z$ might be a parse tree, or some other semantic representation, for an observable sentence (word string) $y$.

\vfill
$z$ might be a segmentation of an image $y$.

\vfill
$z$ might be a depth map (or 3D representation) of an image $y$.

\vfill
$z$ might be a class label for an image $y$.

\vfill
Here we are interested in the case where $z$ is {\bf latent} in the sense that we do not have training labels for $z$.

\slide{Rate-Distortion Autoencoders}

Consider image compression where we compress an image $y$ into a compressed file $z$.

\vfill
We will assume a stochastic compression algorithm
which we will call the ``encoder'' $P_\enc(z|y)$.

\vfill
The number of bits needed for the compressed file is given by $H(z)$.  $H(z)$
is the ``rate'' (bits per image) for transmitting compressed images.

\vfill
The number of unknown additional bits needed to exactly recover $y$ is $H(y|z)$.
$H(y|z)$ is a measure of the ``distortion'' of $y$ when $y$ is decoded without the missing bits.

\slide{Rate-Distortion Autoencoders}

In practice we model $H(z)$ with a ``prior model'' $P_\pri(z)$ and model $H(y|z)$ with a ``decoder model''
$P_\dec(y|z)$.

\vfill
So the rate-distortion auto-encoder has three parts $P_\enc(z|y)$, $P_\pri(z)$, and $P_\dec(y|z)$.

\vfill
The {\bf variational autoencoder (VAE)} with latent variable $z$ is mathematically the same as a rate-distortion autoencoder
with compressed form $z$.


\slide{An ``Encoder First'' Treatment of VAEs}

Fix an arbitrary encoder model $P_\enc(z|y)$.

\vfill
For $y \sim \pop$ and $z \sim P_\enc(z|y)$ train models $\pri$ and $\dec$.

\begin{eqnarray*}
\mbox{Prior Model:}\;\;\pri^* & = & \argmin_\pri \;\; E_{y,z}\;\;-\ln P_\pri(z) \\
\\
\mbox{Decoder Model:}\;\dec^* & = & \argmin_\dec\;\; E_{y,z}\;\;-\ln P_\dec(y|z)
\end{eqnarray*}

\vfill
For any $P_\enc(z|y)$ the universality assumption for $\pri^*$ and $\dec^*$ gives

\vfill
$$\pop(y) = \sum_z\;\;P_{\pri^*}(z)P_{\dec^*}(y|z)$$

\slide{An Encoder First Treatment of VAEs}

Fix an arbitrary encoder model $P_\enc(z|y)$.

\vfill
$$\pop(y) = \sum_z\;\;P_{\pri^*}(z)P_{\dec^*}(y|z)$$

\vfill
This encoder first formulation will be particularly relevant to the diffusion models underlying DALL$\cdot$E-2.

\slide{An Encoder First Treatment of VAEs}

Sample $y \sim \pop$ and $z \sim P_\enc(z|y)$.

\vfill
$$H(y,z) = H(y) + H(z|y) = H(z) + H(y|z)$$

\vfill
Solving for $H(y)$ gives

$$H(y) = H(z) + H(y|z) - H(z|y)$$

\slide{An Encoder First Treatment of VAEs}
Sample $y \sim \pop$ and $z \sim P_\enc(z|y)$.

\vfill
\begin{eqnarray*}
H(y) & = & H(z) + H(y|z) - H_\enc(z|y) \\
\\
 & \leq & H_\pri(z) + H_\dec(y|z) - H_\enc(z|y) \\
 \\
H_\pri(z) & = & E_{y,z}\;[-\ln P_\pri(z)] \\
\\
H_\dec(y|z) & = & E_{y,z}\;[- P_\dec(y|z)]
\end{eqnarray*}

\slide{VAE}
Sample $y \sim \pop$ and $z \sim P_\enc(z|y)$.

\vfill
\begin{eqnarray*}
H(y) & \leq & H_\pri(z) + H_\dec(y|z) - H_\enc(z|y) \\
\\
 & = & E_{y,z}\;\left[- \ln \frac{P_\pri(z)P_\dec(y|z)}{P_\enc(z|y)}\right]
\end{eqnarray*}

\vfill
$$\enc^*,\pri^*,\dec^* = \argmin_{\enc,\pri,\dec}\;\;E_{y,z}\;\left[- \ln \frac{P_\pri(z)P_\dec(y|z)}{P_\enc(z|y)}\right]$$


\slide{VAE}
Sample $y \sim \pop$ and $z \sim P_\enc(z|y)$.

\vfill
$$\enc^*,\pri^*,\dec^* = \argmin_{\enc,\pri,\dec}\;\;E_{y,z}\;\left[- \ln \frac{P_\pri(z)P_\dec(y|z)}{P_\enc(z|y)}\right]$$

\vfill
Under the universality assumption for the prior and the decoder we do not need to optimize the encoder.

\vfill
Before the deep revolution the structure of the prior and the decoder was typically highly restricted.
In such cases we are far from universality and we need to optimize the encoder.  This is related to EM  as discussed below.

\slide{The ELBO}

\begin{eqnarray*}
P_{\pri,\dec}(y) & = & \sum_z\;P_\pri(z)P_\dec(y|z) \\
\\
& = & E_{z\sim P_\pri(z)}\;\;\;P_\dec(y|z)
\end{eqnarray*}


\vfill
$$\pri^*,\dec^* = \argmin_{\pri,\dec}\;E_{y \sim \pop}\;[- \ln P_{\pri,\dec}(y)]$$

\slide{The ELBO}

We will show

$$\mathrm{ELBO:}\;\;\ln \;P_{\pri,\dec}(y) \geq E_{z\sim P_\enc(z|y)}\;\left[\ln \frac{P_\pri(z)P_\dec(y|z)}{P_\enc(z|y)}\right]$$

\vfill
The left hand side is the log-probability of the evidence $y$ and the right hand side is the {\bf evidence lower bound} or ELBO.

\vfill
$$\mathrm{VAE:}\;\; \enc^*,\pri^*,\dec^* = \argmin_{\enc,\pri,\dec}\;\;E_{y,z}\;\left[- \ln \frac{P_\pri(z)P_\dec(y|z)}{P_\enc(z|y)}\right]$$

\vfill
The negative ELBO is the loss function of the VAE.

\slide{Deriving the ELBO}

But even when $P_\pri(z)$ and $P_\dec(y|z)$ are samplable, if $z$ is a structured value we cannot typically compute $P_{\pri,\dec}(y)$.

\vfill
$$P_{\pri,\dec}(y) = \sum_z\;P_\pri(z)P_\dec(y|z) = E_{z\sim P_\pri(z)}\;P_\dec(y|z)$$

\vfill
The sum is too large and sampling $z$ from $P_\pri(z)$ is unlikely to sample the values that dominate the sum.

\slide{Deriving the ELBO}

\vfill
A much better estimate could be achieved by importance sampling --- sampling $z$ from the posterior $P_{\pri,\dec}(z|y)$.

{\huge
\begin{eqnarray*}
P_{\pri,\dec}(y) & = & \sum_z\;P_\pri(z) P_\dec(y|z) \\
\\
 & = & \sum_z\;P_{\pri,\dec}(z|y)\;\frac{P_\pri(z)P_\dec(y|z)}{P_{\pri,\dec}(z|y)} \\
 \\
 \\
  & = & E_{z\sim P_{\pri,\dec}(z|y)}\;\;\;\frac{P_\pri(z)P_\dec(y|z)}{P_{\pri,\dec}(z|y)}
\end{eqnarray*}
}

\slide{Deriving the ELBO}

\begin{eqnarray*}
P_{\pri,\dec}(y) & = & E_{z\sim P_{\pri,\dec}(z|y)}\;\;\frac{P_\pri(z)P_\dec(y|z)}{P_{\pri,\dec}(z|y)}
\end{eqnarray*}

\vfill
Unfortunately the conditional distribution $P_{\pri,\dec}(z|y)$ also cannot be computed or sampled from.

\vfill
Variational Bayes side-steps the intractability problem by introducing another model component --- a model $P_\enc(z|y)$ to approximate the intractible $P_{\pri,\dec}(z|y)$.

\slide{The Evidence Lower Bound (The ELBO)}

{\huge
\begin{eqnarray*}
 {\ln P_{\pri,\dec}(y)} & = & \ln E_{z \sim { P_\enc(z|y)}} \frac{P_{\pri,\dec}(y)}{P_{\enc}(z|y)} \\
\\
\\
  &  \geq & E_{z \sim { P_\enc(z|y)}} \ln \frac{P_{\pri,\dec}(z,y)}{{ P_\enc(z|y)}} \;\;\;\;\;\mbox{The ELBO}
\end{eqnarray*}
}


Where the second line follows from Jensen's inequality for concave functions.

\slide{The Re-Parameterization Trick}

$$\enc^*,\pri^*,\dec^* = \argmin_{\enc,\pri,\dec}\;\;E_{y,{\color{red} z\sim P_\enc(z|y)}}\;\left[- \ln \frac{P_\pri(z)P_\dec(y|z)}{P_\enc(z|y)}\right]$$

\vfill
Gradient descent on the encoder parameters must take into account the fact that we are sampling from the encoder.

\vfill
To handle this we sample noise $\epsilon$ from a fixed noise distribution and replace $z$ with a determinstc function $z_\enc(y,\epsilon)$

\vfill
$$\enc^*,\pri^*,\dec^* = \argmin_{\enc,\pri,\dec}\;\;E_{y,{\color{red} \epsilon,z=z_\enc(y,\epsilon)}}\;\left[- \ln \frac{P_\pri(z)P_\dec(y|z)}{P_\enc(z|y)}\right]$$

\slide{The Re-Parameterization Trick}

$$\enc^*,\pri^*,\dec^* = \argmin_{\enc,\pri,\dec}\;\;E_{y,{\color{red} \epsilon,z=z_\enc(y,\epsilon)}}\;\left[- \ln \frac{P_\pri(z)P_\dec(y|z)}{P_\enc(z|y)}\right]$$

\vfill
To get gradients we must have that $z_\enc(y,\epsilon)$ is a smooth function of the encoder parameters and all probabilties must be a smooth function of $z$.

\vfill
Most commonly $\epsilon \in R^d$ with $\epsilon \sim {\cal N}(0,I)$ and
$$z^i_\enc(y,\epsilon) = \hat{z}^i_\enc(y) + \sigma^i\epsilon^i.$$

\vfill
Optimizing the encoder is tricky for discrete $z$.  Discrete $z$ is handled effectively in EM algorithms and in VQ-VAEs.

\slide{EM is Alternating Optimization of the VAE}

Expectation Maximimization (EM) applies in the (highly special) case where the exact posterior $P_{\pri,\dec}(z|y)$ is samplable and computable.
EM alternates exact optimization of $\enc$ and the pair $(\pri,\dec)$ in:
$$\mbox{VAE:}\;\;\;\;\;\;\; {\color{red} \pri^*,\dec^*} = \argmin_{\color{red} \pri,\dec} \min_{\color{red} \enc} E_{y,\;z \sim P_{\color{red} \enc}(z|y)}\;\;- \ln \frac{P_{\color{red} \pri}(z,y)}{P_{\color{red} \enc}(z|y)}$$

\vfill
$$\mbox{EM:}\;\;\;\;\;\; {\color{red} \pri^{t+1},\dec^{t+1}} =  \argmin_{\color{red} \pri,\dec}\;\;\;\;E_{y,\;z \sim P_{\color{red} \pri^t,\dec^t}(z|y)}\; - \ln P_{\color{red} \pri,\dec}(z,y)$$

\vfill
\centerline{\hspace{1em} Inference \hspace{6em} Update \hspace{2.5em}~}
\centerline{(E Step) \hspace{6em} (M Step) ~}
\centerline{ $P_\enc(z|y) = P_{\pri^{\color{red} t},\dec^{\color{red} t}}(z|y)$ \hspace{2.5em} Hold $P_\enc(z|y)$ fixed \hspace{0em}~}

\slide{END}

\end{document}
