\input /users/davidmcallester/icloud/tex/SlidePreamble
\input /users/davidmcallester/icloud/tex/preamble

\begin{document}

{\Huge

  \centerline{\bf TTIC 31230, Fundamentals of Deep Learning}
  \bigskip
  \centerline{David McAllester, Winter 2022}
  \vfill
  \vfill
  \centerline{\bf Variational Auto-Encoders (VAEs)}
  \vfill
  \vfill

\slide{Rate-Distortion Autoencoders}

Consider image compression where we compress an image $y$ into a compressed file $z$.

\vfill
We will assume a stochastic compression algorithm
which we will call the ``encoder'' $P_\enc(z|y)$.

\vfill
The number of bits needed for the compressed file is given by $H(z)$.  $H(z)$
is the ``rate'' (bits per image) for transmitting compressed images.

\vfill
The number of unknown additional bits needed to exactly recover $y$ is $H(y|z)$.
$H(y|z)$ is a measure of the ``distortion'' of $y$ when $y$ is decoded without the missing bits.

\slide{Rate-Distortion Autoencoders}

In practice we model $H(z)$ with a ``prior model'' $P_\pri(z)$ and model $H(y|z)$ with a ``decoder model''
$P_\dec(y|z)$.

\vfill
So the rate-distortion auto-encoder has three parts $P_\enc(z|y)$, $P_\pri(z)$, and $P_\dec(y|z)$.

\vfill
The {\bf variational autoencoder (VAE)} with latent variable $z$ is mathematically the same as a rate-distortion autoencoder
with compressed form $z$.


\slide{An ``Encoder First'' Treatment of VAEs}

Fix an arbitrary encoder model $P_\enc(z|y)$.

\vfill
For $y \sim \pop$ and $z \sim P_\enc(z|y)$ train models $\pri$ and $\dec$.

\begin{eqnarray*}
\mbox{Prior Model:}\;\;\pri^* & = & \argmin_\pri \;\; E_{y,z}\;\;-\ln P_\pri(z) \\
\\
\mbox{Decoder Model:}\;\dec^* & = & \argmin_\dec\;\; E_{y,z}\;\;-\ln P_\dec(y|z)
\end{eqnarray*}

\vfill
For any $P_\enc(z|y)$ the universality assumption for $\pri^*$ and $\dec^*$ gives

\vfill
$$\pop(y) = \sum_z\;\;P_{\pri^*}(z)P_{\dec^*}(y|z)$$


\slide{Upper Bounding $H(y)$}

Cross-entropy upper bounds $H(y)$ and equals $H(y)$ assuming universality.

\vfill
The ELBO plays the role of cross-entropy for latent variable models.

\vfill
The negative ELBO uppr bounds $H(y)$ and equals $H(y)$ assuming universality.


\slide{Deriving the ELBO}

Sample $y \sim \pop$ and $z \sim P_\enc(z|y)$.

\vfill
$$H(y,z) = H(y) + H(z|y) = H(z) + H(y|z)$$

\vfill
Solving for $H(y)$ gives

$$H(y) = H(z) + H(y|z) - H(z|y)$$

\slide{An Encoder First Treatment of VAEs}
Replace the first two entropies by cross entropies.

\vfill
\begin{eqnarray*}
H(y) & = & H(z) + H(y|z) - H_\enc(z|y) \\
\\
 & \leq & H_\pri(z) + H_\dec(y|z) - H_\enc(z|y) \\
 \\
H_\pri(z) & = & E_{y,z}\;[-\ln P_\pri(z)] \\
\\
H_\dec(y|z) & = & E_{y,z}\;[- P_\dec(y|z)]
\end{eqnarray*}

\slide{VAE}
\begin{eqnarray*}
H(y) & = & H(z) + H(y|z) - H_\enc(z|y) \\
\\
 & \leq & H_\pri(z) + H_\dec(y|z) - H_\enc(z|y) \\
\\
 & = & E_{y,z}\;\left[- \ln \frac{P_\pri(z)P_\dec(y|z)}{P_\enc(z|y)}\right]
\end{eqnarray*}

\vfill
$$\enc^*,\pri^*,\dec^* = \argmin_{\enc,\pri,\dec}\;\;E_{y,z}\;\left[- \ln \frac{P_\pri(z)P_\dec(y|z)}{P_\enc(z|y)}\right]$$


\slide{The Re-Parameterization Trick}

$$\enc^*,\pri^*,\dec^* = \argmin_{\enc,\pri,\dec}\;\;E_{y,{\color{red} z\sim P_\enc(z|y)}}\;\left[- \ln \frac{P_\pri(z)P_\dec(y|z)}{P_\enc(z|y)}\right]$$

\vfill
Gradient descent on the encoder parameters must take into account the fact that we are sampling from the encoder.

\vfill
To handle this we sample noise $\epsilon$ from a fixed noise distribution and replace $z$ with a determinstc function $z_\enc(y,\epsilon)$

\vfill
$$\enc^*,\pri^*,\dec^* = \argmin_{\enc,\pri,\dec}\;\;E_{y,{\color{red} \epsilon,z=z_\enc(y,\epsilon)}}\;\left[- \ln \frac{P_\pri(z)P_\dec(y|z)}{P_\enc(z|y)}\right]$$

\slide{The Re-Parameterization Trick}

$$\enc^*,\pri^*,\dec^* = \argmin_{\enc,\pri,\dec}\;\;E_{y,{\color{red} \epsilon,z=z_\enc(y,\epsilon)}}\;\left[- \ln \frac{P_\pri(z)P_\dec(y|z)}{P_\enc(z|y)}\right]$$

\vfill
To get gradients we must have that $z_\enc(y,\epsilon)$ is a smooth function of the encoder parameters and all probabilties must be a smooth function of $z$.

\vfill
Most commonly $\epsilon \in R^d$ with $\epsilon \sim {\cal N}(0,I)$ and
$$z^i_\enc(y,\epsilon) = \hat{z}^i_\enc(y) + \sigma^i\epsilon^i.$$

\vfill
Optimizing the encoder is tricky for discrete $z$.  Discrete $z$ is handled effectively in EM algorithms and in VQ-VAEs.

\slide{EM is Alternating Optimization of the VAE}

Expectation Maximimization (EM) applies in the (highly special) case where the exact posterior $P_{\pri,\dec}(z|y)$ is samplable and computable.
EM alternates exact optimization of $\enc$ and the pair $(\pri,\dec)$ in:
$$\mbox{VAE:}\;\;\;\;\;\;\; {\color{red} \pri^*,\dec^*} = \argmin_{\color{red} \pri,\dec} \min_{\color{red} \enc} E_{y,\;z \sim P_{\color{red} \enc}(z|y)}\;\;- \ln \frac{P_{\color{red} \pri}(z,y)}{P_{\color{red} \enc}(z|y)}$$

\vfill
$$\mbox{EM:}\;\;\;\;\;\; {\color{red} \pri^{t+1},\dec^{t+1}} =  \argmin_{\color{red} \pri,\dec}\;\;\;\;E_{y,\;z \sim P_{\color{red} \pri^t,\dec^t}(z|y)}\; - \ln P_{\color{red} \pri,\dec}(z,y)$$

\vfill
\centerline{\hspace{1em} Inference \hspace{6em} Update \hspace{2.5em}~}
\centerline{(E Step) \hspace{6em} (M Step) ~}
\centerline{ $P_\enc(z|y) = P_{\pri^{\color{red} t},\dec^{\color{red} t}}(z|y)$ \hspace{2.5em} Hold $P_\enc(z|y)$ fixed \hspace{0em}~}

\slide{END}

\end{document}
