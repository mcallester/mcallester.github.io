\input /users/davidmcallester/icloud/tex/SlidePreamble
\input /users/davidmcallester/icloud/tex/preamble

\begin{document}

{\Huge

  \centerline{\bf TTIC 31230, Fundamentals of Deep Learning}
  \bigskip
  \centerline{David McAllester, Winter 2022}
  \vfill
  \vfill
  \centerline{\bf Variational Auto-Encoders (VAEs)}
  \vfill
  \vfill


\slide{Image Compression and Image Generation}

Suppose that we want to model a population distribution on $y$, for example the distribution of ``natural images''.

\vfill
Shannon's source coding theorem implies that there exists a coding function with an inverse decoding function such that
decoding a random string samples an image from the population distribution on images.

\vfill
While we cannot optimally compress images, it can be useful to represent a population distribution on $y$ in terms of a latent (unabserved) variable $z(y)$ loosely analogous to a compressed form.

\slide{The Encoder, Decoder and the Prior}

Consider a probabilistic encoder algorithm $P_\enc(z|y)$ --- perhaps a stochastic image compression algorithm.

\vfill
The encoder $P_\enc(z|y)$ defines a joint probability distribution on pairs $(y,z)$.

\vfill
We will also introduce a decoder model (decompressor) $P_\dec(y|z)$ and a prior probability model $P_\pri(z)$ which are to be trained
using a cross-entropy loss to $P(y|z)$ and and $P(z)$ as defined by the encoder.

\slide{The ELBO}

{\huge
\begin{eqnarray*}
H(y,z) & = & H(y) + H(z|y) \; = \; H(z) + H(y|z) \\
\\
H(y) & = & H(z) + H(y|z) - H(z|y) \\
\\
& \leq & CE(P(z),P_\pri(z)) + CE(P(y|z),P_\dec(y|z)) -H_\enc(z|y) \\
\\
& = & E_{y\sim \pop, z \sim P_\enc(z|y)}\;-\ln \frac{P_\pri(z)P_\dec(y|z)}{P_\enc(z|y)}
\end{eqnarray*}
}
\vfill
The last line is the (negative) ELBO.  ELBO stands for ``Evidence Lower Bound'' but this terminaology is obscure and unhelpful.


\slide{The ELBO Loss}

We now interpret the ELBO as a loss on a given value $y$.

{\huge
\begin{eqnarray*}
H(y) & \leq & E_{y\sim \pop, z \sim P_\enc(z|y)}\;-\ln \frac{P_\pri(z)P_\dec(y|z)}{P_\enc(z|y)} \\
\\
& = & E_{y \sim \pop}\; {\cal L}_E(y) \\
\\
\\
{\cal L}_E(y) & = & E_{ z \sim P_\enc(z|y)}\;-\ln \frac{P_\pri(z)P_\dec(y|z)}{P_\enc(z|y)} \\
\end{eqnarray*}
}

\slide{A Third Fundamental Equation}

$$\pri^*,\dec^*  = \argmin_{\pri,\dec}\;\;E_{y}\;{\cal L}_E(y)$$

\vfill
\begin{eqnarray*}
  & &   E_y\;{\cal L}_E(y) \\
  \\
  & = & E_{y \sim \pop,\;z\sim P_\enc(z|y)} \;\; \ln \frac{P_\enc(z|y)}{P_\pri(z)P_\dec(y|z)} \\
  \\
  & = & E_y\;\left[-\ln \pop(y)\right] + KL(\pop(y)P_\enc(z|y),\;\;P_\pri(z)P_\dec(y|z)) \\
  \\
  & = & H(y) + KL(\pop(y)P_\enc(z|y),\;\;P_\pri(z)P_\dec(y|z))
\end{eqnarray*}

\slide{The ELBO Loss}

\begin{eqnarray*}
  E_y\;{\cal L}_E(y)
  & = & H(y) + KL(\pop(y)P_\enc(z|y),\;\;P_\pri(z)P_\dec(y|z))
\end{eqnarray*}

\vfill
Minimization occurs when the prior and the docoder satisfy

\vfill
$$P_\pri(z)P_\dec(y|z) = \pop(y)P_\enc(z|y)$$


\slide{The ELBO Loss}
\begin{eqnarray*}
  E_y\;{\cal L}_E(y)
  & = & H(y) + KL(\pop(y)P_\enc(z|y),\;\;P_\pri(z)P_\dec(y|z))
\end{eqnarray*}

\vfill
Minimizing gives
$$P_\pri(z)P_\dec(y|z) = \pop(y)P_\enc(z|y) = P(y,z)$$

\vfill
and hence
$${\cal L}_E(y) = \;\;E_z\;\ln \frac{P(z|y)}{P(z,y)} = E_z\;\ln \frac{P(z,y)/P(y)}{P(z,y)}\;\; = -\ln \pop(y)$$
\vfill
After optimization one can interpret ${\cal L}_E(y)$ as $-\ln \pop(y)$.

\slide{Optimizing the Encoder}

Although in princile the encoder need not be trained, it is sometimes jointly optimized with the prior and the decoder.

\vfill
$$\pri^*,\dec^*,\enc^* = \argmin_{\pri,\dec,\enc}\;\;E_{y,{\color{red} z\sim P_\enc(z|y)}}\;\left[- \ln \frac{P_\pri(z)P_\dec(y|z)}{P_\enc(z|y)}\right]$$

\vfill
This is necessary if want to interpret $z$ as some kind of ``understanding'' of the distribution on $y$ that facilitates representing the prior and the decoder.

\slide{Optimizing the Encoder}

$$\pri^*,\dec^*,\enc^* = \argmin_{\pri,\dec,\enc}\;\;E_{y,{\color{red} z\sim P_\enc(z|y)}}\;\left[- \ln \frac{P_\pri(z)P_\dec(y|z)}{P_\enc(z|y)}\right]$$

\vfill
Gradient descent on the encoder parameters must take into account the fact that we are sampling from the encoder.

\vfill
To handle this we sample noise $\epsilon$ from a fixed noise distribution and replace $z$ with a determinstc function $z_\enc(y,\epsilon)$

\vfill
$$\enc^*,\pri^*,\dec^* = \argmin_{\enc,\pri,\dec}\;\;E_{y,{\color{red} \epsilon,z=z_\enc(y,\epsilon)}}\;\left[- \ln \frac{P_\pri(z)P_\dec(y|z)}{P_\enc(z|y)}\right]$$

\slide{The Re-Parameterization Trick}

$$\enc^*,\pri^*,\dec^* = \argmin_{\enc,\pri,\dec}\;\;E_{y,{\color{red} \epsilon,z=z_\enc(y,\epsilon)}}\;\left[- \ln \frac{P_\pri(z)P_\dec(y|z)}{P_\enc(z|y)}\right]$$

\vfill
To get gradients we must have that $z_\enc(y,\epsilon)$ is a smooth function of the encoder parameters and all probabilties must be a smooth function of $z$.

\vfill
Most commonly $\epsilon \in R^d$ with $\epsilon \sim {\cal N}(0,I)$ and
$$z^i_\enc(y,\epsilon) = \hat{z}^i_\enc(y) + \sigma^i\epsilon^i.$$

\vfill
Optimizing the encoder is tricky for discrete $z$.  Discrete $z$ is handled effectively in EM algorithms and in VQ-VAEs.

\slide{EM is Alternating Optimization of the ELBO Loss}

Expectation Maximimization (EM) applies in the (highly special) case where the exact posterior $P_{\pri,\dec}(z|y)$ is samplable and computable.
EM alternates exact optimization of $\enc$ and the pair $(\pri,\dec)$ in:
$$\mbox{VAE:}\;\;\;\;\;\;\; {\color{red} \pri^*,\dec^*} = \argmin_{\color{red} \pri,\dec} \min_{\color{red} \enc} E_{y,\;z \sim P_{\color{red} \enc}(z|y)}\;\;- \ln \frac{P_{\color{red} \pri,\dec}(z,y)}{P_{\color{red} \enc}(z|y)}$$

\vfill
$$\mbox{EM:}\;\;\;\;\;\; {\color{red} \pri^{t+1},\dec^{t+1}} =  \argmin_{\color{red} \pri,\dec}\;\;\;\;E_{y,\;z \sim P_{\color{red} \pri^t,\dec^t}(z|y)}\; - \ln P_{\color{red} \pri,\dec}(z,y)$$

\vfill
\centerline{\hspace{1em} Inference \hspace{6em} Update \hspace{2.5em}~}
\centerline{(E Step) \hspace{6em} (M Step) ~}
\centerline{ $P_\enc(z|y) = P_{\pri^{\color{red} t},\dec^{\color{red} t}}(z|y)$ \hspace{2.5em} Hold $P_\enc(z|y)$ fixed \hspace{0em}~}

\slide{END}

\end{document}
