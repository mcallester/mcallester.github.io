\input ../SlidePreamble
\input ../preamble


\begin{document}

{\Huge

  \centerline{\bf TTIC 31230, Fundamentals of Deep Learning}
  \bigskip
  \centerline{David McAllester, Autumn 2022}
  \vfill
  \vfil
  \centerline{Progressive VAEs}
  \vfill
  \vfill

\slide{Progressive VAEs}
These slides were written ``gedanken'' (as a thought experient) while teaching this class in 2021.  They were not based on any paper.

\vfill
While written independently of diffusion models, these slides provide a good theoretical framework for diffusion models (currently very hot).

\vfill
The original motivation for these slides was to provide a theoretically clean approach to a multi-layer VQ-VAE.


\slide{Progressive VAEs}

We consider a VAE with layers of latent variables $z_1,\dots,z_{L}$ and a population distribution on an observable variable $y$.

\vfill
The encoder will define $P_\enc(z_1|y)$ and $P_\enc(z_{\ell+1}|z_\ell)$.

\vfill
The decoder will define $P_\dec(z_{\ell-1}|z_\ell)$ and $P_\dec(y|z_1)$.

\vfill
Following VQ-VAE, we will train the encoder and the decoder independent of any prior.

\vfill
We then train a prior on the top layer latent variable.  The top level prior and decoder allow us to sample $y$ from the model.

\slide{Phase One Training}

We train a encoders and decoders $\enc_1,\dec_1$, $\ldots$, $\enc_L,\dec_L$ where the distribution on $z_1,\ldots,Z_L$ is defined by $y$ and the encoder.

$$\enc_1^*,\dec_1^* =\argmin_{\enc_1,\dec_1}\;E_{y,z_1} \left[-\ln P_{\dec_1}(y|z_1)\right]$$

\vfill
$$\enc_{\ell+1}^*,\dec_{\ell+1}^* = \argmin_{\enc_{\ell+1},\dec_{\ell+1}}\;E_{z_\ell,z_{\ell+1}}\;\left[-\ln P_{\dec_{\ell+1}}(z_{\ell-1}|z_\ell)\right]$$

\vfill
If these encoders and decoders share parameters the shared parameters are influenced by all of the above training losses (this observation was added
after seeing DALLE-2's diffision model).

\slide{Phase Two Training}

$$\pri^*   =  \argmin_{\pri}\;E_{z_L}\left[-\ln P_\pri(z_L)\right]$$

\vfill
Because of the autonomy of the encoder, the universality assumption implies that we get a perfect model of the population distribution on $y$.

\vfill
Given the prior and the decoder we can sample images.

\slide{END}

\end{document}
