\documentclass{article}
\input ../preamble
\parindent = 0em

%\newcommand{\solution}[1]{}
\newcommand{\solution}[1]{\bigskip {\color{red} {\bf Solution}: #1}}

\begin{document}

\centerline{\bf TTIC 31230 Fundamentals of Deep Learning}
\bigskip
\centerline{\bf Regularization and  Generalization Problems}

\bigskip
{\bf PAC-Bayes Background} Consider any probability distribution $P(h)$ over a discrete class ${\cal H}$.
Assume $0 \leq {\cal L}(h,x,y) \leq \lmax$. Define
\begin{eqnarray*}
{\cal L}(h)  & = &  E_{(x,y)\sim \mathrm{Pop}}\;{\cal L}(h,x,y) \\
\\
\hat{{\cal L}}(h) & = & E_{(x,y)\sim \mathrm{Train}}\;{\cal L}(h,x,y)
\end{eqnarray*}
We now have the theorem that with probability
at least $1-\delta$ over the draw of training data the following holds simultaneously for all $h$.
$${\cal L}(h) \leq \frac{10}{9}\parens{\hat{\cal L}(h) + \frac{5 L_\mathrm{max}}{N}\parens{\ln \frac{1}{P(h)} + \ln\frac{1}{\delta}}} \;\;\;(1)$$

This motivates
$$h^* = \argmin_h \;\hat{{\cal L}}(h) + \frac{5\lmax}{N_{\mathrm{train}}} \ln\frac{1}{P(h)}\;\;\;\;(2)$$
The Bayesian maximum a-posteriori (MAP) rule is
$$h^* = \argmax_h\;P(h)\prod_{(x,y) \in \mathrm{Train}}\;P(y|x,h)\;\;\;\;(3)$$

\medskip
{\bf Problem 1. The Meaning of a PAC-Bayes Prior.}  Consider an optimal hypothysis for the population distribution.
$$h^* = \argmin_h\;E_{\tuple{x,y} \sim \pop}\;{\cal L}(h,x,y)$$
Equation (1) holds for any prior $P$.  Consider two priors $P_{\mathrm{lucky}}$ and $P_{\mathrm{unlucky}}$ where we have
$$P_{\mathrm{lucky}}(h^*) >> P_{\mathrm{unlucky}}(h^*)$$
Explain how equation (1) can hold for both of these priors.

\solution{
  The prior $P$ should be interpreted as saying which hyopthesis will be measured accurately first as $N_{\mathrm{train}}$ increases.
  We can interpret $P$ as a ``guess'' as to where we think the good hypotheses are.
  The prior $P$ is not stating any actual propability of where the optimal hypothesis is.
  We get accurate measurements first
  for the hypotheses $h$ for which $P(h)$ is large.
  For $P_{\mathrm{unlucky}}$ we get an accurate measureent of ${\cal L}(h^*)$ only much later than we do under $P_{\mathrm{lucky}}$.
}

\medskip
{\bf Problem 2. Code Length as Probability.}
Assume that a model $h$ is represented by a (compressed) file $|h|$ bits long.  Files have a specific length and no file is a proper prefix of any other file.
We say that the set of file bit strings is {\bf prefix free}.

\medskip
(a) Show that for any prefix-free representation of files as bit strings we have
the following Kraft inequality where the sum is over all possible files (of unbounded size).
$$\sum_h\;2^{-|h|} \leq 1$$

\solution{Consider a probabilistic process which flips an unbiased coin to determine a next bit until it generates a legal file bit string at which point
  it outputs that file.  This process generates file $h$ with probability $2^{-|h|}$ and, by the prefix-free property, all files can be generated by this process.
  However, it is possible that this process never terminates 
  The Kraft inequality then follows from $\sum_h P(h) \leq 1$ where we also have $P(\mathrm{divergence}) + \sum_h\;P(h) = 1$.
}

\medskip
(b) rewrite (1) in terms of $|h|$ where we take $P(h) = 2^{-|h|}$.

\solution{
  $${\cal L}(h) \leq \frac{10}{9}\parens{\hat{\cal L}(h) + \frac{5 L_\mathrm{max}}{N_{\mathrm{train}}}\parens{(\ln 2)|h| + \ln\frac{1}{\delta}}} \;\;\;(1)$$
}

\medskip
{\bf Problem 3. Comparing Bayesian MAP to PAC-Bayes}
For ${\cal L}(h,x,y) = - \ln P(y|x,h)$ (cross entropy loss) rewrite (2) so as to be as similar to (3) as possible.
Note that (1) holds independent of any ``truth'' of the ``prior'' $P$.

\solution{
  \begin{eqnarray*}
    & & \argmin_h \;\left(\frac{1}{N} \sum_{(x,y)\sim \mathrm{Train}}\;-\ln P(y|x,h)\right) + \frac{5\lmax}{N} \ln\frac{1}{P(h)} \\
    \\
    & =  & \argmax_h \;\left(\frac{1}{N} \sum_{(x,y)\sim \mathrm{Train}}\;\ln P(y|x,h)\right) + \frac{5\lmax}{N} \ln P(h) \\
    \\
    & =  & \argmax_h \left(\sum_{(x,y)\sim \mathrm{Train}}\;\ln P(y|x,h)\right) + 5\lmax \ln P(h) \\
    \\
    & =  & \argmax_h \ln \left(P(h)^{5\lmax}\prod_{(x,y)\sim \mathrm{Train}}\; P(y|x,h)\right) \\
    \\
    & = & \argmax_h\;  P(h)^{5\lmax}\prod_{(x,y)\sim \mathrm{Train}}\; P(y|x,h)
  \end{eqnarray*}
}

\bigskip
{\bf Problem 4. Finite Precision Parameters.}

\medskip
(a) Consider a model where the parameter vector $\Phi$ has $d$ parameters each of which is represented by a 16 bit floating point number.
Express the bound (1) in terms of the dimension $d$ assuming all parameter vectors are equally likely.

\solution{
  $${\cal L}(\Phi) \leq \frac{10}{9}\parens{\hat{\cal L}(\Phi) + \frac{5 L_\mathrm{max}}{N}\parens{16d\ln 2 + \ln\frac{1}{\delta}}}$$
}

\medskip
(b) Assume a variable precision representation of numbers where $\Phi[i]$ is given with $|\Phi[i]|$ bits.  Express the bound (1) as a function of $\Phi$ assuming that $P(\Phi)$
is defined so that each parameter is selected independently and that
$$P(\Phi[i]) = 2^{-|\Phi[i]|}$$.

\solution{
  \begin{eqnarray*}
    {\cal L}(h) & \leq & \frac{10}{9}\parens{\hat{\cal L}(h) + \frac{5 L_\mathrm{max}}{N_{\mathrm{train}}}\parens{\ln 2|\Phi| + \ln\frac{1}{\delta}}} \\
    \\
    |\Phi| & = & \sum_i |\Phi[i]|
  \end{eqnarray*}
}

\medskip
(c) Repeat part (a) but for a model with $I$ parameters represented by
$\Phi[i] = \Psi[k[i]]$ where $k[i]$ is an integer index with $0 \leq k[i] \leq K-1$ and where $\Psi[k]$ is a $b$-bit floating point number.  We define a prior probability on models
by selecting each $k[i]$ uniformily from the integers from $0$ to $K-1$ and selecting $\Psi[k]$ uniformly from all $b$-fit floating point numbers.

\solution{
  $${\cal L}(h) \leq \frac{10}{9}\parens{\hat{\cal L}(h) + \frac{5 L_\mathrm{max}}{N}\parens{Kb\ln 2 + I\ln k + \ln\frac{1}{\delta}}}$$
  For $I >> K$ this is a much tighter bound than using floating point or even integer representations of parameters.  It is a much more compact representaiton
  of the parameters.
}

\medskip
~{\bf Problem 5. Implicit Bias for SGD on Least Squares Regression.}  Consider a hypothesis space ${\cal H}$ and a learning algorithm ${\cal A}$
that maps trainging data to a hyothesis in ${\cal H}$.  Write ${\cal A}(\mathrm{Train})$ for the result of running algorithm ${\cal A}$ on training data $\mathrm{Train}$.
Also consider a given population distribution $\pop$ where $\mathrm{Train}$ consists of $N_{\mathrm{train}}$ samples drawn independently from $\pop$.
Let $P_{{\cal A},\pop}(h)$ be the probability that ${\cal A}(\mathrm{Train}) = h$
when $\mathrm{Train}$ is drawn at random from $\pop$.  The propbability distribution $P_{{\cal A},\pop}$ is independent of any particular training sample
and can be used as a PAC-Bayes prior on ${\cal H}$. A PAC-Bayes prior represents a learning bias.
The distribution $P_{{\cal A},\pop}$ is the {\bf implicit bias} of algorithm ${\cal A}$ run on population $\pop$.

\medskip
In this problem we consider the implicit bias of the SGD algorithm applied to least squares regression in the case where there are many more parameters than data points.
Least squares regression is defined by
$$\Phi[J]^* = \argmin_\Phi \; E_{\tuple{x,y} \sim \mathrm{Train}}\;(\Phi[J]x[J] - y)^2$$
To solve this optimization problem we consider using SGD where $\Phi$ is initialized to the zero vector and we then apply the update
\begin{eqnarray*}
  \Phi_{t+1} & = & \Phi_t - \eta \nabla_\Phi\;(\Phi^\top x_y - y)^2 \\
  \\
  & = & \Phi_t - 2 \eta (\Phi^\top x_t - y)x_t
\end{eqnarray*}

\medskip
(a) In the case where $N_{\mathrm{train}} < d$, where $d$ is the dimension of $\Phi$ and $x$, define a linear proper subspace of $R^d$ such that we are guaranteed
that $\Phi_t$ is in that space for al $t$.

\solution{
  Since every update is in the direction of some input vector $x_t$ in the training data, SGD maintains the invariant that $\Phi_t$ is some linear combination
  of the training vectors $x_1,\ldots x_{N_{\mathrm{train}}}$. Since $N_{\mathrm{train}} < d$ the span of the training vectors must be a proper subspace of $R^d$.
}

(b) Assume that the training vectors $x_1,\ldots,x_{N_{\mathrm{train}}}$ are linearly independent.  In this case it can be shown that there exists a unique solution $\Phi^*$ in the space
spanned by these vectors for which the square loss of the training data is zero (if these were not independent then we would have more training points than degrees of freedom
in the space spanned by the input vectors). Let $b_1,\ldots,b_{N_{\mathrm{train}}}$ be an orthonormal basis
for the space spanned by the input vectors. For any $\Phi \in R^d$ define the projection of $\Phi$ into the subspace by
\begin{eqnarray*}
  \Phi_\pi & = & \sum_i\;(\Phi^\top b_i)b_i \\
  \\
  \Phi_\bot & = & \Phi - \Phi_\pi
\end{eqnarray*}

The convergence theorem for SGD now gives that SGD on least squares regression will converge in the limit to $\Phi^*$.  Show that SGD applied to least
squared regression has a form of implicit bias similar to $L_2$ regression in that the result $\Phi^*$ is the least norm point in $R^d$ for which
the square loss of the training data is zero.

\solution{
  Consider any $\Phi \in R^d$ for which the training loss is zero.  The projection $\Phi_\pi$ must also have zero training loss because
  each training vector can be written as a linear combination of basis vectors and $\Phi$ and $\Phi_\pi$ have the same inner product with each basis vector.
  Therefore $\Phi_\pi = \Phi^*$.  Futhermore $\Phi = \Phi_\pi + \Phi_\bot$ and
  $$||\Phi||^2 = ||\Phi_\pi||^2 + ||\Phi_\bot||^2 = ||\Phi^*||^2 + ||\Phi_\bot||^2$$
  which gives that $||\Phi|| \geq ||\Phi^*||$ as desired.
}


\bigskip
{\bf Problem 6. Generalization Bounds for the realizable case. (25 points)}

Consider a finite hyothesis class ${\cal H}$ and a population distribution $\pop$ on pairs $\tuple{x,y}$ such that for $\tuple{x,y}$ drawn from the
population and $h \in {\cal H}$ we have that $h$ makes a prediction for $y$ which we will write as $h(x)$.  The error rate of hypothesis $h$ on the population is defined by
$$\mathrm{Err}_{\pop}(h) = P_{\tuple{x,y} \sim \pop}(h(x) \not = y)$$
We draw a training sample $\mathrm{Train}$ consisting of $N_{\mathrm{Train}}$ pairs $\tuple{x,y}$ drawn IID from the population.
$$\mathrm{Err}_{\mathrm{train}}(h) = \frac{1}{N_{\mathrm{train}}} \sum_{\tuple{x,y} \in \mathrm{Train}}\;\bbone(h(x) \not = y)$$

\medskip
(a) For a given hypothesis $h$ with error rate $\epsilon$ what is the probability that $\mathrm{Err}_{\mathrm{train}}(h) = 0$.

\solution{
  $(1-\epsilon)^{N_{\mathrm{train}}}$
}

\medskip
(b) We now consider a fixed threshold $\epsilon$ and consider the hypotheses $h$ satisfying $\mathrm{Err}_{\pop} \geq \epsilon$.
We will call these the ``bad'' hypotheses. 

\medskip
The simple form of the union bound is
$$P(A \cup B) \leq P(A) + P(B)$$
This can be generalized to
$$P(\exists z\; Q(z)) \leq \sum_z P(Q(z))$$
where $Q(z)$ is any statement about $z$.  

\medskip
Use your answer to (a) and the union bound to give an upper bound on the probability that 
there exists a bad hypothesis $h$ with $\mathrm{Err}_{\mathrm{train}}(h) = 0$.
You solution should be stated in terms of $\epsilon$, the number of elements $|{\cal H}|$ of ${\cal H}$,
and the number of training pairs $N_{\mathrm{train}}$.
Simplify your solution using the inequality $1-\epsilon \leq e^{-\epsilon}$.

\solution{
  \begin{eqnarray*}
    & & P(\exists h\; \;\mathrm{Err}_{\pop} \geq \epsilon,\; \mathrm{Err}_{\mathrm{train}}(h) = 0) \\
    \\
    & \leq & \sum_{h:\;\mathrm{Err}_{\pop}(h) \geq \epsilon}\;P(\mathrm{Err}_{\mathrm{train}}(h) = 0) \\
    \\
    & \leq & |{\cal H}| (1-\epsilon)^{N_{\mathrm{train}}} \\
    \\
    & \leq & |{\cal H}| e^{-N_{\mathrm{train}}\epsilon} \\
  \end{eqnarray*}
}

\medskip
(c) Now consider a small positive number $\delta$ and solve for $\epsilon$ such that the probability that a bad hypothesis has zero training error is less than $\delta$.
Your solution gives a value of $\epsilon$  such that with probability $1-\delta$ over the draw of the training error all hypothesis with zero training error
have population error no lareger than $\epsilon$.

\solution{
  \begin{eqnarray*}
    \delta & = & |{\cal H}| e^{-N_{\mathrm{train}}\epsilon} \\
    \\
    \epsilon & = & \frac{\ln |{\cal H}| + \ln \frac{1}{\delta}}{N_{\mathrm{train}}}
  \end{eqnarray*}
  }
    
\bigskip
{\bf Problem 7.} This problem is on robust loss functions.  With a robust loss one identifies ``outliers'' in the data and ``gives up'' on modeling the outliers.
In particular we can consider the following bounded version of cross-entropy loss
\begin{eqnarray*}
  {\cal L}(\Phi,x,y) & = & \lmax \;\tanh \left(\frac{- \ln P_\Phi(y|x)}{\lmax}\right) \\
  \\
  \tanh(z) & = & \frac{2}{1+e^{-2z}} - 1.
\end{eqnarray*}
For $z \geq 0$ we have $\tanh(z) \geq 0$ and we have that the above robust loss is non-negative and can never be larger than $\lmax$.

\medskip
{\bf (a)}  Consider the function $\lmax \;\tanh(\frac{z}{\lmax})$. Use a first order Taylor expansion of the $\tanh$ function about zero to show that for $|z| << \lmax$ we have
$$\lmax\; \tanh\left(\frac{z}{\lmax}\right) \approx z$$
This implies that the robust cross entropy loss is essentially equal to the cross entropy loss when the cross entropy loss is small compared to $\lmax$.

\solution{
  The first order Taylor expansion of the $\tanh$ function about zero is
  $$\tanh(u) \approx u$$ yielding the desired result.
}

\medskip
{\bf (b)} Consider the case where the cross-entropy loss is large compared to $\lmax$.  For $z >> 1$ we have that the derivative $\tanh'(z)$ is essentially zero.
What parameter update is made on a training point whose cross entropy loss is large compared to $\lmax$ if we model $\tanh'(z) = 0$ in such cases.

\solution{
  The update on a data point $(x,y)$ is
  $$\Phi^{t+1} = \Phi^t - \eta \nabla_\Phi {\cal L}(\Phi,x,y)$$
  At a point where the derivative of the sigmoid is essentially zero this update will be essentially zero.  So ``outliers'' do not effect the model parameters.
}

\medskip
{\bf (c)} Look up the PAC-Bayesian generalization guarantee that is stated in terms of the $L_2$ norm of the weight vector.
Explain why the robust loss function comes with a better PAC-Bayesian generalization guarantee.  Intuitively, the improvement in generalization
is due to insensitivity to ``outliers'' (or things the model cannot understand).

\solution{
  The $L_2$ PAC-Bayeisan guarantee in the notes is
  $${\cal L}_\sigma(\Phi) \leq \frac{10}{9}\left(\hat{\cal L}_\sigma(\Phi) + \frac{5\lmax}{N}\left(\frac{||\Phi -\Phi_{\mathrm{init}}||^2}{2\sigma^2} + \ln \frac{1}{\delta}\right)\right)$$
  Reducing $\lmax$ both reduces $\hat{L}_\sigma(\Phi)$ and reduces the penalty for the model complexity (the norm squared of the distance from the initialization).
}

\medskip
{\bf (d)} Curriculum learning is the idea that one first learns how to solve easy problems and then gradually learns ever harder problems. At a high informal level describe a learning algorithm based on the above robust loss function
which can be intuitively motivated as curriculum learning.


\solution{
  Easier problems should correspond to cases where the cross entropy loss can be made small.  So setting $\lmax$ to be smallish will focus the model on the easy problems while ignoring the hard problems.
  Gradually increasing $\lmax$ will gradually pay more attention to the harder problems.

  \medskip
  Another possible answer is that holding $\lmax$ fixed will focus first on the easy problems ignoring the hard problems but as the understanding of easy problems improves the harder problems become easy problems and
  we automatically gradually pay attention to harder and harder problems even with a fixed value of $\lmax$.
}

\bigskip
{\bf Problem 8.} This problem is on PAC-Bayes bounds for classifiers built on CLIP using {\bf prompt engineering}.  CLIP is a joint probability model on images
and English descriptions (image captions).  Clip is trained on a large corpus of captioned images drawn from the web and defines a probability distribution over captions $c$ given an image $x$.
We can use CLIP for image classification (as in ImageNet) using ``prompt engineering''.  A ``prompt'' is caption specific to an image label.  For example the caption ``this is an image of a cat'' for the label ``cat'' or ``this is an image of a dog''
for the label ``dog''. For each image class $y$ we have a prompt (hypothetical caption) $c(y)$.
We can then label an image $x$ with class $\hat{y}$ using the rule
$$\hat{y}(x) = \argmax_y \;P_{\mathrm{CLIP}}(c(y)|x)$$
Suppose that we search (somehow) over the captions $c(y_1),\ldots,c(y_n)$ assigned to the $n$
image classes $y_1,\ldots,y_n$ to find a set of captions minimizing the error rate (0-1 loss) on a set of $N$ labeled training images.  Let $\hat{\cal L}$ be the error rate on the training data.
Also suppose that CLIP assigns a prior probability $P_{\mathrm{CLIP}}(c)$ to any caption $c$ independent of any image.  Consider the PAC-Bayes bound on generalization loss for predictive rule $h$
where the bound is guaranteed to hold for all $h$ with probability at least $1-\delta$.
$${\cal L}(h) \leq \frac{10}{9}\left(\hat{{\cal L}}(h) + \frac{5\lmax}{N_\mathrm{Train}}\left(- \ln P(h) +\ln\frac{1}{\delta}\right)\right)$$
Apply this rule to the CLIP image classifier using CLIP's ``prior probability'' on the caption space.

\solution{
  $${\cal L}(h) \leq \frac{10}{9}\left(\hat{{\cal L}}(h) + \frac{5}{N}\left(\left(\sum_y - \ln P_{\mathrm{CLIP}}(c(y))\right) +\ln\frac{1}{\delta}\right)\right)$$

  I am not proposing that searching over all captions is a good idea.  Some narrower prior is called for.
}


\bigskip
    {\bf Problem 1: Generalization Bounds and The Lottery Ticket Hypothesis.}  Suppose that we want to construct a linear classifier (a linear threshold unit) for binary classification defined by

    $$\hat{y}_\alpha(x) = \left\{\begin{array}{ll} 1 & \mbox{if $\sum_{i = 1}^d \alpha_i f_i(x) >= 0$} \\ \\ -1 & \mbox{otherwise}\end{array}\right.$$

    where each $\alpha_i$ is a scalar weight, $f_i(x)$ is a scalar value, and the functions $f_i$ are (random) features constructed independent of any observed values of $x$ or $y$.

    We will assume a population distribution $\pop$ of pairs $\tuple{x,y}$ with $y \in \{-1,1\}$ and a training set $\mathrm{Train}$ of $N$ pairs drawn IID from $\mathrm{pop}$.

    We can define both test and train losses (error rates).

    \begin{eqnarray*}
      \hat{\cal L}(\alpha) & = & E_{{x,y} \sim \train}\;\mathbf{1}[\hat{y}_\alpha(x_i) \neq y_i] \\
      \\
      {\cal L}(\alpha) & = & E_{{x,y} \sim \pop}\;\;\mathbf{1}[\hat{y}_\alpha(x_i) \not = y_i] \\
    \end{eqnarray*}

    Assume finite precision arithmetic so that we have discrete rather than continuous possible values of $\alpha$.
    The course slides state that for any (prior) distribution $P$ on the values of $\alpha$ we have that with probability at least $1-\delta$
    over the draw of the training data the following holds simultneously for all $\alpha$.

    $${\cal L}(\alpha) \leq \frac{10}{9}\left(\hat{\cal L}(\alpha) + \frac{5L_\mathrm{max}}{N}\left(-\ln P(\alpha) + \ln \frac{1}{\delta}\right)\right)$$

    We will now incorporate the lottery ticket hypothesis into the prior distribution on $\alpha$ by assuming that low training error can be achieved with some small subset of the random features.
    More formally, we define a prior favoring sparse $\alpha$ --- cases where most weights are zero.
    
    \medskip
    (a) To define $P(\alpha)$, first define a prior probability distribution $P(s)$ over the number $s$ of nonzero values.

    \solution{
      There are of course many solutions.  A uniform distribution on the numbers from $1$ to $d$ will work giving $P(s) = 1/d$.  Another
      possibility is $P(s) = \epsilon(1-\epsilon)^s$ which defines a distribution on all $s \geq 0$.}
    
    \medskip
    (b) Given a specified number $s$ of nonzero values, define a probability distribution $P(U|s)$ where $U$ is a subset of the random features with $|U| = s$.

    \solution{A reasonable choice here is a uniform distribution on the $\binom{d}{s}$ possibilities giving $P(U|s) = 1/\binom{d}{s}$.}
    
    \medskip
    (c) Assuming that each nonzero value is represented by $b$ bits, give a probability distribution over $P(\alpha|U,s)$.

    \solution{Here we can use the uniform distribution on the $2^{bs}$ ways of assigning numbers to the $s$ nonzero weights in $\alpha$ giving
      $P(\alpha|U,s) = P(\alpha|U) = 2^{-bs}$.}
      

    \medskip
    (d) Combine (a), (b) and (c) to define $P(\alpha)$.

    \solution{Under $P(s) = 1/d$ we get
      $P(\alpha) = \frac{1}{d\binom{d}{s}2^{bs}}$
      and using $\binom{d}{s} \leq d^s$ we get $P(\alpha) \geq \frac{1}{dd^s2^{bs}} = \frac{1}{d^{s+1}2^{bs}}$.

      \medskip
      Under $P(s) = \epsilon(1-\epsilon)^s$ we get
      $P(\alpha) = \frac{\epsilon(1-\epsilon)^s}{\binom{d}{s}2^{bs}}$
      and using $\binom{d}{s} \leq d^s$ 
      $P(\alpha) \geq \frac{\epsilon(1-\epsilon)^s}{d^s 2^{bs}}$}
      

    \medskip
    (e) Plug your answer to (c) into the above generalization bound to get a bound in terms of the number of random features $d$, the number $s$ of nonzero values of $\alpha$,
    and the number $b$ of bits used to represent each nonzero value and any additional parameters used in defining your distributions.

    \solution{Under $P(s) = 1/d$ we get
      \begin{eqnarray*}
        {\cal L} & \leq & \frac{10}{9}\left(\hat{\cal L} + \frac{5}{N}\left(\ln d + \ln\binom{d}{s} +  sb\ln 2 + \ln\frac{1}{\delta}\right)\right) \\
        \\
        & \leq & \frac{10}{9}\left(\hat{\cal L} + \frac{5}{N}\left((s+1)\ln d + sb\ln 2 + \ln\frac{1}{\delta}\right)\right)
      \end{eqnarray*}

      Under $P(s) = \epsilon(1-\epsilon)^s$ we get
      \begin{eqnarray*}
        {\cal L} & \leq & \frac{10}{9}\left(\hat{\cal L} + \frac{5}{N}\left(\ln \frac{1}{\epsilon} + s\ln\frac{1}{1-\epsilon} + \ln\binom{d}{s} +  sb\ln 2 + \ln\frac{1}{\delta}\right)\right) \\
        \\
        & \leq & \frac{10}{9}\left(\hat{\cal L} + \frac{5}{N}\left(\ln \frac{1}{\epsilon} + s\ln\frac{1}{1-\epsilon} + s\ln d + sb\ln 2 + \ln\frac{1}{\delta}\right)\right)
      \end{eqnarray*}

      Note that in either case the bound is logarithmic in $d$ allowing $d$ to be extremely large.  The choice of the uniform distribution for $s$ is simpler and gives a completely satisfactory result.
      However there are regimes in which the second prior on $s$ is very slightly better.
      }

\end{document}
