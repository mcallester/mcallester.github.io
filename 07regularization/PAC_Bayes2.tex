\input ../../SlidePreamble
\input ../../preamble

\begin{document}

{\Huge


\centerline{\bf TTIC 31230, Fundamentals of Deep Learning}
\bigskip
\centerline{David McAllester, Autumn 2020}

\vfill
\centerline{\bf Learning Theory II}
\vfill
\vfill
\centerline{\bf The PAC-Bayes Guarantee}

\vfill
\vfill

\slide{The PAC-Bayes Guarantee}

Let $p$ be any ``prior'' and $q$ be any ``posterior'' on any  (possibly continuous) model space.
Define
\begin{eqnarray*}
  L(q) & =  &\expectsub{h \sim q}{L(h)} \\
  \\
  \hat{L}(q) & =  &\expectsub{h \sim q}{\hat{L}(h)}
\end{eqnarray*}


\vfill
For any $p$ and any $\lambda > \frac{1}{2}$, with probability
at least $1-\delta$ over the draw of the training data, the following holds simultaneously for all $q$.
\vfill
$$L(q) \leq \frac{1}{1-\frac{1}{2\lambda}}\parens{\hat{L}(q) + \frac{\lambda \lmax}{N_\mathrm{Train}}\parens{KL(q,p) + \ln \frac{1}{\delta}}}$$

\slide{Adding Noise Simulates Limiting Precision}

Assume $0 \leq {\cal L}(\Phi,x,y) \leq \lmax$.

\vfill
Define:
\begin{eqnarray*}
{\cal L}_\sigma(\Phi) & = & E_{\;(x,y)\sim \pop,\;\epsilon \sim {\cal N}(0,\sigma)^d}\;{{\cal L}(\Phi+\epsilon,x,y)} \\
\\
\hat{{\cal L}}_\sigma(\Phi) & = & E_{\;(x,y)\sim \mathrm{Train},\;\epsilon \sim {\cal N}(0,\sigma)^d}\;{{\cal L}(\Phi+\epsilon,x,y)} \\
\end{eqnarray*}

\vfill
Theorem: With probability at least $1-\delta$ over the draw of training data the following holds {\bf simultaneously} for all $\Phi$.
\begin{eqnarray*}
   {\color{red} {\cal L}_\sigma(\Phi)} & {\color{red} \leq} & {\color{red} \frac{10}{9}\parens{\hat{{\cal L}}_\sigma(\Phi)
   + \frac{5 \lmax}{N_\mathrm{Train}}\parens{\frac{||\Phi-\Phi_{\mathrm{init}}||^2}{2\sigma^2} + \ln \frac{1}{\delta}}}}
\end{eqnarray*}

\slide{Non-Vacuous Generalization Guarantees}

Model compression has recently been used to achieve ``non-vacuous'' PAC-Bayes generalization guarantees for ImageNet classification
--- error rate guarantees less than 1.

\vfill
Non-Vacuous PAC-Bayes Bounds at ImageNet Scale.

\bigskip
Wenda Zhou, Victor Veitch, Morgane Austern, Ryan P. Adams, Peter Orbanz

\bigskip
ICLR 2019

\slide{END}

}
\end{document}
