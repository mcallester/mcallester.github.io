\input ../SlidePreamble
\input ../preamble


\begin{document}

{\Huge


\centerline{\bf TTIC 31230, Fundamentals of Deep Learning}
\bigskip
\centerline{David McAllester, Autumn 2022}

\vfill
\centerline{\bf Implicit Regularization}
\vfill
\vfill

\slide{Implicit Regularization}

Any stochastic learning algorithm, such as SGD, determines a stochastic mapping from training data to models.

\vfill
The algorithm, especially with early stopping, can implicitly incorporate a preference or bias for models.

\slide{Implicit Regularization in Linear Regression}

Linear regression (minimizing the $L_2$ loss of a linear predictor) where we have more parameters than data points
has many solutions.

\vfill
But SGD converges to the minimum norm solution ($L_2$-regularized solution) without the need for explicit regularization.

\slide{Implicit Regularization in Linear Regression}

For linear regression SGD maintains the invariant that $\Phi$ is a linear combination of the (small number of) training vectors.

\vfill
Any zero-loss (squared loss) solution can be projected on the span of training vectors to give a smaller (or no larger) norm solution.

\vfill
It can be shown that when the training vectors are linearly independent any zero loss solution in the span of the training vectors is a least-norm solution.

\slide{Implict Priors}

Let $A$ be any algorithm mapping a training set $\train$ to a probability density $p(\Phi|\train)$.

\vfill
For example, the algorithm might be SGD where we add a small amount of noise to the final parameter vector so that $p(\Phi|\train)$ is a smooth density.

\vfill
But in general we can consider any leaning algorithm that produces a smooth density $p(\Phi|\train)$.

\slide{Implicit Priors}

Drawing $\train$ from $\pop^N$ and $\Phi$ from $P(\Phi|\train)$ defines a joint distribution on $\train$ and $\Phi$.  We can take the marginal distribution on $\Phi$
to be a prior distribution (independent of any training data).

\vfill
$$p(\Phi) = E_{\parens{\mathrm{Train} \sim \pop^N}}\;\;p(\Phi\;|\train)$$

\vfill
It can be shown that the implicit prior $p(\Phi)$ is an optimal prior for the PAC-Bayesian generalization guarantees applied to the algorithm defining $p(\Phi|\train)$

\vfill
\slide{A PAC-Bayes Analysis of Implicit Regularization}

\begin{eqnarray*}
{\cal L}(\train) & = & E_{\tuple{x,y} \sim {\color{red}  \pop}, \;\;\Phi \sim p(\Phi|\train)}\;{\cal L}(\Phi,x,y) \\
\\
\hat{\cal L}(\train) & = & E_{\tuple{x,y} \sim {\color{red} \train}, \;\;\Phi \sim p(\Phi|\train)}\;{\cal L}(\Phi,x,y)
\end{eqnarray*}

\slide{A PAC-Bayes Analysis of Implicit Regularization}

With probability at least $1-\delta$ over the draw of $\train$ we have
\vfill
{\huge
\begin{eqnarray*}
{\cal L}(\train) & \leq & \frac{10}{9}\parens{ \hat{\cal L}(\train) + \frac{5\lmax}{N_\mathrm{Train}}\parens{KL(p(\Phi|\train),p(\Phi)})+ \ln\frac{1}{\delta}} \\
\\
\\
& = & \frac{10}{9}\parens{ \hat{\cal L}(\train) + \frac{5\lmax}{N_\mathrm{Train}}\parens{I(\Phi,\train)+ \ln\frac{1}{\delta}}}
\end{eqnarray*}
}
\vfill
There is no obvious way to calculate this guarantee.

\vfill
However, it can be shown that $p(\Phi)$ is the optimal PAC-Bayeisan prior for given algorithm run on training data data drawn from $\pop^N$.

\slide{END}

}
\end{document}
