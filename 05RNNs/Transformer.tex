\input ../SlidePreamble
\input ../preamble


\begin{document}

{\Huge
  \centerline{\bf TTIC 31230,  Fundamentals of Deep Learning}
  \vfill
  \centerline{David McAllester}
  \vfill
  \centerline{\bf  The Transformer Part I}
  \vfill
  \vfill

\slide{The Transformer}

Attention is All You Need, Vaswani et al., June 2017

\vfill
The Transformer has now essentially replaced RNNs and is now used in speech, protein folding and vision.

\slide{Vector Sequences}
\centerline{\includegraphics[height=3in]{\images/transformer}}

\vfill
Each layer in the Transformer has shape $L[T,J]$ where $t$ ranges over the position in the input sequence and $j$ ranges over neurons at that position
(and omitting the batch index).

\vfill
This is the same shape as layers in an RNN --- a sequence of vectors $L[t,J]$.

\slide{Parallel Layer Computation}

However, in the transformer we can compute the layer $L_{\ell+1}[T,J]$ from $L_\ell[T,J]$ in parallel.

\vfill
This is an important difference from RNNs which compute sequentially over time.

\vfill
In this respect the transformer is more similar to a CNN than to an RNN.

\slide{Self-Attention}

The fundamental innovation of the transformer is the self-attention layer.

\vfill
For each position $t$ in the sequence we compute an attention over the other positions in the sequence.

\slide{Transformer Heads}

There is an intuitive analogy between the Transformer's self attention and a dependency parse tree.

\vfill
In a dependency parse cibsists if edges between words labeled with grammatical roles such as ``subject-of'' or ``object-of''.

\vfill
The self attention layers of the transformer we have ``heads'' which can be viewed as labels for dependency edges.

\vfill
Self attention constructs a tensor $\alpha[k,t_1,t_2]$ --- the strength of the attention weight (edge weight)
from $t_1$ to $t_2$ with head (label) $k$.

\slide{Query-Key Attention}

For each head $k$ and position $t$ we compute a key vector and a query vector with dimension $I$ typically smaller than dimension $J$.

\begin{eqnarray*}
\mathrm{Query}_{\ell+1}[k,t,I] & = & W^Q_{\ell+1}[k,I,J]L_\ell[t,J] \\
\\
\mathrm{Key}_{\ell+1}[k,t,I] & = &  W^K_{\ell+1}[k,I,J]L_\ell[t,J] \\
\\
\alpha_{\ell+1}[k,t_1,t_2] & = & \softmax_{t_2}\; \frac{1}{\sqrt{I}}\;\mathrm{Query}_{\ell+1}[k,t_1,I]\mathrm{Key}_{\ell+1}[k,t_2,I]
\end{eqnarray*}

\slide{Computing the Output}
      
\begin{eqnarray*}
\mathrm{Value}_{\ell+1}[k,t,I] & = & W^V_{\ell+1}[k,I,J]L_\ell[t,J] \\
\\
h^1_{\ell+1}[k,t,I] & = & \alpha[k,t,T]\mathrm{Value}[k,T,I] \\
\\
h^2_{\ell+1}[t,C] & = & h^1_{\ell+1}[0,t,I];\cdots;h^1_{\ell+1}[K-1,t,I] \\
\\
L_{\ell+1}[t,J] & = & W^0_{\ell+1}[J,C]h^2[t,C]
\end{eqnarray*}

\vfill
Here semicolon denotes vector concatenation.

\slide{The Transformer Layer}

Each ``transformer layer'' consists of six ``sublayers'' the first of which is the self-attention layer.


\centerline{\includegraphics[height=3.5in]{\images/transformer}}

{\Large
\centerline{Jay Alammar's blog}
}

The other layers are discussed in the next unit.

\slide{END}

}
\end{document}
